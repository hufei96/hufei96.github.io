<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>hufei的博客</title>
  <meta name="keywords" content="">
  <meta name="description" content="hufei的博客">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="网络模型方案均以粗糙的 Python 玩具代码为例 方案 1（Accept + Read&#x2F;Write）import socket  def handle(client_socket, client_address):     &#x2F;&#x2F; L6     while True:         data &#x3D; client_socket.recv(4096)         if data:">
<meta property="og:type" content="article">
<meta property="og:title" content="hufei的博客">
<meta property="og:url" content="http://example.com/2023/01/22/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="hufei的博客">
<meta property="og:description" content="网络模型方案均以粗糙的 Python 玩具代码为例 方案 1（Accept + Read&#x2F;Write）import socket  def handle(client_socket, client_address):     &#x2F;&#x2F; L6     while True:         data &#x3D; client_socket.recv(4096)         if data:">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://sleepy-1256633542.cos.ap-beijing.myqcloud.com/20181222103157.png">
<meta property="og:image" content="https://sleepy-1256633542.cos.ap-beijing.myqcloud.com/20181222102937.png">
<meta property="og:image" content="https://sleepy-1256633542.cos.ap-beijing.myqcloud.com/20181222102957.png">
<meta property="og:image" content="https://sleepy-1256633542.cos.ap-beijing.myqcloud.com/20181222103019.png">
<meta property="og:image" content="https://sleepy-1256633542.cos.ap-beijing.myqcloud.com/20181222103040.png">
<meta property="og:image" content="https://sleepy-1256633542.cos.ap-beijing.myqcloud.com/20181222103102.png">
<meta property="article:published_time" content="2023-01-22T03:54:02.008Z">
<meta property="article:modified_time" content="2022-12-29T10:29:19.779Z">
<meta property="article:author" content="hufei">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sleepy-1256633542.cos.ap-beijing.myqcloud.com/20181222103157.png">


<link rel="icon" href="/img/avatar.jpg">

<link href="/css/style.css?v=1.1.0" rel="stylesheet">

<link href="/css/hl_theme/atom-light.css?v=1.1.0" rel="stylesheet">

<link href="//cdn.jsdelivr.net/npm/animate.css@4.1.0/animate.min.css" rel="stylesheet">

<script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
<script src="/js/titleTip.js?v=1.1.0" ></script>

<script src="//cdn.jsdelivr.net/npm/highlightjs@9.16.2/highlight.pack.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script>



<script src="//cdn.jsdelivr.net/npm/jquery.cookie@1.4.1/jquery.cookie.min.js" ></script>

<script src="/js/iconfont.js?v=1.1.0" ></script>

<meta name="generator" content="Hexo 6.3.0"></head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="">
  <input class="theme_blog_path" value="">
  <input id="theme_shortcut" value="true" />
  <input id="theme_highlight_on" value="true" />
  <input id="theme_code_copy" value="true" />
</div>



<body>
<aside class="nav">
    <div class="nav-left">
        <a href="/"
   class="avatar_target">
    <img class="avatar"
         src="/img/avatar.jpg"/>
</a>
<div class="author">
    <span>hufei</span>
</div>

<div class="icon">
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
</div>




<ul>
    <li>
        <div class="all active" data-rel="全部文章">全部文章
            
                <small>(40)</small>
            
        </div>
    </li>
    
</ul>
<div class="left-bottom">
    <div class="menus">
        
            
            
    </div>
    <div>
        
        
    </div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="40">

<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="iconfont icon-left"></i>
    </div>
    <div class="friends-content">
        <ul>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <div class="right-top">
        <div id="default-panel">
            <i class="iconfont icon-search" data-title="搜索 快捷键 i"></i>
            <div class="right-title">全部文章</div>
            <i class="iconfont icon-file-tree" data-title="切换到大纲视图 快捷键 w"></i>
        </div>
        <div id="search-panel">
            <i class="iconfont icon-left" data-title="返回"></i>
            <input id="local-search-input" autocomplete="off"/>
            <label class="border-line" for="input"></label>
            <i class="iconfont icon-case-sensitive" data-title="大小写敏感"></i>
            <i class="iconfont icon-tag" data-title="标签"></i>
        </div>
        <div id="outline-panel" style="display: none">
            <div class="right-title">大纲</div>
            <i class="iconfont icon-list" data-title="切换到文章列表"></i>
        </div>
    </div>

    <div class="tags-list">
    <input id="tag-search" />
    <div class="tag-wrapper">
        
    </div>

</div>

    
    <div id="local-search-result">

    </div>
    
    <nav id="title-list-nav">
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:02">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E7%BD%91%E7%BB%9Cio%E6%A8%A1%E5%9E%8B/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:02">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:02">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%AE%9A%E6%97%B6%E5%99%A8/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:02">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E9%9D%9E%E6%B3%95%E8%BE%93%E5%85%A5%E4%BB%A5%E5%8F%8A%E6%9E%84%E9%80%A0%E5%BC%82%E5%B8%B8%E6%83%85%E5%86%B5/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B%E5%92%8CPOD%E7%B1%BB%E5%9E%8B/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E7%B1%BB%E5%9E%8B%E5%AE%89%E5%85%A8/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E9%9D%99%E6%80%81%E9%93%BE%E6%8E%A5%E5%92%8C%E5%8A%A8%E6%80%81%E9%93%BE%E6%8E%A5/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E8%B0%B7%E6%AD%8Cc++%E9%A3%8E%E6%A0%BC/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E9%98%B2%E5%BE%A1%E5%BC%8F%E7%BC%96%E7%A8%8B%E5%92%8C%E5%A5%91%E7%BA%A6%E5%BC%8F%E7%BC%96%E7%A8%8B%EF%BC%8C%E6%96%AD%E8%A8%80%E5%92%8C%E5%BC%82%E5%B8%B8/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E5%A4%9A%E7%BB%B4%E6%95%B0%E7%BB%84/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E7%AC%94%E8%AE%B0%EF%BC%88%E7%B1%BB%E4%BC%BCcsapp%E7%9A%84%E4%B9%A6%EF%BC%89/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E5%B8%B8%E7%94%A8api%E5%92%8C%E5%A4%B4%E6%96%87%E4%BB%B6/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E7%BC%96%E8%AF%91%E6%9C%9F%E5%B8%B8%E9%87%8F%E5%92%8Cconstexpr/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/vscode%20c++%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/stdref/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/stdaccumulate%E5%92%8Cstdtransform/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/socket%20api/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/mysql/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/muduo%E7%BD%91%E7%BB%9C%E6%BA%90%E7%A0%81/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/muduo%E6%97%A5%E5%BF%97/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/Linux%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%BC%96%E7%A8%8B%EF%BC%9A%E4%BD%BF%E7%94%A8muduo%20C++%E7%BD%91%E7%BB%9C%E5%BA%93/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/linux/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/IO%E5%AF%86%E9%9B%86%E5%9E%8B%E4%BB%BB%E5%8A%A1%E5%92%8C%E8%AE%A1%E7%AE%97%E5%AF%86%E9%9B%86%E5%9E%8B%E4%BB%BB%E5%8A%A1/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/io%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%E5%8E%9F%E7%90%86/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/csapp/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/CMake%20%E6%A8%A1%E5%9D%97%E5%8C%96%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/c++%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/c++%E7%B1%BB%E7%9A%84%E9%BB%98%E8%AE%A4%E5%87%BD%E6%95%B0/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88%E7%9A%84%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E6%80%A7/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E5%BC%82%E5%B8%B8/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B%E9%98%9F%E5%88%97/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/22/%E7%8E%B0%E4%BB%A3CMake/"
           data-tag=""
           data-author="" >
            <span class="post-title" title=""></span>
            <span class="post-date" title="2023-01-22 11:54:01">2023/01/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2023/01/21/hello-world/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Hello World">Hello World</span>
            <span class="post-date" title="2023-01-21 23:44:22">2023/01/21</span>
        </a>
        
        <div id="no-item-tips">

        </div>
    </nav>
    <div id="outline-list">
    </div>
</div>

    </div>
    <div class="hide-list">
        <div class="semicircle" data-title="切换全屏 快捷键 s">
            <div class="brackets first"><</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div id="post">
    <div class="pjax">
        <article id="post-操作系统" class="article article-type-post" itemscope itemprop="blogPost">
    
    <div class="article-meta">
        
        
        
        
    </div>
    <div class="article-meta">
        
            发布时间 : <time class="date" title='最后更新: 2022-12-21 18:07:46'>2023-01-22 11:54</time>
        
    </div>
    <div class="article-meta">
        
        
        <span id="busuanzi_container_page_pv">
            阅读 :<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F"><span class="toc-text">操作系统</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84"><span class="toc-text">硬件结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%98%E5%82%A8%E5%99%A8%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84"><span class="toc-text">存储器层次结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%86%99%E5%87%BA%E8%AE%A9-CPU-%E8%B7%91%E5%BE%97%E6%9B%B4%E5%BF%AB%E7%9A%84%E4%BB%A3%E7%A0%81"><span class="toc-text">如何写出让 CPU 跑得更快的代码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E7%BC%93%E5%AD%98%E7%9A%84%E5%91%BD%E4%B8%AD%E7%8E%87"><span class="toc-text">如何提升数据缓存的命中率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%8F%90%E5%8D%87%E6%8C%87%E4%BB%A4%E7%BC%93%E5%AD%98%E7%9A%84%E5%91%BD%E4%B8%AD%E7%8E%87"><span class="toc-text">如何提升指令缓存的命中率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%8F%90%E5%8D%87%E5%A4%9A%E6%A0%B8-CPU-%E7%9A%84%E7%BC%93%E5%AD%98%E5%91%BD%E4%B8%AD%E7%8E%87"><span class="toc-text">如何提升多核 CPU 的缓存命中率</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7"><span class="toc-text">CPU缓存一致性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%AA%E5%85%B1%E4%BA%AB%E9%97%AE%E9%A2%98"><span class="toc-text">伪共享问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%81%BF%E5%85%8D%E4%BC%AA%E5%85%B1%E4%BA%AB%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-text">避免伪共享的方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Linux%E6%98%AF%E6%80%8E%E4%B9%88%E8%BF%9B%E8%A1%8C%E8%BF%9B%E7%A8%8B%E8%B0%83%E5%BA%A6%E7%9A%84"><span class="toc-text">Linux是怎么进行进程调度的</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B0%83%E5%BA%A6%E7%B1%BB"><span class="toc-text">调度类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#CPU-%E8%BF%90%E8%A1%8C%E9%98%9F%E5%88%97"><span class="toc-text">CPU 运行队列</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8C%E5%85%A8%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6"><span class="toc-text">完全公平调度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B0%83%E6%95%B4%E4%BC%98%E5%85%88%E7%BA%A7"><span class="toc-text">调整优先级</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%AD%E6%96%AD%E5%92%8C%E8%BD%AF%E4%B8%AD%E6%96%AD"><span class="toc-text">中断和软中断</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E8%BD%AF%E4%B8%AD%E6%96%AD%EF%BC%9F"><span class="toc-text">什么是软中断？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84"><span class="toc-text">操作系统结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Linux%E5%86%85%E6%A0%B8%E5%92%8CWindows%E5%86%85%E6%A0%B8%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">Linux内核和Windows内核的区别</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Linux%E7%9A%84%E8%AE%BE%E8%AE%A1"><span class="toc-text">Linux的设计</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A0%88%E5%9C%B0%E5%9D%80%E4%BB%8E%E9%AB%98%E5%88%B0%E4%BD%8E%EF%BC%8C%E5%A0%86%E4%BB%8E%E4%BD%8E%E5%88%B0%E9%AB%98"><span class="toc-text">为什么栈地址从高到低，堆从低到高</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Windows%E7%9A%84%E8%AE%BE%E8%AE%A1"><span class="toc-text">Windows的设计</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%A6%82%E5%BF%B5"><span class="toc-text">操作系统概念</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="toc-text">内存管理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98"><span class="toc-text">虚拟内存</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98"><span class="toc-text">什么是虚拟内存</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98"><span class="toc-text">为什么需要虚拟内存</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%A1%E7%90%86%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80%E5%92%8C%E7%89%A9%E7%90%86%E5%9C%B0%E5%9D%80%E7%9A%84%E6%98%A0%E5%B0%84"><span class="toc-text">管理虚拟地址和物理地址的映射</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E5%88%86%E6%AE%B5"><span class="toc-text">内存分段</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%98%A0%E5%B0%84%E6%9C%BA%E5%88%B6"><span class="toc-text">映射机制</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BC%BA%E9%99%B7"><span class="toc-text">缺陷</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E5%88%86%E9%A1%B5"><span class="toc-text">内存分页</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%98%A0%E5%B0%84%E6%9C%BA%E5%88%B6-1"><span class="toc-text">映射机制</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B7%9F%E5%88%86%E6%AE%B5%E7%9B%B8%E6%AF%94%E5%8C%BA%E5%88%AB"><span class="toc-text">跟分段相比区别</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BC%BA%E9%99%B7-1"><span class="toc-text">缺陷</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%9A%E7%BA%A7%E9%A1%B5%E8%A1%A8"><span class="toc-text">多级页表</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#TLB"><span class="toc-text">TLB</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AE%B5%E9%A1%B5%E5%BC%8F%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="toc-text">段页式内存管理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Linux%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%9C%BA%E5%88%B6"><span class="toc-text">Linux内存管理机制</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#malloc%E6%98%AF%E6%80%8E%E4%B9%88%E5%88%86%E9%85%8D%E5%86%85%E5%AD%98%E7%9A%84"><span class="toc-text">malloc是怎么分配内存的</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#malloc-%E5%88%86%E9%85%8D%E7%9A%84%E6%98%AF%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E5%90%97%EF%BC%9F"><span class="toc-text">malloc() 分配的是物理内存吗？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#malloc-1-%E5%88%86%E9%85%8D%E5%A4%9A%E5%B0%91%E5%86%85%E5%AD%98"><span class="toc-text">malloc(1)分配多少内存</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#free-%E9%87%8A%E6%94%BE%E5%86%85%E5%AD%98%EF%BC%8C%E4%BC%9A%E5%BD%92%E8%BF%98%E7%BB%99%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%90%97%EF%BC%9F"><span class="toc-text">free 释放内存，会归还给操作系统吗？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%85%A8%E9%83%A8%E4%BD%BF%E7%94%A8-mmap-%E6%9D%A5%E5%88%86%E9%85%8D%E5%86%85%E5%AD%98%EF%BC%9F"><span class="toc-text">为什么不全部使用 mmap 来分配内存？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%85%A8%E9%83%A8%E4%BD%BF%E7%94%A8-brk-%E6%9D%A5%E5%88%86%E9%85%8D%EF%BC%9F"><span class="toc-text">为什么不全部使用 brk 来分配？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#free-%E5%87%BD%E6%95%B0%E5%8F%AA%E4%BC%A0%E5%85%A5%E4%B8%80%E4%B8%AA%E5%86%85%E5%AD%98%E5%9C%B0%E5%9D%80%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E7%9F%A5%E9%81%93%E8%A6%81%E9%87%8A%E6%94%BE%E5%A4%9A%E5%A4%A7%E7%9A%84%E5%86%85%E5%AD%98%EF%BC%9F"><span class="toc-text">free() 函数只传入一个内存地址，为什么能知道要释放多大的内存？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ptmalloc%E3%80%81tcmalloc%E4%B8%8Ejemalloc%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90"><span class="toc-text">ptmalloc、tcmalloc与jemalloc对比分析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E6%BB%A1%E4%BA%86%E4%BC%9A%E5%8F%91%E7%94%9F%E4%BB%80%E4%B9%88"><span class="toc-text">内存满了会发生什么</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E8%BF%87%E7%A8%8B"><span class="toc-text">内存分配过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%93%AA%E4%BA%9B%E5%86%85%E5%AD%98%E5%8F%AF%E4%BB%A5%E5%9B%9E%E6%94%B6"><span class="toc-text">哪些内存可以回收</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BF%9D%E6%8A%A4%E4%B8%80%E4%B8%AA%E8%BF%9B%E7%A8%8B%E4%B8%8D%E8%A2%AB-OOM-%E6%9D%80%E6%8E%89"><span class="toc-text">如何保护一个进程不被 OOM 杀掉</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8-4GB-%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E7%9A%84%E6%9C%BA%E5%99%A8%E4%B8%8A%EF%BC%8C%E7%94%B3%E8%AF%B7-8G-%E5%86%85%E5%AD%98%E4%BC%9A%E6%80%8E%E4%B9%88%E6%A0%B7"><span class="toc-text">在 4GB 物理内存的机器上，申请 8G 内存会怎么样</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86"><span class="toc-text">进程管理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9B%E7%A8%8B"><span class="toc-text">进程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%9B%E7%A8%8B%E7%9A%84%E7%8A%B6%E6%80%81"><span class="toc-text">进程的状态</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%9B%E7%A8%8B%E7%9A%84%E6%8E%A7%E5%88%B6%E7%BB%93%E6%9E%84"><span class="toc-text">进程的控制结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%9B%E7%A8%8B%E6%8E%A7%E5%88%B6"><span class="toc-text">进程控制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%9B%E7%A8%8B%E7%9A%84%E4%B8%8A%E4%B8%8B%E6%96%87%E5%88%87%E6%8D%A2"><span class="toc-text">进程的上下文切换</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E7%A8%8B"><span class="toc-text">线程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E7%A8%8B%E7%9A%84%E4%B8%8A%E4%B8%8B%E6%96%87%E5%88%87%E6%8D%A2"><span class="toc-text">线程的上下文切换</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-text">线程的实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Linux%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-text">Linux线程模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%BF%98%E6%98%AF%E5%A4%9A%E8%BF%9B%E7%A8%8B"><span class="toc-text">多线程还是多进程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9B%E7%A8%8B%E8%B0%83%E5%BA%A6"><span class="toc-text">进程调度</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95"><span class="toc-text">调度算法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1"><span class="toc-text">进程通信</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%A1%E9%81%93"><span class="toc-text">管道</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8C%BF%E5%90%8D%E7%AE%A1%E9%81%93%E5%88%9B%E5%BB%BA%E5%8E%9F%E7%90%86"><span class="toc-text">匿名管道创建原理</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97"><span class="toc-text">消息队列</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98"><span class="toc-text">共享内存</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%A1%E5%8F%B7%E9%87%8F"><span class="toc-text">信号量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%A1%E5%8F%B7"><span class="toc-text">信号</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#socket"><span class="toc-text">socket</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%B2%E7%AA%81"><span class="toc-text">多线程冲突</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%94%81"><span class="toc-text">锁</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%BF%99%E7%AD%89%E5%BE%85%E9%94%81"><span class="toc-text">忙等待锁</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%97%A0%E7%AD%89%E5%BE%85%E9%94%81"><span class="toc-text">无等待锁</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84%E9%94%81"><span class="toc-text">常见的锁</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%A1%E5%8F%B7%E9%87%8F-1"><span class="toc-text">信号量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E9%97%AE%E9%A2%98"><span class="toc-text">生产者消费者问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%93%B2%E5%AD%A6%E5%AE%B6%E5%B0%B1%E9%A4%90%E9%97%AE%E9%A2%98"><span class="toc-text">哲学家就餐问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E8%80%85-%E5%86%99%E8%80%85%E9%97%AE%E9%A2%98"><span class="toc-text">读者-写者问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%BB%E9%94%81"><span class="toc-text">死锁</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%8E%92%E6%9F%A5%E6%AD%BB%E9%94%81"><span class="toc-text">排查死锁</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%81%BF%E5%85%8D%E6%AD%BB%E9%94%81"><span class="toc-text">避免死锁</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E8%BF%9B%E7%A8%8B%E6%9C%80%E5%A4%9A%E5%8F%AF%E4%BB%A5%E5%88%9B%E5%BB%BA%E5%A4%9A%E5%B0%91%E4%B8%AA%E7%BA%BF%E7%A8%8B"><span class="toc-text">一个进程最多可以创建多少个线程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E7%A8%8B%E5%B4%A9%E6%BA%83%E4%BA%86%EF%BC%8C%E8%BF%9B%E7%A8%8B%E4%B9%9F%E4%BC%9A%E5%B4%A9%E6%BA%83%E5%90%97%EF%BC%9F"><span class="toc-text">线程崩溃了，进程也会崩溃吗？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%9B%E7%A8%8B%E6%98%AF%E5%A6%82%E4%BD%95%E5%B4%A9%E6%BA%83%E7%9A%84-%E4%BF%A1%E5%8F%B7%E6%9C%BA%E5%88%B6%E7%AE%80%E4%BB%8B"><span class="toc-text">进程是如何崩溃的-信号机制简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E7%BA%BF%E7%A8%8B%E5%B4%A9%E6%BA%83%E4%B8%8D%E4%BC%9A%E5%AF%BC%E8%87%B4-JVM-%E8%BF%9B%E7%A8%8B%E5%B4%A9%E6%BA%83"><span class="toc-text">为什么线程崩溃不会导致 JVM 进程崩溃</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95-1"><span class="toc-text">调度算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E9%A1%B5%E9%9D%A2%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95"><span class="toc-text">内存页面置换算法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9C%80%E4%BD%B3%E9%A1%B5%E9%9D%A2%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95"><span class="toc-text">最佳页面置换算法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%85%88%E8%BF%9B%E5%85%88%E5%87%BA%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95"><span class="toc-text">先进先出置换算法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9C%80%E8%BF%91%E6%9C%80%E4%B9%85%E6%9C%AA%E4%BD%BF%E7%94%A8%E7%9A%84%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95"><span class="toc-text">最近最久未使用的置换算法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%97%B6%E9%92%9F%E9%A1%B5%E9%9D%A2%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95"><span class="toc-text">时钟页面置换算法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9C%80%E4%B8%8D%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95"><span class="toc-text">最不常用算法</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95"><span class="toc-text">磁盘调度算法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%85%88%E6%9D%A5%E5%85%88%E6%9C%8D%E5%8A%A1"><span class="toc-text">先来先服务</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9C%80%E7%9F%AD%E5%AF%BB%E9%81%93%E6%97%B6%E9%97%B4%E4%BC%98%E5%85%88"><span class="toc-text">最短寻道时间优先</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%89%AB%E6%8F%8F%E7%AE%97%E6%B3%95"><span class="toc-text">扫描算法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E6%89%AB%E6%8F%8F%E7%AE%97%E6%B3%95"><span class="toc-text">循环扫描算法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#LOOK-%E4%B8%8E-C-LOOK%E7%AE%97%E6%B3%95"><span class="toc-text">LOOK 与 C-LOOK算法</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="toc-text">文件系统</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%99%9A%E6%8B%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="toc-text">虚拟文件系统</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-text">文件的使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E7%9A%84%E5%AD%98%E5%82%A8"><span class="toc-text">文件的存储</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E5%AD%98%E6%94%BE%E6%96%B9%E5%BC%8F"><span class="toc-text">连续空间存放方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9D%9E%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E5%AD%98%E6%94%BE%E6%96%B9%E5%BC%8F"><span class="toc-text">非连续空间存放方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Unix-%E6%96%87%E4%BB%B6%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F"><span class="toc-text">Unix 文件的实现方式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A9%BA%E9%97%B2%E7%A9%BA%E9%97%B4%E7%AE%A1%E7%90%86"><span class="toc-text">空闲空间管理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A9%BA%E9%97%B2%E8%A1%A8%E6%B3%95"><span class="toc-text">空闲表法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A9%BA%E9%97%B2%E9%93%BE%E8%A1%A8%E6%B3%95"><span class="toc-text">空闲链表法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%8D%E5%9B%BE%E6%B3%95"><span class="toc-text">位图法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%BB%93%E6%9E%84"><span class="toc-text">文件系统的结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E5%BD%95%E7%9A%84%E5%AD%98%E5%82%A8"><span class="toc-text">目录的存储</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AF%E9%93%BE%E6%8E%A5%E5%92%8C%E7%A1%AC%E9%93%BE%E6%8E%A5"><span class="toc-text">软链接和硬链接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E4%BB%B6-I-x2F-O"><span class="toc-text">文件 I&#x2F;O</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%93%E5%86%B2%E4%B8%8E%E9%9D%9E%E7%BC%93%E5%86%B2-I-x2F-O"><span class="toc-text">缓冲与非缓冲 I&#x2F;O</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B4%E6%8E%A5%E4%B8%8E%E9%9D%9E%E7%9B%B4%E6%8E%A5-I-x2F-O"><span class="toc-text">直接与非直接 I&#x2F;O</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%98%BB%E5%A1%9E%E4%B8%8E%E9%9D%9E%E9%98%BB%E5%A1%9E-I-x2F-O-VS-%E5%90%8C%E6%AD%A5%E4%B8%8E%E5%BC%82%E6%AD%A5-I-x2F-O"><span class="toc-text">阻塞与非阻塞 I&#x2F;O VS 同步与异步 I&#x2F;O</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%94%E5%A4%A7IO%E6%A8%A1%E5%9E%8B"><span class="toc-text">五大IO模型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BE%E5%A4%87%E7%AE%A1%E7%90%86"><span class="toc-text">设备管理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#I-x2F-O%E6%8E%A7%E5%88%B6%E5%99%A8"><span class="toc-text">I&#x2F;O控制器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#I-x2F-O-%E6%8E%A7%E5%88%B6%E6%96%B9%E5%BC%8F"><span class="toc-text">I&#x2F;O 控制方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F"><span class="toc-text">设备驱动程序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E7%94%A8%E5%9D%97%E5%B1%82"><span class="toc-text">通用块层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F-I-x2F-O-%E8%BD%AF%E4%BB%B6%E5%88%86%E5%B1%82"><span class="toc-text">存储系统 I&#x2F;O 软件分层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%94%AE%E7%9B%98%E6%95%B2%E5%85%A5%E5%AD%97%E6%AF%8D%E6%97%B6%EF%BC%8C%E6%9C%9F%E9%97%B4%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-text">键盘敲入字母时，期间发生了什么？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E4%BC%98%E5%8C%96"><span class="toc-text">文件传输优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#DMA"><span class="toc-text">DMA</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E7%9A%84%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E6%9C%89%E5%A4%9A%E7%B3%9F%E7%B3%95%EF%BC%9F"><span class="toc-text">传统的文件传输有多糟糕？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E7%9A%84%E6%80%A7%E8%83%BD%EF%BC%9F"><span class="toc-text">如何优化文件传输的性能？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E9%9B%B6%E6%8B%B7%E8%B4%9D%EF%BC%9F"><span class="toc-text">如何实现零拷贝？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#mmap-write"><span class="toc-text">mmap + write</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sendfile"><span class="toc-text">sendfile</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PageCache-%E6%9C%89%E4%BB%80%E4%B9%88%E4%BD%9C%E7%94%A8%EF%BC%9F"><span class="toc-text">PageCache 有什么作用？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E7%94%A8%E4%BB%80%E4%B9%88%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%EF%BC%9F"><span class="toc-text">大文件传输用什么方式实现？</span></a></li></ol></li></ol></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h1><h2 id="硬件结构"><a href="#硬件结构" class="headerlink" title="硬件结构"></a>硬件结构</h2><h3 id="存储器层次结构"><a href="#存储器层次结构" class="headerlink" title="存储器层次结构"></a>存储器层次结构</h3><p>以hello程序执行为例：</p>
<p>先由C语言的源文件<code>hello.c</code>编译得到了可执行目标文件<code>hello</code></p>
<ol>
<li>shell读入我们输入的字符<code>./hello</code>后，将其逐一读入到CPU的寄存器中，然后再将其存放到主存中。</li>
<li>输入回车后，shell执行一系列指令将hello目标文件中的代码和数据从磁盘复制到主存。</li>
<li>CPU开始执行hello的main程序中的机器指令，它将<code>hello, world\n</code>字符串中的字节从主存复制到CPU寄存器，再从CPU寄存器复制到显示设备。</li>
</ol>
<p>由此可见执行代码时，会花费大量时间将代码和数据进行复制。而<strong>不同设备之间运行速度差距极大</strong>，在等待复制的过程中cpu浪费了大量的时间，为了加快复制速度和提高cpu利用率，引入了存储器体系结构。</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/image-20220512235134334.png" alt="image-20220512235134334"></p>
<p>根据<strong>局部性原理</strong>可知，程序具有访问局部区域内的数据和代码的趋势，所以在处理器和一个较大较慢的设备之间插入一个更小更快的存储设备，来暂时保存处理器近期可能会需要的数据，使得大部分的内存操作都能在高速缓存内完成，就能极大提高系统速度了。</p>
<p>存储器层次结构的<strong>主要思想</strong>是将上一层的存储器作为下一层存储器的高速缓存。</p>
<p>从 寄存器、CPU Cache，到内存、硬盘，这样一层层下来的存储器，访问速度越来越慢，存储容量越来越大，价格也越来越便宜，而且每个存储器只和相邻的一层存储器设备打交道，于是这样就形成了存储器的层次结构。</p>
<p>那机械硬盘、固态硬盘、内存这三个存储器，到底和 <code>CPU L1 Cache</code> 相比速度差多少倍呢？</p>
<p>CPU L1 Cache 随机访问延时是 1 纳秒，内存则是 100 纳秒，所以 <strong>CPU L1 Cache 比内存快 <code>100</code> 倍左右</strong>。</p>
<p>SSD 随机访问延时是 150 微秒，所以 <strong>CPU L1 Cache 比 SSD 快 <code>150000</code> 倍左右</strong>。</p>
<p>最慢的机械硬盘随机访问延时已经高达 10 毫秒，我们来看看机械硬盘到底有多龟速：</p>
<ul>
<li><strong>SSD 比机械硬盘快 70 倍左右；</strong></li>
<li><strong>内存比机械硬盘快 100000 倍左右；</strong></li>
<li><strong>CPU L1 Cache 比机械硬盘快 10000000 倍左右；</strong></li>
</ul>
<h3 id="如何写出让-CPU-跑得更快的代码"><a href="#如何写出让-CPU-跑得更快的代码" class="headerlink" title="如何写出让 CPU 跑得更快的代码"></a>如何写出让 CPU 跑得更快的代码</h3><p>CPU Cache 通常分为大小不等的三级缓存，分别是 <strong>L1 Cache、L2 Cache 和 L3 Cache</strong>。如果 CPU 运算时，直接从 CPU Cache 读取数据，而不是从内存的话，运算速度就会很快。</p>
<p>其中，<strong>L1 Cache 通常会分为「数据缓存」和「指令缓存」</strong>，这意味着数据和指令在 L1 Cache 这一层是分开缓存的</p>
<p>另外，L3 Cache 比 L1 Cache 和 L2 Cache 大很多，这是因为 <strong>L1 Cache 和 L2 Cache 都是每个 CPU 核心独有的，而 L3 Cache 是多个 CPU 核心共享的。</strong></p>
<p>程序执行时，会先将内存中的数据加载到共享的 L3 Cache 中，再加载到每个核心独有的 L2 Cache，最后进入到最快的 L1 Cache，之后才会被 CPU 读取。它们之间的层级关系，如下图：</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/CPU-Cache.png" alt="CPU-Cache"></p>
<p>越靠近 CPU 核心的缓存其访问速度越快，CPU 访问 L1 Cache 只需要 <code>2~4</code> 个时钟周期，访问 L2 Cache 大约 <code>10~20</code> 个时钟周期，访问 L3 Cache 大约 <code>20~60</code> 个时钟周期，而访问内存速度大概在 <code>200~300</code> 个 时钟周期之间。</p>
<p>CPU Cache 的数据是从内存中读取过来的，它是以一小块一小块读取数据的，而不是按照单个数组元素来读取数据的，在 CPU Cache 中的，这样一小块一小块的数据，称为 <strong>Cache Line（缓存块）</strong>。</p>
<p>于是，如何写出让 CPU 跑得更快的代码这个问题，可以<strong>改成如何写出 CPU 缓存命中率高</strong>的代码。</p>
<p>在前面提到， L1 Cache 通常分为数据缓存和指令缓存，这是因为 CPU 会分别处理数据和指令，比如 <code>1+1=2</code> 这个运算，<code>+</code> 就是指令，会被放在指令缓存中，而输入数字 <code>1</code> 则会被放在数据缓存里。</p>
<p>因此，<strong>我们要分开来看数据缓存和指令缓存的缓存命中率</strong>。</p>
<h4 id="如何提升数据缓存的命中率"><a href="#如何提升数据缓存的命中率" class="headerlink" title="如何提升数据缓存的命中率"></a>如何提升数据缓存的命中率</h4><p>Cache一块块地批量将数据从内存读到Cache，<strong>遇到遍历数组的情况时，按照内存布局顺序访问而不是跳跃性访问，将可以有效的利用 CPU Cache 带来的好处，这样我们代码的性能就会得到很大的提升</strong></p>
<h4 id="如何提升指令缓存的命中率"><a href="#如何提升指令缓存的命中率" class="headerlink" title="如何提升指令缓存的命中率"></a>如何提升指令缓存的命中率</h4><p>我们以一个例子来看看，假设有一个元素为 0 到 100 之间随机数字组成的一维数组：</p>
<p>接下来，对这个数组做两个操作：</p>
<ul>
<li>第一个操作，循环遍历数组，把小于 50 的数组元素置为 0；</li>
<li>第二个操作，将数组排序；</li>
</ul>
<p>那么问题来了，你觉得先遍历再排序速度快，还是先排序再遍历速度快呢？</p>
<p>在回答这个问题之前，我们先了解 CPU 的<strong>分支预测器</strong>。对于 if 条件语句，意味着此时至少可以选择跳转到两段不同的指令执行，也就是 if 还是 else 中的指令。那么，<strong>如果分支预测可以预测到接下来要执行 if 里的指令，还是 else 指令的话，就可以「提前」把这些指令放在指令缓存中，这样 CPU 可以直接从 Cache 读取到指令，于是执行速度就会很快</strong>。</p>
<p>当数组中的元素是随机的，分支预测就无法有效工作，而当数组元素都是是顺序的，分支预测器会动态地根据历史命中数据对未来进行预测，这样命中率就会很高。</p>
<p>因此，先排序再遍历速度会更快，这是因为排序之后，数字是从小到大的，那么前几次循环命中 <code>if &lt; 50</code> 的次数会比较多，于是分支预测就会缓存 <code>if</code> 里的 <code>array[i] = 0</code> 指令到 Cache 中，后续 CPU 执行该指令就只需要从 Cache 读取就好了。</p>
<p>所以对于指令缓存，有规律的条件分支语句能够让 CPU 的分支预测器发挥作用，进一步提高执行的效率；</p>
<h4 id="如何提升多核-CPU-的缓存命中率"><a href="#如何提升多核-CPU-的缓存命中率" class="headerlink" title="如何提升多核 CPU 的缓存命中率"></a>如何提升多核 CPU 的缓存命中率</h4><p>在单核 CPU，虽然只能执行一个线程，但是操作系统给每个线程分配了一个时间片，时间片用完了，就调度下一个线程，于是各个线程就按时间片交替地占用 CPU，从宏观上看起来各个线程同时在执行。</p>
<p>而现代 CPU 都是多核心的，线程可能在不同 CPU 核心来回切换执行，这对 CPU Cache 不是有利的，虽然 L3 Cache 是多核心之间共享的，但是 L1 和 L2 Cache 都是每个核心独有的，<strong>如果一个线程在不同核心来回切换，各个核心的缓存命中率就会受到影响</strong>，相反如果线程都在同一个核心上执行，那么其数据的 L1 和 L2 Cache 的缓存命中率可以得到有效提高，缓存命中率高就意味着 CPU 可以减少访问 内存的频率。</p>
<p><strong>当有多个同时执行「计算密集型」的线程</strong>，为了防止因为切换到不同的核心，而导致缓存命中率下降的问题，我们可以把<strong>线程绑定在某一个 CPU 核心上</strong>，这样性能可以得到非常可观的提升。</p>
<p>在 Linux 上提供了 <code>sched_setaffinity</code> 方法，来实现将线程绑定到某个 CPU 核心这一功能。</p>
<h3 id="CPU缓存一致性"><a href="#CPU缓存一致性" class="headerlink" title="CPU缓存一致性"></a>CPU缓存一致性</h3><p>CPU 在读写数据的时候，都是在 CPU Cache 读写数据的，原因是 Cache 离 CPU 很近，读写性能相比内存高出很多。对于 Cache 里没有缓存 CPU 所需要读取的数据的这种情况，CPU 则会从内存读取数据，并将数据缓存到 Cache 里面，最后 CPU 再从 Cache 读取数据。</p>
<p>而对于数据的写入，CPU 都会先写入到 Cache 里面，然后再在找个合适的时机写入到内存，那就有「写直达」和「写回」这两种策略来保证 Cache 与内存的数据一致性：</p>
<ul>
<li>写直达，只要有数据写入，都会直接把数据写入到内存里面，这种方式简单直观，但是性能就会受限于内存的访问速度；</li>
<li>写回，对于已经缓存在 Cache 的数据的写入，只需要更新其数据就可以，不用写入到内存，只有在需要把缓存里面的脏数据交换出去的时候，才把数据同步到内存里，这种方式在缓存命中率高的情况，性能会更好；</li>
</ul>
<p>但是在当今 CPU 都是多核的，每个核心都有各自独立的 L1&#x2F;L2 Cache，只有 L3 Cache 是多个核心之间共享的。写回法中内存存储的值不一定是最新值，这可能会导致多核间的缓存不一致。</p>
<p>假设 A 号核心和 B 号核心同时运行两个线程，都操作共同的变量 i（初始值为 0 ）。</p>
<p>这时如果 A 号核心执行了 <code>i++</code> 语句的时候，为了考虑性能，使用了我们前面所说的写回策略，先把值为 <code>1</code> 的执行结果写入到 L1&#x2F;L2 Cache 中，然后把 L1&#x2F;L2 Cache 中对应的 Block 标记为脏的，这个时候数据其实没有被同步到内存中的，因为写回策略，只有在 A 号核心中的这个 Cache Block 要被替换的时候，数据才会写入到内存里。</p>
<p>如果这时旁边的 B 号核心尝试从内存读取 i 变量的值，则读到的将会是错误的值，因为刚才 A 号核心更新 i 值还没写入到内存中，内存中的值还依然是 0。<strong>这个就是所谓的缓存一致性问题，A 号核心和 B 号核心的缓存，在这个时候是不一致，从而会导致执行结果的错误。</strong></p>
<p>要想实现缓存一致性，关键是要满足 2 点：</p>
<ul>
<li>第一点是<strong>写传播</strong>，也就是当某个 CPU 核心发生写入操作时，需要把该事件广播通知给其他核心；</li>
<li>第二点是<strong>事务的串行化</strong>，某个 CPU 核心里对数据的操作顺序，必须在其他核心看起来顺序是一样的，这个很重要，只有保证了这个，才能保障我们的数据是真正一致的，我们的程序在各个不同的核心上运行的结果也是一致的；</li>
</ul>
<p>举个例子来理解事务的串行化：</p>
<p>假设我们有一个含有 4 个核心的 CPU，这 4 个核心都操作共同的变量 i（初始值为 0 ）。A 号核心先把 i 值变为 100，而此时同一时间，B 号核心先把 i 值变为 200，这里两个修改，都会「传播」到 C 和 D 号核心。</p>
<p>那么问题就来了，C 号核心先收到了 A 号核心更新数据的事件，再收到 B 号核心更新数据的事件，因此 C 号核心看到的变量 i 是先变成 100，后变成 200。而如果 D 号核心收到的事件是反过来的，则 D 号核心看到的是变量 i 先变成 200，再变成 100，虽然是做到了写传播，但是各个 Cache 里面的数据还是不一致的。</p>
<p>所以，我们要保证 C 号核心和 D 号核心都能看到<strong>相同顺序的数据变化</strong>，比如变量 i 都是先变成 100，再变成 200，这样的过程就是事务的串行化。</p>
<p>基于总线嗅探机制的 MESI 协议，就满足上面了这两点，因此它是保障缓存一致性的协议。总线嗅探指 CPU核心 需要每时每刻监听总线上的一切活动，同时当核心更新了 Cache 中的数据时，要把该事件广播通知到其他核心，不管别的核心的 Cache 是否缓存相同的数据。</p>
<p>MESI 协议，是已修改、独占、共享、已失效这四个状态的英文缩写的组合(Modified,Exclusive,Shared,Invalidated)。这四个状态来标记 Cache Line 四个不同的状态。</p>
<p>「已修改」状态就是我们前面提到的脏标记，代表该 Cache Block 上的数据已经被更新过，但是还没有写到内存里。而「已失效」状态，表示的是这个 Cache Block 里的数据已经失效了，不可以读取该状态的数据。</p>
<p>「独占」和「共享」状态都代表 Cache Block 里的数据是干净的，也就是说，这个时候 Cache Block 里的数据和内存里面的数据是一致性的。</p>
<p>「独占」和「共享」的差别在于，独占状态的时候，数据只存储在一个 CPU 核心的 Cache 里，而其他 CPU 核心的 Cache 没有该数据。这个时候，如果要向独占的 Cache 写数据，就可以直接自由地写入，而不需要通知其他 CPU 核心，因为只有你这有这个数据，就不存在缓存一致性的问题了，于是就可以随便操作该数据。</p>
<p>另外，在「独占」状态下的数据，如果有其他核心从内存读取了相同的数据到各自的 Cache ，那么这个时候，独占状态下的数据就会变成共享状态。</p>
<p>那么，「共享」状态代表着相同的数据在多个 CPU 核心的 Cache 里都有，所以当我们要更新 Cache 里面的数据的时候，不能直接修改，而是要先向所有的其他 CPU 核心广播一个请求，要求先把其他核心的 Cache 中对应的 Cache Line 标记为「无效」状态，然后再更新当前 Cache 里面的数据。</p>
<p>我们举个具体的例子来看看这四个状态的转换：</p>
<ol>
<li>当 A 号 CPU 核心从内存读取变量 i 的值，数据被缓存在 A 号 CPU 核心自己的 Cache 里面，此时其他 CPU 核心的 Cache 没有缓存该数据，于是标记 Cache Line 状态为「独占」，此时其 Cache 中的数据与内存是一致的；</li>
<li>然后 B 号 CPU 核心也从内存读取了变量 i 的值，此时会发送消息给其他 CPU 核心，由于 A 号 CPU 核心已经缓存了该数据，所以会把数据返回给 B 号 CPU 核心。在这个时候， A 和 B 核心缓存了相同的数据，Cache Line 的状态就会变成「共享」，并且其 Cache 中的数据与内存也是一致的；</li>
<li>当 A 号 CPU 核心要修改 Cache 中 i 变量的值，发现数据对应的 Cache Line 的状态是共享状态，则要向所有的其他 CPU 核心广播一个请求，要求先把其他核心的 Cache 中对应的 Cache Line 标记为「无效」状态，然后 A 号 CPU 核心才更新 Cache 里面的数据，同时标记 Cache Line 为「已修改」状态，此时 Cache 中的数据就与内存不一致了。</li>
<li>如果 A 号 CPU 核心「继续」修改 Cache 中 i 变量的值，由于此时的 Cache Line 是「已修改」状态，因此不需要给其他 CPU 核心发送消息，直接更新数据即可。</li>
<li>如果 A 号 CPU 核心的 Cache 里的 i 变量对应的 Cache Line 要被「替换」，发现 Cache Line 状态是「已修改」状态，就会在替换前先把数据同步到内存。</li>
</ol>
<p>所以，可以发现当 Cache Line 状态是「已修改」或者「独占」状态时，修改更新其数据不需要发送广播给其他 CPU 核心，这在一定程度上减少了总线带宽压力。</p>
<p>整个 MESI 的状态可以用一个有限状态机来表示它的状态流转。还有一点，对于不同状态触发的事件操作，可能是来自本地 CPU 核心发出的广播事件，也可以是来自其他 CPU 核心通过总线发出的广播事件。</p>
<h3 id="伪共享问题"><a href="#伪共享问题" class="headerlink" title="伪共享问题"></a>伪共享问题</h3><p>多核心CPU会出现伪共享的问题，即Cache失效的问题。</p>
<p>现在假设有一个双核心的 CPU，这两个 CPU 核心并行运行着两个不同的线程，它们同时从内存中读取两个不同的数据，分别是变量 A 和 B，这个两个数据的地址在物理内存上是<strong>连续</strong>的，如果 Cahce Line 的大小是 64 字节，并且变量 A 在 Cahce Line 的开头位置，那么这两个数据是位于<strong>同一个 Cache Line 中</strong>，又因为 CPU Line 是 CPU 从内存读取数据到 Cache 的单位，所以这两个数据会被同时读入到了两个 CPU 核心中各自 Cache 中。</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/cacheline.png" alt="cacheline"></p>
<p>如果这两个不同核心的线程分别修改不同的数据，比如 1 号 CPU 核心的线程只修改了 变量 A，或 2 号 CPU 核心的线程的线程只修改了变量 B，会发生什么呢？</p>
<p>①. 最开始变量 A 和 B 都还不在 Cache 里面，假设 1 号核心绑定了线程 A，2 号核心绑定了线程 B，线程 A 只会读写变量 A，线程 B 只会读写变量 B。</p>
<p>②. 1 号核心读取变量 A，由于 CPU 从内存读取数据到 Cache 的单位是 Cache Line，也正好变量 A 和 变量 B 的数据归属于同一个 Cache Line，所以 A 和 B 的数据都会被加载到 Cache，并将此 Cache Line 标记为「独占」状态。</p>
<p>③. 接着，2 号核心开始从内存里读取变量 B，同样的也是读取 Cache Line 大小的数据到 Cache 中，此 Cache Line 中的数据也包含了变量 A 和 变量 B，此时 1 号和 2 号核心的 Cache Line 状态变为「共享」状态。</p>
<p>④. 1 号核心需要修改变量 A，发现此 Cache Line 的状态是「共享」状态，所以先需要通过总线发送消息给 2 号核心，通知 2 号核心把 Cache 中对应的 Cache Line 标记为「已失效」状态，然后 1 号核心对应的 Cache Line 状态变成「已修改」状态，并且修改变量 A。</p>
<p>⑤. 之后，2 号核心需要修改变量 B，此时 2 号核心的 Cache 中对应的 Cache Line 是已失效状态，另外由于 1 号核心的 Cache 也有此相同的数据，且状态为「已修改」状态，所以要先把 1 号核心的 Cache 对应的 Cache Line 写回到内存，然后 2 号核心再从内存读取 Cache Line 大小的数据到 Cache 中，最后把变量 B 修改到 2 号核心的 Cache 中，并将状态标记为「已修改」状态。</p>
<p>所以，可以发现如果 1 号和 2 号 CPU 核心这样持续交替的分别修改变量 A 和 B，就会重复 ④ 和 ⑤ 这两个步骤，Cache 并没有起到缓存的效果，虽然变量 A 和 B 之间其实并没有任何的关系，但是因为同时归属于一个 Cache Line ，这个 Cache Line 中的任意数据被修改后，都会相互影响，从而出现 ④ 和 ⑤ 这两个步骤。</p>
<p>因此，这种因为多个线程同时读写同一个 Cache Line 的不同变量时，而导致 CPU Cache 失效的现象称为<strong>伪共享（False Sharing）</strong>。</p>
<h4 id="避免伪共享的方法"><a href="#避免伪共享的方法" class="headerlink" title="避免伪共享的方法"></a>避免伪共享的方法</h4><p>因此，对于多个线程共享的热点数据，即经常会修改的数据，应该避免这些数据刚好在同一个 Cache Line 中，否则就会出现为伪共享的问题。</p>
<p>在 Linux 内核中存在 <code>__cacheline_aligned_in_smp</code> 宏定义，是用于解决伪共享的问题。</p>
<ul>
<li>如果在多核（MP）系统里，该宏定义是 <code>__cacheline_aligned</code>，也就是 Cache Line 的大小；</li>
<li>而如果在单核系统里，该宏定义是空的；</li>
</ul>
<p>针对在同一个 Cache Line 中的共享的数据，如果在多核之间竞争比较严重，为了防止伪共享现象的发生，可以采用上面的宏定义使得变量在 Cache Line 里是对齐的。</p>
<p>举个例子，有下面这个结构体：</p>
<pre><code class="c">struct test &#123;
    int a;
    int b;
&#125;
</code></pre>
<p>结构体里的两个成员变量 a 和 b 在物理内存地址上是连续的，于是它们可能会位于同一个 Cache Line 中</p>
<p>所以，为了防止前面提到的 Cache 伪共享问题，我们可以使用上面介绍的宏定义，将 b 的地址设置为 Cache Line 对齐地址，如下：</p>
<pre><code class="c">struct test &#123;
    int a;
    int b __cacheline_aligned_in_smp;
&#125;
</code></pre>
<p>这样 a 和 b 变量就不会在同一个 Cache Line 中了，所以，<strong>避免 Cache 伪共享实际上是用空间换时间的思想</strong>，浪费一部分 Cache 空间，从而换来性能的提升。</p>
<p>除此之外还可以在结构体中填充一些没有意义的变量，这些变量不会被读写，这样可以减少一个Cache Line中修改操作的数量，从而降低伪共享的概率。</p>
<h3 id="Linux是怎么进行进程调度的"><a href="#Linux是怎么进行进程调度的" class="headerlink" title="Linux是怎么进行进程调度的"></a>Linux是怎么进行进程调度的</h3><p>在 Linux 内核中，进程和线程都是用 <code>task_struct</code> 结构体表示的，区别在于线程的 task_struct 结构体里部分资源是共享了进程已创建的资源，比如内存地址空间、代码段、文件描述符等，所以 Linux 中的线程也被称为轻量级进程，因为线程的 task_struct 相比进程的 task_struct 承载的 资源比较少，因此以「轻」得名。</p>
<p>所以，Linux 内核里的调度器，调度的对象就是 <code>task_struct</code>，接下来我们就把这个数据结构统称为<strong>任务</strong>。</p>
<p>在 Linux 系统中，根据任务的优先级以及响应要求，主要分为两种，其中优先级的数值越小，优先级越高：</p>
<ul>
<li>实时任务，对系统的响应时间要求很高，也就是要尽可能快的执行实时任务，优先级在 <code>0~99</code> 范围内的就算实时任务；</li>
<li>普通任务，响应时间没有很高的要求，优先级在 <code>100~139</code> 范围内都是普通任务级别；</li>
</ul>
<h4 id="调度类"><a href="#调度类" class="headerlink" title="调度类"></a>调度类</h4><p>由于任务有优先级之分，Linux 系统为了保障高优先级的任务能够尽可能早的被执行，于是分为了这几种调度类，如下图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E4%BC%AA%E5%85%B1%E4%BA%AB/%E8%B0%83%E5%BA%A6%E7%B1%BB.png" alt="img"></p>
<p>Deadline 和 Realtime 这两个调度类，都是应用于实时任务的，这两个调度类的调度策略合起来共有这三种，它们的作用如下：</p>
<ul>
<li><em>SCHED_DEADLINE</em>：是按照 deadline 进行调度的，距离当前时间点最近的 deadline 的任务会被优先调度；</li>
<li><em>SCHED_FIFO</em>：对于相同优先级的任务，按先来先服务的原则，但是优先级更高的任务，可以抢占低优先级的任务，也就是优先级高的可以「插队」；</li>
<li><em>SCHED_RR</em>：对于相同优先级的任务，轮流着运行，每个任务都有一定的时间片，当用完时间片的任务会被放到队列尾部，以保证相同优先级任务的公平性，但是高优先级的任务依然可以抢占低优先级的任务；</li>
</ul>
<p>而 Fair 调度类是应用于普通任务，都是由 CFS 调度器管理的，分为两种调度策略：</p>
<ul>
<li><em>SCHED_NORMAL</em>：普通任务使用的调度策略；</li>
<li><em>SCHED_BATCH</em>：后台任务的调度策略，不和终端进行交互，因此在不影响其他需要交互的任务，可以适当降低它的优先级。</li>
</ul>
<h4 id="CPU-运行队列"><a href="#CPU-运行队列" class="headerlink" title="CPU 运行队列"></a>CPU 运行队列</h4><p>一个系统通常都会运行着很多任务，多任务的数量基本都是远超 CPU 核心数量，因此这时候就需要<strong>排队</strong>。</p>
<p>事实上，每个 CPU 都有自己的<strong>运行队列（Run Queue, rq）</strong>，用于描述在此 CPU 上所运行的所有进程，其队列包含三个运行队列，Deadline 运行队列 dl_rq、实时任务运行队列 rt_rq 和 CFS 运行队列 csf_rq，<strong>其中 csf_rq 是用红黑树来描述的</strong>，按 vruntime 大小来排序的，最左侧的叶子节点，就是下次会被调度的任务。</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/CPU_queue.png" alt="CPU_queue"></p>
<p>这几种调度类是有优先级的，优先级如下：Deadline &gt; Realtime &gt; Fair，这意味着 Linux 选择下一个任务执行的时候，会按照此优先级顺序进行选择，也就是说先从 <code>dl_rq</code> 里选择任务，然后从 <code>rt_rq</code> 里选择任务，最后从 <code>csf_rq</code> 里选择任务。因此，<strong>实时任务总是会比普通任务优先被执行</strong>。</p>
<h4 id="完全公平调度"><a href="#完全公平调度" class="headerlink" title="完全公平调度"></a>完全公平调度</h4><p>我们平日里遇到的基本都是普通任务，对于普通任务来说，公平性最重要，在 Linux 里面，实现了一个基于 CFS 的调度算法，也就是<strong>完全公平调度（Completely Fair Scheduling）</strong>。</p>
<p>这个算法的理念是想让分配给每个任务的 CPU 时间是一样，于是它为每个任务安排一个虚拟运行时间 vruntime，如果一个任务在运行，其运行的越久，该任务的 vruntime 自然就会越大，而没有被运行的任务，vruntime 是不会变化的。</p>
<p>那么，<strong>在 CFS 算法调度的时候，会优先选择 vruntime 少的任务</strong>，以保证每个任务的公平性。</p>
<p>这就好比，让你把一桶的奶茶平均分到 10 杯奶茶杯里，你看着哪杯奶茶少，就多倒一些；哪个多了，就先不倒，这样经过多轮操作，虽然不能保证每杯奶茶完全一样多，但至少是公平的。</p>
<p>当然，上面提到的例子没有考虑到优先级的问题，虽然是普通任务，但是普通任务之间还是有优先级区分的，所以在计算虚拟运行时间 vruntime 还要考虑普通任务的<strong>权重值</strong>，注意权重值并不是优先级的值，内核中会有一个 nice 级别与权重值的转换表，nice 级别越低的权重值就越大，至于 nice 值是什么，我们后面会提到。 于是就有了以下这个公式：</p>
<p>在「同样的实际运行时间」里，高权重任务的 vruntime 比低权重任务的 vruntime <strong>少</strong>， CFS 调度会优先选择 vruntime 少的任务进行调度，所以高权重的任务就会被优先调度了，于是高权重的获得的实际运行时间自然就多了。</p>
<h4 id="调整优先级"><a href="#调整优先级" class="headerlink" title="调整优先级"></a>调整优先级</h4><p>如果我们启动任务的时候，没有特意去指定优先级的话，默认情况下都是普通任务，普通任务的调度类是 Fair，由 CFS 调度器来进行管理。CFS 调度器的目的是实现任务运行的公平性，也就是保障每个任务的运行的时间是差不多的。</p>
<p>如果你想让某个普通任务有更多的执行时间，可以调整任务的 <code>nice</code> 值，从而让优先级高一些的任务执行更多时间。nice 的值能设置的范围是 <code>-20～19</code>， 值越低，表明优先级越高，因此 -20 是最高优先级，19 则是最低优先级，默认优先级是 0。</p>
<p>是不是觉得 nice 值的范围很诡异？事实上，nice 值并不是表示优先级，而是表示优先级的修正数值，它与优先级（priority）的关系是这样的：priority(new) &#x3D; priority(old) + nice。内核中，priority 的范围是 0<del>139，值越低，优先级越高，其中前面的 0</del>99 范围是提供给实时任务使用的，而 nice 值是映射到 100~139，这个范围是提供给普通任务用的，因此 nice 值调整的是普通任务的优先级。</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/priority.png" alt="priority"></p>
<p>我们可以在启动任务的时候，可以指定 nice 的值，比如将 mysqld 以 -3 优先级：</p>
<pre><code class="shell">$ nice -n -3 /usr/sbin/mysqld
</code></pre>
<p>如果想修改已经运行中的任务的优先级，则可以使用 <code>renice</code> 来调整 nice 值：</p>
<pre><code class="shell">$ renice -10 -p &lt;进程pid&gt;
</code></pre>
<p>nice 调整的是普通任务的优先级，所以不管怎么缩小 nice 值，任务永远都是普通任务，如果某些任务要求实时性比较高，那么你可以考虑改变任务的优先级以及调度策略，使得它变成实时任务，比如：</p>
<pre><code class="shell"># 修改调度策略为 SCHED_FIFO，并且优先级为1
$ chrt -f 1 -p 1996
</code></pre>
<h3 id="中断和软中断"><a href="#中断和软中断" class="headerlink" title="中断和软中断"></a>中断和软中断</h3><p>在计算机中，中断是系统用来响应硬件设备请求的一种机制，操作系统收到硬件的中断请求，会打断正在执行的进程，然后调用内核中的中断处理程序来响应请求。</p>
<p>中断是一种异步的事件处理机制，可以提高系统的并发处理能力。</p>
<p>操作系统收到了中断请求，会打断其他进程的运行，所以<strong>中断请求的响应程序，也就是中断处理程序，要尽可能快的执行完，这样可以减少对正常进程运行调度地影响。</strong></p>
<p>而且，中断处理程序在响应中断时，可能还会「临时关闭中断」，这意味着，如果当前中断处理程序没有执行完之前，系统中其他的中断请求都无法被响应，也就说<strong>中断有可能会丢失</strong>，所以<strong>中断处理程序要短且快</strong>。</p>
<h4 id="什么是软中断？"><a href="#什么是软中断？" class="headerlink" title="什么是软中断？"></a>什么是软中断？</h4><p>前面我们也提到了，中断请求的处理程序应该要短且快，这样才能减少对正常进程运行调度地影响，而且中断处理程序可能会暂时关闭中断，这时如果中断处理程序执行时间过长，可能在还未执行完中断处理程序前，会丢失当前其他设备的中断请求。</p>
<p>那 Linux 系统<strong>为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是「上半部和下半部分」</strong>。</p>
<ul>
<li><strong>上半部，对应硬中断，由硬件触发，用来快速处理中断</strong>，一般会暂时关闭中断请求，主要负责处理跟硬件紧密相关或者时间敏感的事情。</li>
<li><strong>下半部，对应软中断，由内核触发，用来延迟处理上半部未完成的工作</strong>，一般以「内核线程」的方式运行。</li>
</ul>
<p>举一个计算机中的例子，常见的网卡接收网络包的例子。</p>
<p>网卡收到网络包后，会通过<strong>硬件中断</strong>通知内核有新的数据到了，于是内核就会调用对应的中断处理程序来响应该事件，这个事件的处理也是会分成上半部和下半部。</p>
<p>上部分要做到快速处理，所以只要把网卡的数据读到内存中，然后更新一下硬件寄存器的状态，比如把状态更新为表示数据已经读到内存中的状态值。</p>
<p>接着，内核会触发一个<strong>软中断</strong>，把一些处理比较耗时且复杂的事情，交给「软中断处理程序」去做，也就是中断的下半部，其主要是需要从内存中找到网络数据，再按照网络协议栈，对网络数据进行逐层解析和处理，最后把数据送给应用程序。</p>
<p>所以，中断处理程序的上部分和下半部可以理解为：</p>
<ul>
<li><strong>上半部直接处理硬件请求，也就是硬中断</strong>，主要是负责耗时短的工作，特点是快速执行；</li>
<li><strong>下半部是由内核触发，也就说软中断</strong>，主要是负责上半部未完成的工作，通常都是耗时比较长的事情，特点是延迟执行；</li>
</ul>
<p>还有一个区别，硬中断（上半部）是会打断 CPU 正在执行的任务，然后立即执行中断处理程序，而软中断（下半部）是以内核线程的方式执行，并且每一个 CPU 都对应一个软中断内核线程，名字通常为「ksoftirqd&#x2F;CPU 编号」，比如 0 号 CPU 对应的软中断内核线程的名字是 <code>ksoftirqd/0</code></p>
<p>不过，软中断不只是包括硬件设备中断处理程序的下半部，一些内核自定义事件也属于软中断，比如内核调度等、RCU 锁（内核里常用的一种锁）等</p>
<p>Linux 中的软中断包括网络收发、定时、调度、RCU 锁等各种类型，可以通过查看 &#x2F;proc&#x2F;softirqs 来观察软中断的累计中断次数情况，如果要实时查看中断次数的变化率，可以使用 watch -d cat &#x2F;proc&#x2F;softirqs 命令。</p>
<p>每一个 CPU 都有各自的软中断内核线程，我们还可以用 ps 命令来查看内核线程，一般名字在中括号里面到，都认为是内核线程。</p>
<p>如果在 top 命令发现，CPU 在软中断上的使用率比较高，而且 CPU 使用率最高的进程也是软中断 ksoftirqd 的时候，这种一般可以认为系统的开销被软中断占据了。</p>
<p>这时我们就可以分析是哪种软中断类型导致的，一般来说都是因为网络接收软中断导致的，如果是的话，可以用 sar 命令查看是哪个网卡的有大量的网络包接收，再用 tcpdump 抓网络包，做进一步分析该网络包的源头是不是非法地址，如果是就需要考虑防火墙增加规则，如果不是，则考虑硬件升级等。</p>
<h2 id="操作系统结构"><a href="#操作系统结构" class="headerlink" title="操作系统结构"></a>操作系统结构</h2><h3 id="Linux内核和Windows内核的区别"><a href="#Linux内核和Windows内核的区别" class="headerlink" title="Linux内核和Windows内核的区别"></a>Linux内核和Windows内核的区别</h3><p>什么是内核呢？</p>
<p>计算机是由各种外部硬件设备组成的，比如内存、cpu、硬盘等，如果每个应用都要和这些硬件设备对接通信协议，那这样太累了，所以这个中间人就由内核来负责，<strong>让内核作为应用连接硬件设备的桥梁</strong>，应用程序只需关心与内核交互，不用关心硬件的细节。</p>
<p>现代操作系统，内核一般会提供 4 个基本能力：</p>
<ul>
<li>管理进程、线程，决定哪个进程、线程使用 CPU，也就是进程调度的能力；</li>
<li>管理内存，决定内存的分配和回收，也就是内存管理的能力；</li>
<li>管理硬件设备，为进程与硬件设备之间提供通信能力，也就是硬件通信能力；</li>
<li>提供系统调用，如果应用程序要运行更高权限运行的服务，那么就需要有系统调用，它是用户程序与操作系统之间的接口。</li>
</ul>
<p>内核是怎么工作的？</p>
<p>内核具有很高的权限，可以控制 cpu、内存、硬盘等硬件，而应用程序具有的权限很小，因此大多数操作系统，把内存分成了两个区域：</p>
<ul>
<li>内核空间，这个内存空间只有内核程序可以访问；</li>
<li>用户空间，这个内存空间专门给应用程序使用；</li>
</ul>
<p>用户空间的代码只能访问一个局部的内存空间，而内核空间的代码可以访问所有内存空间。因此，当程序使用用户空间时，我们常说该程序在<strong>用户态</strong>执行，而当程序使内核空间时，程序则在<strong>内核态</strong>执行。</p>
<p>应用程序如果需要进入内核空间，就需要通过系统调用，下面来看看系统调用的过程：</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/systemcall.png"></p>
<p>内核程序执行在内核态，用户程序执行在用户态。当应用程序使用系统调用时，会产生一个中断。发生中断后， CPU 会中断当前在执行的用户程序，转而跳转到中断处理程序，也就是开始执行内核程序。内核处理完后，主动触发中断，把 CPU 执行权限交回给用户程序，回到用户态继续工作。</p>
<h4 id="Linux的设计"><a href="#Linux的设计" class="headerlink" title="Linux的设计"></a>Linux的设计</h4><p>Linux 内核设计的理念主要有这几个点：</p>
<ul>
<li><em>MultiTask</em>，多任务</li>
<li><em>SMP</em>，对称多处理</li>
<li><em>ELF</em>，可执行文件链接格式</li>
<li><em>Monolithic Kernel</em>，宏内核</li>
</ul>
<p><strong>ultiTask</strong></p>
<p>MultiTask 的意思是<strong>多任务</strong>，代表着 Linux 是一个多任务的操作系统。</p>
<p>多任务意味着可以有多个任务同时执行，这里的「同时」可以是并发或并行：</p>
<ul>
<li>对于单核 CPU 时，可以让每个任务执行一小段时间，时间到就切换另外一个任务，从宏观角度看，一段时间内执行了多个任务，这被称为并发。</li>
<li>对于多核 CPU 时，多个任务可以同时被不同核心的 CPU 同时执行，这被称为并行。</li>
</ul>
<p><strong>SMP</strong></p>
<p>SMP 的意思是<strong>对称多处理</strong>，代表着每个 CPU 的地位是相等的，对资源的使用权限也是相同的，多个 CPU 共享同一个内存，每个 CPU 都可以访问完整的内存和硬件资源。</p>
<p>这个特点决定了 Linux 操作系统不会有某个 CPU 单独服务应用程序或内核程序，而是每个程序都可以被分配到任意一个 CPU 上被执行。</p>
<p><strong>ELF</strong></p>
<p>ELF 的意思是<strong>可执行文件链接格式</strong>，它是 Linux 操作系统中可执行文件的存储格式，你可以从下图看到它的结构：</p>
<h5 id="为什么栈地址从高到低，堆从低到高"><a href="#为什么栈地址从高到低，堆从低到高" class="headerlink" title="为什么栈地址从高到低，堆从低到高"></a><strong>为什么栈地址从高到低，堆从低到高</strong></h5><p><strong>这样设计可以使得堆和栈能够充分利用空闲的地址空间。</strong>如果栈向上涨的话，我们就必须得指定栈和堆的一个严格分界线，但这个分界线怎么确定呢？平均分？但是有的程序使用的堆空间比较多，而有的程序使用的栈空间比较多。所以就可能出现这种情况：一个程序因为栈溢出而崩溃的时候，其实它还有大量闲置的堆空间，但是我们却无法使用这些闲置的堆空间。所以呢，最好的办法就是让堆和栈一个向上涨，一个向下涨，这样它们就可以最大程度地共用这块剩余的地址空间，达到利用率的最大化。</p>
<p><strong>Monolithic Kernel</strong></p>
<p>Monolithic Kernel 的意思是<strong>宏内核</strong>，Linux 内核架构就是宏内核，意味着 <strong>Linux 的内核是一个完整的可执行程序，</strong>且拥有最高的权限。</p>
<p>宏内核的特征是系统内核的所有模块，比如进程调度、内存管理、文件系统、设备驱动等，都运行在内核态。</p>
<p>不过，Linux 也实现了动态加载内核模块的功能，例如大部分设备驱动是以可加载模块的形式存在的，与内核其他模块解藕，让驱动开发和驱动加载更为方便、灵活。</p>
<p>与宏内核相反的是<strong>微内核</strong>，微内核架构的内核只保留最基本的能力，比如进程调度、虚拟机内存、中断等，把一些应用放到了用户空间，比如驱动程序、文件系统等。这样服务与服务之间是隔离的，单个服务出现故障或者完全攻击，也不会导致整个操作系统挂掉，提高了操作系统的稳定性和可靠性。</p>
<p>微内核内核功能少，可移植性高，相比宏内核有一点不好的地方在于，由于驱动程序不在内核中，而且驱动程序一般会频繁调用底层能力的，于是驱动和硬件设备交互就需要频繁切换到内核态，这样会带来性能损耗。华为的鸿蒙操作系统的内核架构就是微内核。</p>
<p>还有一种内核叫<strong>混合类型内核</strong>，它的架构有点像微内核，内核里面会有一个最小版本的内核，然后其他模块会在这个基础上搭建，然后实现的时候会跟宏内核类似，也就是把整个内核做成一个完整的程序，大部分服务都在内核中，这就像是宏内核的方式包裹着一个微内核。</p>
<p><img src="https://pic3.zhimg.com/80/v2-9988d404585168775f779194ec3a53a6_720w.jpg" alt="img"></p>
<h4 id="Windows的设计"><a href="#Windows的设计" class="headerlink" title="Windows的设计"></a>Windows的设计</h4><p>当今 Windows 7、Windows 10 使用的内核叫 Windows NT，NT 全称叫 New Technology。</p>
<p>下图是 Windows NT 的结构图片：</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/windowNT.png" alt="Windows NT 的结构"></p>
<p>Windows 和 Linux 一样，同样支持 MultiTask 和 SMP，但不同的是，<strong>Window 的内核设计是混合型内核</strong>，在上图你可以看到内核中有一个 <em>MicroKernel</em> 模块，这个就是最小版本的内核，而整个内核实现是一个完整的程序，含有非常多模块。</p>
<p>Windows 的可执行文件的格式与 Linux 也不同，所以这两个系统的可执行文件是不可以在对方上运行的。</p>
<p>Windows 的可执行文件格式叫 PE，称为<strong>可移植执行文件</strong>，扩展名通常是<code>.exe</code>、<code>.dll</code>、<code>.sys</code>等。</p>
<h3 id="操作系统概念"><a href="#操作系统概念" class="headerlink" title="操作系统概念"></a>操作系统概念</h3><p><img src="https://raw.githubusercontent.com/hufei96/Image/main/image-20220512235848305.png" alt="image-20220512235848305"></p>
<p>操作系统定义：给应用程序提供服务的程序。现代操作系统主要是因为<strong>更好的管理和支持多任务执行</strong>的需求而诞生的。</p>
<p>如果计算机没有操作系统，那么它将不支持多任务的执行。如单片机基本没有操作系统，所以单片机只支持执行单个程序。</p>
<p>程序并没有直接访问cpu，内存，I&#x2F;O这些硬件设备，真正访问硬件设备的是操作系统。所有程序对硬件的操作都必须通过操作系统来完成。它可以看成是应用程序和硬件之间的一层软件，给程序员提供硬件的抽象。</p>
<p>这样设计的目的有两个：</p>
<ol>
<li>防止硬件被失控的应用程序滥用，提高系统安全性</li>
<li>提供统一对硬件操作的接口，简化硬件操作</li>
</ol>
<p>为了实现这些功能操作系统引入了几个抽象的概念，<strong>抽象就是对外提供统一接口而隐藏内部复杂的细节</strong>，操作系统对于程序来说就是硬件的抽象。</p>
<p>操作系统将cpu，内存，I&#x2F;O设备抽象为进程；将内存和磁盘I&#x2F;O抽象为虚拟内存；将I&#x2F;O设备抽象为文件的形式。让程序员能够直接通过这层软件很好地调用硬件，避免了过多的硬件细节。</p>
<h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2><h3 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h3><h4 id="什么是虚拟内存"><a href="#什么是虚拟内存" class="headerlink" title="什么是虚拟内存"></a>什么是虚拟内存</h4><p>操作系统为每个进程分配独立的一套「<strong>虚拟地址</strong>」，人人都有，互不干涉。每个进程都不能访问物理地址，虚拟地址最终怎么落到物理内存里对进程来说是透明的，操作系统已经把这些都安排的明明白白了。</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/image-20220513215415380.png" alt="image-20220513215415380"></p>
<p><strong>操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。</strong></p>
<p>如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样不同的进程运行的时候，写入的是不同的物理地址，这样就不会冲突了。</p>
<p>于是，这里就引出了两种地址的概念：</p>
<ul>
<li>我们程序所使用的内存地址叫做<strong>虚拟内存地址</strong>（<em>Virtual Memory Address</em>）</li>
<li>实际存在硬件里面的空间地址叫<strong>物理内存地址</strong>（<em>Physical Memory Address</em>）。</li>
</ul>
<p>操作系统引入了虚拟内存，进程持有的虚拟地址会通过 CPU 芯片中的内存管理单元（MMU）的映射关系，来转换变成物理地址，然后再通过物理地址访问内存。</p>
<h4 id="为什么需要虚拟内存"><a href="#为什么需要虚拟内存" class="headerlink" title="为什么需要虚拟内存"></a>为什么需要虚拟内存</h4><p>解决多程序系统中内存分配和管理问题。主要为了隔离地址空间，如果程序可以直接操作物理地址(如单片机)，那么计算机无法同时运行多个程序，因为程序A可能会擦掉程序B的数据，这会导致程序B崩溃。</p>
<ol>
<li><strong>解决地址空间不隔离问题</strong>：若所有程序都直接访问物理地址，程序所使用的内存空间不是相互隔离的。程序可以很容易改写其他程序的内存数据，以达到破坏的目的，这对于需要安全稳定的计算环境的用户来说是不能容忍的。用户希望他在使用计算机的时候，其中一个任务失败了，至少不会影响其他任务。</li>
<li><strong>解决内存使用效率低问题：</strong>如果没有有效的内存管理机制，通常需要一个程序执行时，监控程序就将整个程序装入内存中然后开始执行。而当内存没有程序所需的连续空间时，就不得不将其他程序换出到磁盘，效率十分低下。</li>
<li><strong>解决程序运行地址不确定问题：</strong>程序每次运行时都要给它在内存中分配一块空闲空间，这个空间位置不确定。这会导致程序编写十分困难，因为程序中访问数据和指令跳转的目标地址很多都是固定的。</li>
<li><strong>虚拟内存可以使得进程对运行内存超过物理内存大小</strong>，因为程序运行符合局部性原理，CPU 访问内存会有很明显的重复访问的倾向性，对于那些没有被经常使用到的内存，我们可以把它换出到物理内存之外，比如硬盘上的 swap 区域。</li>
<li>页表里的页表项中除了物理地址之外，还有一些标记属性的比特，比如控制一个页的读写权限，标记该页是否存在等。在内存访问方面，操作系统提供了更好的安全性。</li>
</ol>
<p>虚拟内存屏蔽了虚拟地址到物理地址的转换过程，让程序看起来就像是在独占地使用内存。</p>
<h3 id="管理虚拟地址和物理地址的映射"><a href="#管理虚拟地址和物理地址的映射" class="headerlink" title="管理虚拟地址和物理地址的映射"></a>管理虚拟地址和物理地址的映射</h3><h4 id="内存分段"><a href="#内存分段" class="headerlink" title="内存分段"></a>内存分段</h4><h5 id="映射机制"><a href="#映射机制" class="headerlink" title="映射机制"></a>映射机制</h5><p>程序是由若干个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。<strong>不同的段是有不同的属性的，所以就用分段（Segmentation）的形式把这些段分离出来。</strong></p>
<p>分段机制下的虚拟地址由两部分组成，<strong>段选择因子</strong>和<strong>段内偏移量</strong>。</p>
<p><img src="https://img-blog.csdnimg.cn/a9ed979e2ed8414f9828767592aadc21.png" alt="img"></p>
<p>段选择因子和段内偏移量：</p>
<ul>
<li><strong>段选择因子</strong>就保存在段寄存器里面。段选择因子里面最重要的是<strong>段号</strong>，用作段表的索引。<strong>段表</strong>里面保存的是这个<strong>段的基地址、段的界限和特权等级</strong>等。</li>
<li>虚拟地址中的<strong>段内偏移量</strong>应该位于 0 和段界限之间，如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。</li>
</ul>
<p>在上面，知道了虚拟地址是通过<strong>段表</strong>与物理地址进行映射的，分段机制会把程序的虚拟地址分成多个段(<strong>分段的数量跟段寄存器相关，跟虚拟空间的段无关，图片仅为假设</strong>)，每个段在段表中有一个项，在这一项找到段的基地址，再加上偏移量，于是就能找到物理内存中的地址，如下图：</p>
<p><img src="https://img-blog.csdnimg.cn/c5e2ab63e6ee4c8db575f3c7c9c85962.png" alt="img"></p>
<h5 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h5><p>分段的办法很好，解决了程序本身不需要关心具体的物理内存地址的问题，但它也有一些不足之处：</p>
<ul>
<li>第一个就是<strong>内存碎片</strong>的问题。</li>
<li>第二个就是<strong>内存交换的效率低</strong>的问题。</li>
</ul>
<p><strong>内存碎片</strong></p>
<p>内存碎片主要分为，内部内存碎片和外部内存碎片。</p>
<p>内存分段管理可以做到段根据实际需求分配内存(<strong>除了堆区，需要分配的空间大小在编译时已经确定</strong>)，所以有多少需求就分配多大的段，可以减少内部内存碎片(堆区无法避免内部碎片)。</p>
<p>但是由于每个段的长度不固定，所以多个段未必能恰好使用所有的内存空间，会产生了多个不连续的小物理内存，导致新的程序无法被装载，所以<strong>会出现外部内存碎片</strong>的问题。</p>
<p>解决「外部内存碎片」的问题就是<strong>内存交换</strong>。</p>
<p>可以把内存碎片写到硬盘上，然后再从硬盘上读回来到内存里。不过再读回的时候，我们不能装载回原来的位置，而是紧紧跟着连续的空间。这样就能将碎片拼接起来，从而空出大片的连续空间，于是新的程序就可以装载进来。</p>
<p>这个内存交换空间，在 Linux 系统里，也就是我们常看到的 Swap 空间，这块空间是从硬盘划分出来的，用于内存与硬盘的空间交换。</p>
<p><strong>内存交换的效率低</strong></p>
<p>对于多进程的系统来说，用分段的方式，外部内存碎片是很容易产生的，产生了外部内存碎片，那不得不重新 <code>Swap</code> 内存区域，这个过程会产生性能瓶颈。</p>
<p>因为硬盘的访问速度要比内存慢太多了，每一次内存交换，我们都需要把一大段连续的内存数据写到硬盘上。</p>
<p>所以，<strong>如果内存交换的时候，交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。</strong></p>
<p>为了解决内存分段的「外部内存碎片和内存交换效率低」的问题，就出现了内存分页。</p>
<h4 id="内存分页"><a href="#内存分页" class="headerlink" title="内存分页"></a>内存分页</h4><h5 id="映射机制-1"><a href="#映射机制-1" class="headerlink" title="映射机制"></a>映射机制</h5><p>分段的好处就是能产生连续的内存空间，但是会出现「外部内存碎片和内存交换的空间太大」的问题。</p>
<p>要解决这些问题，那么就要想出能少出现一些内存碎片的办法。另外，当需要进行内存交换的时候，让需要交换写入或者从磁盘装载的数据更少一点，这样就可以解决问题了。这个办法，也就是<strong>内存分页</strong>（<em>Paging</em>）。</p>
<p><strong>分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小</strong>。这样一个连续并且尺寸固定的内存空间，我们叫<strong>页</strong>（<em>Page</em>）。在 Linux 下，每一页的大小为 <code>4KB</code>。</p>
<p>虚拟地址与物理地址之间通过<strong>页表</strong>来映射，页表是存储在内存里的，<strong>内存管理单元</strong> （<em>MMU</em>）就做将虚拟内存地址转换成物理地址的工作。而当进程访问的虚拟地址没有装入物理内存时，系统会产生一个<strong>缺页异常</strong>，进入系统内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。</p>
<p><strong>页表是一个有序数组，占据连续的内存空间</strong>，页表的空间必须预先分配并且按顺序初始化（也就是<strong>页表必须覆盖全部的虚拟内存地址空间</strong>），这主要是利用数组可以随机访问的特性，使得可以在O(1)时间内找到虚拟地址对应的页表项。如果使用其他的映射方法，比如链表之类的数据结构，遍历链表查找页表位置会耗费巨大性能。</p>
<p>虚拟地址分为两部分，<strong>页号</strong>和<strong>页内偏移</strong>。页号作为页表的索引，<strong>页表</strong>包含物理页每页所在<strong>物理内存的基地址</strong>，这个基地址与页内偏移的组合就形成了物理内存地址，见下图。</p>
<p><img src="https://img-blog.csdnimg.cn/7884f4d8db4949f7a5bb4bbd0f452609.png" alt="img"></p>
<p>总结一下，对于一个内存地址转换，其实就是这样三个步骤：</p>
<ul>
<li>把虚拟内存地址，切分成页号和偏移量；</li>
<li>根据页号，从页表里面，查询对应的物理页号；</li>
<li>直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。</li>
</ul>
<p>下面举个例子，虚拟内存中的页通过页表映射为了物理内存中的页，如下图：</p>
<p><img src="https://img-blog.csdnimg.cn/8f187878c809414ca2486b0b71e8880e.png" alt="img"></p>
<h5 id="跟分段相比区别"><a href="#跟分段相比区别" class="headerlink" title="跟分段相比区别"></a>跟分段相比区别</h5><p>内存分页由于内存空间都是预先划分好的，也就不会像内存分段一样，在段与段之间会产生间隙非常小的内存，这正是分段会产生外部内存碎片的原因。而<strong>采用了分页，页与页之间是紧密排列的，所以不会有外部碎片。</strong></p>
<p>但是，因为内存分页机制分配内存的最小单位是一页，即使程序不足一页大小，我们最少只能分配一个页，所以页内会出现内存浪费，所以针对<strong>内存分页机制会有内部内存碎片</strong>的现象。</p>
<p>如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉，也就是暂时写在硬盘上，称为<strong>换出</strong>（<em>Swap Out</em>）。一旦需要的时候，再加载进来，称为<strong>换入</strong>（<em>Swap In</em>）。所以，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，<strong>内存交换的效率就相对比较高。</strong></p>
<p>更进一步地，分页的方式使得我们在加载程序的时候，不再需要一次性都把程序加载到物理内存中。我们完全可以在进行虚拟内存和物理内存的页之间的映射之后，并不真的把页加载到物理内存里，而是<strong>只有在程序运行中，需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。</strong></p>
<h5 id="缺陷-1"><a href="#缺陷-1" class="headerlink" title="缺陷"></a>缺陷</h5><p>有空间上的缺陷。</p>
<p>因为操作系统是可以同时运行非常多的进程的，那这不就意味着页表会非常的庞大。</p>
<p>在 32 位的环境下，虚拟地址空间共有 4GB，假设一个页的大小是 4KB（2^12），那么就需要大约 100 万 （2^20） 个页，每个「页表项」需要 4 个字节大小来存储，那么整个 4GB 空间的映射就需要有 <code>4MB</code> 的内存来存储页表。</p>
<p>这 4MB 大小的页表，看起来也不是很大。但是<strong>页表需要连续空间</strong>，连续的4MB就不是一个小数字了。同时每个进程都有自己的虚拟地址空间的，也就说都有自己的页表。</p>
<p>那么，<code>100</code> 个进程的话，就需要 <code>400MB</code> 的内存来存储页表，这是非常大的内存了，更别说 64 位的环境了。</p>
<h5 id="多级页表"><a href="#多级页表" class="headerlink" title="多级页表"></a>多级页表</h5><p>要解决上面的问题，就需要采用一种叫作<strong>多级页表</strong>（<em>Multi-Level Page Table</em>）的解决方案。</p>
<p>在前面我们知道了，对于单页表的实现方式，在 32 位和页大小 <code>4KB</code> 的环境下，一个进程的页表需要装下 100 多万个「页表项」，并且每个页表项是占用 4 字节大小的，于是相当于每个页表需占用 4MB 大小的空间。</p>
<p>我们把这个 100 多万个「页表项」的单级页表再分页，将页表（一级页表）分为 <code>1024</code> 个页表（二级页表），每个表（二级页表）中包含 <code>1024</code> 个「页表项」，形成<strong>二级分页</strong>。如下图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/19296e249b2240c29f9c52be70f611d5.png" alt="img"></p>
<blockquote>
<p>你可能会问，分了二级表，映射 4GB 地址空间就需要 4KB（一级页表）+ 4MB（二级页表）的内存，这样占用空间不是更大了吗？</p>
</blockquote>
<p>当然如果 4GB 的虚拟地址全部都映射到了物理内存上的话，二级分页占用空间确实是更大了，但是，我们往往不会为一个进程分配那么多内存。</p>
<p>其实我们应该换个角度来看问题，还记得计算机组成原理里面无处不在的<strong>局部性原理</strong>么？</p>
<p>每个进程都有 4GB 的虚拟地址空间，而显然对于大多数程序来说，其使用到的空间远未达到 4GB，因为会存在部分对应的页表项都是空的，根本没有分配，对于已分配的页表项，如果存在最近一定时间未访问的页表，在物理内存紧张的情况下，操作系统会将页面换出到硬盘，也就是说不会占用物理内存。</p>
<p>如果使用了二级分页，一级页表就可以覆盖整个 4GB 虚拟地址空间，但<strong>如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表</strong>。做个简单的计算，假设只有 20% 的一级页表项被用到了，那么页表占用的内存空间就只有 4KB（一级页表） + 20% * 4MB（二级页表）&#x3D; <code>0.804MB</code>，这对比单级页表的 <code>4MB</code> 是不是一个巨大的节约？</p>
<p>那么为什么不分级的页表就做不到这样节约内存呢？</p>
<p><strong>页表是一个有序数组，占据连续的内存空间</strong>，页表的空间必须预先分配并且按顺序初始化。所以<strong>页表一定要覆盖全部虚拟地址空间，不分级的页表就需要有 100 多万个页表项来映射，而二级分页则只需要 1024 个页表项</strong>（此时一级页表覆盖到了全部虚拟地址空间，二级页表在需要时创建）。</p>
<p>我们把二级分页再推广到多级页表，就会发现页表占用的内存空间更少了，这一切都要归功于对局部性原理的充分应用。</p>
<p>对于 64 位的系统，两级分页肯定不够了，就变成了四级目录，分别是：</p>
<ul>
<li>全局页目录项 PGD（<em>Page Global Directory</em>）；</li>
<li>上层页目录项 PUD（<em>Page Upper Directory</em>）；</li>
<li>中间页目录项 PMD（<em>Page Middle Directory</em>）；</li>
<li>页表项 PTE（<em>Page Table Entry</em>）；</li>
</ul>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/%E5%9B%9B%E7%BA%A7%E5%88%86%E9%A1%B5.png" alt="img"></p>
<h5 id="TLB"><a href="#TLB" class="headerlink" title="TLB"></a>TLB</h5><p>多级页表虽然解决了空间上的问题，但是虚拟地址到物理地址的转换就多了几道转换的工序，这显然就降低了这俩地址转换的速度，也就是带来了时间上的开销。</p>
<p>程序是有局部性的，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域。</p>
<p>我们就可以利用这一特性，把最常访问的几个页表项存储到访问速度更快的硬件，于是计算机科学家们，就在 CPU 芯片中，加入了一个专门存放程序最常访问的页表项的 Cache，这个 Cache 就是 TLB（<em>Translation Lookaside Buffer</em>） ，通常称为页表缓存、转址旁路缓存、快表等。</p>
<p>在 CPU 芯片里面，封装了内存管理单元（<em>Memory Management Unit</em>）芯片，它用来完成地址转换和 TLB 的访问与交互。</p>
<p>有了 TLB 后，那么 CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的页表。</p>
<p>TLB 的命中率其实是很高的，因为程序最常访问的页就那么几个。</p>
<h4 id="段页式内存管理"><a href="#段页式内存管理" class="headerlink" title="段页式内存管理"></a>段页式内存管理</h4><p>内存分段和内存分页并不是对立的，它们是可以组合起来在同一个系统中使用的，那么组合起来后，通常称为<strong>段页式内存管理</strong>。</p>
<p>段页式内存管理实现的方式：</p>
<ul>
<li>先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制；</li>
<li>接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页；</li>
</ul>
<p>这样，地址结构就由<strong>段号、段内页号和页内位移</strong>三部分组成。</p>
<p>用于段页式地址变换的数据结构是每一个程序一张段表，每个段又建立一张页表，段表中的地址是页表的起始地址，而页表中的地址则为某页的物理页号，如图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/8904fb89ae0c49c4b0f2f7b5a0a7b099.png" alt="img"></p>
<p>段页式地址变换中要得到物理地址须经过三次内存访问：</p>
<ul>
<li>第一次访问段表，得到页表起始地址；</li>
<li>第二次访问页表，得到物理页号；</li>
<li>第三次将物理页号与页内位移组合，得到物理地址。</li>
</ul>
<p>可用软、硬件相结合的方法实现段页式地址变换，这样虽然增加了硬件成本和系统开销，但提高了内存的利用率。</p>
<h4 id="Linux内存管理机制"><a href="#Linux内存管理机制" class="headerlink" title="Linux内存管理机制"></a>Linux内存管理机制</h4><p>早期 Intel 的处理器从 80286 开始使用的是段式内存管理。但是很快发现，光有段式内存管理而没有页式内存管理是不够的，这会使它的 X86 系列会失去市场的竞争力。因此，在不久以后的 80386 中就实现了页式内存管理。也就是说，80386 除了完成并完善从 80286 开始的段式内存管理的同时还实现了页式内存管理。</p>
<p>但是这个 80386 的页式内存管理设计时，没有绕开段式内存管理，而是建立在段式内存管理的基础上，这就意味着，<strong>页式内存管理的作用是在由段式内存管理所映射而成的地址上再加上一层地址映射。</strong></p>
<p>由于此时由段式内存管理映射而成的地址不再是“物理地址”了，Intel 就称之为“线性地址”（也称虚拟地址）。于是，段式内存管理先将逻辑地址映射成线性地址，然后再由页式内存管理将线性地址映射成物理地址。</p>
<p><img src="https://img-blog.csdnimg.cn/bc0aaaf379fc4bc8882efd94b9052b64.png" alt="img"></p>
<p><strong>Linux 系统主要采用了分页管理，但是由于 Intel 处理器的发展史，Linux 系统无法避免分段管理</strong>。于是 Linux 就把所有段的基地址设为 <code>0</code>，也就意味着所有程序的地址空间都是线性地址空间（虚拟地址），相当于屏蔽了 CPU 逻辑地址的概念，所以段只被用于访问控制和内存保护。</p>
<p>另外，Linux 系统中虚拟空间分布可分为<strong>用户态</strong>和<strong>内核态</strong>两部分，其中用户态的分布：代码段、全局变量、BSS、函数栈、堆内存、映射区。</p>
<ul>
<li>进程在用户态时，只能访问用户空间内存；</li>
<li>只有进入内核态后，才可以访问内核空间的内存；</li>
</ul>
<p>虽然每个进程都各自有独立的虚拟内存，但是<strong>每个虚拟内存中的内核地址，其实关联的都是相同的物理内存</strong>。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。</p>
<h3 id="malloc是怎么分配内存的"><a href="#malloc是怎么分配内存的" class="headerlink" title="malloc是怎么分配内存的"></a>malloc是怎么分配内存的</h3><p>实际上，malloc() 并不是系统调用，而是 C 库里的函数，用于动态分配内存。</p>
<p>malloc 申请内存的时候，会有两种方式向操作系统申请堆内存。</p>
<ul>
<li>方式一：通过 brk() 系统调用从堆分配内存；</li>
<li>方式二：通过 mmap() 系统调用在文件映射区域分配内存；</li>
</ul>
<p>方式一实现的方式很简单，就是通过 brk() 函数将「堆顶」指针向高地址移动，获得新的内存空间。如下图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/brk%E7%94%B3%E8%AF%B7.png" alt="img"></p>
<p>方式二通过 mmap() 系统调用中「私有匿名映射」的方式，在文件映射区分配一块内存，也就是从文件映射区“偷”了一块内存。如下图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/mmap%E7%94%B3%E8%AF%B7.png" alt="img"></p>
<blockquote>
<p>什么场景下 malloc() 会通过 brk() 分配内存？又是什么场景下通过 mmap() 分配内存？</p>
</blockquote>
<p>malloc() 源码里默认定义了一个阈值：</p>
<ul>
<li>如果用户分配的内存小于 128 KB，则通过 brk() 申请内存；</li>
<li>如果用户分配的内存大于 128 KB，则通过 mmap() 申请内存；</li>
</ul>
<p>注意，不同的 glibc 版本定义的阈值也是不同的。</p>
<h4 id="malloc-分配的是物理内存吗？"><a href="#malloc-分配的是物理内存吗？" class="headerlink" title="malloc() 分配的是物理内存吗？"></a>malloc() 分配的是物理内存吗？</h4><p>不是的，<strong>malloc() 分配的是虚拟内存</strong>。</p>
<p>如果分配后的虚拟内存没有被访问的话，是不会将虚拟内存不会映射到物理内存，这样就不会占用物理内存了。</p>
<p>只有在访问已分配的虚拟地址空间的时候，操作系统通过查找页表，发现虚拟内存对应的页没有在物理内存中，就会触发缺页中断，然后操作系统会建立虚拟内存和物理内存之间的映射关系。</p>
<h4 id="malloc-1-分配多少内存"><a href="#malloc-1-分配多少内存" class="headerlink" title="malloc(1)分配多少内存"></a>malloc(1)分配多少内存</h4><p>malloc() 在分配内存的时候，并不是老老实实按用户预期申请的字节数来分配内存空间大小，而是<strong>会预分配更大的空间作为内存池</strong>。</p>
<p>具体会预分配多大的空间，跟 malloc 使用的内存管理器有关系。</p>
<h4 id="free-释放内存，会归还给操作系统吗？"><a href="#free-释放内存，会归还给操作系统吗？" class="headerlink" title="free 释放内存，会归还给操作系统吗？"></a>free 释放内存，会归还给操作系统吗？</h4><ul>
<li>malloc 通过 <strong>brk()</strong> 方式申请的内存，free 释放内存的时候，<strong>并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用</strong>；</li>
<li>malloc 通过 <strong>mmap()</strong> 方式申请的内存，free 释放内存的时候，<strong>会把内存归还给操作系统，内存得到真正的释放</strong>。</li>
</ul>
<h4 id="为什么不全部使用-mmap-来分配内存？"><a href="#为什么不全部使用-mmap-来分配内存？" class="headerlink" title="为什么不全部使用 mmap 来分配内存？"></a>为什么不全部使用 mmap 来分配内存？</h4><p>因为向操作系统申请内存，是要通过系统调用的，执行系统调用是要进入内核态的，然后在回到用户态，运行态的切换会耗费不少时间。</p>
<p>所以，申请内存的操作应该避免频繁的系统调用，如果都用 mmap 来分配内存，等于每次都要执行系统调用。</p>
<p>另外，因为 mmap 分配的内存每次释放的时候，都会归还给操作系统，于是每次 mmap 分配的虚拟地址都是缺页状态的，然后在第一次访问该虚拟地址的时候，就会触发缺页中断。</p>
<p>也就是说，<strong>频繁通过 mmap 分配的内存话，不仅每次都会发生运行态的切换，还会发生缺页中断（在第一次访问虚拟地址后），这样会导致 CPU 消耗较大</strong>。</p>
<p>为了改进这两个问题，malloc 通过 brk() 系统调用在堆空间申请内存的时候，由于堆空间是连续的，所以直接预分配更大的内存来作为内存池，当内存释放的时候，就缓存在内存池中。</p>
<p><strong>等下次在申请内存的时候，就直接从内存池取出对应的内存块就行了，而且可能这个内存块的虚拟地址与物理地址的映射关系还存在，这样不仅减少了系统调用的次数，也减少了缺页中断的次数，这将大大降低 CPU 的消耗</strong>。</p>
<h4 id="为什么不全部使用-brk-来分配？"><a href="#为什么不全部使用-brk-来分配？" class="headerlink" title="为什么不全部使用 brk 来分配？"></a>为什么不全部使用 brk 来分配？</h4><p>前面我们提到通过 brk 从堆空间分配的内存，并不会归还给操作系统，那么我们那考虑这样一个场景。</p>
<p>如果我们连续申请了 10k，20k，30k 这三片内存，如果 10k 和 20k 这两片释放了，变为了空闲内存空间，如果下次申请的内存小于 30k，那么就可以重用这个空闲内存空间。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/75edee0cb75450e7987a8a482b975bda.png" alt="图片"></p>
<p>但是如果下次申请的内存大于 30k，没有可用的空闲内存空间，必须向 OS 申请，实际使用内存继续增大。</p>
<p>因此，随着系统频繁地 malloc 和 free ，尤其对于小块内存，堆内将产生越来越多不可用的碎片。</p>
<p>所以，malloc 实现中，充分考虑了 brk 和 mmap 行为上的差异及优缺点，默认分配大块内存 (128KB) 才使用 mmap 分配内存空间。</p>
<h4 id="free-函数只传入一个内存地址，为什么能知道要释放多大的内存？"><a href="#free-函数只传入一个内存地址，为什么能知道要释放多大的内存？" class="headerlink" title="free() 函数只传入一个内存地址，为什么能知道要释放多大的内存？"></a>free() 函数只传入一个内存地址，为什么能知道要释放多大的内存？</h4><p>malloc 返回给用户态的内存起始地址比进程的堆空间起始地址多了 16 字节。</p>
<p>这个多出来的 16 字节就是保存了该内存块的元数据，比如有该内存块的大小。这样当执行 free() 函数时，free 会对传入进来的内存地址向左偏移 16 字节，然后从这个 16 字节的分析出当前的内存块的大小，自然就知道要释放多大的内存了。</p>
<h4 id="ptmalloc、tcmalloc与jemalloc对比分析"><a href="#ptmalloc、tcmalloc与jemalloc对比分析" class="headerlink" title="ptmalloc、tcmalloc与jemalloc对比分析"></a>ptmalloc、tcmalloc与jemalloc对比分析</h4><p>malloc其实就是一个通用的大众货，什么场景下都可以用，<strong>但是什么场景下都可以用就意味着什么场景下都不会有很高的性能</strong>。</p>
<ol>
<li><p>现代计算机内存都很大，你是不是可以牺牲内存利用率为代价换取更高的内存归还&#x2F;重用的效率？同时换取更快的分配速度？或许你会发现，你可以比 libc 的  malloc 平均浪费 30%内存的代价换来两倍以上的性能提升，在一些内存分配成为瓶颈的应用中起到积极的作用。</p>
</li>
<li><p>比如你可以调整大小内存的比值，libc如果认为 8K以下是小内存，那么你可以不那么认为。</p>
</li>
<li><p>比如如果你的系统就是一个单线程的东西，那么你是否能提供开关，完全以单线程的模式进行运作，完全绕过各种锁和针对多核进行的各种冗余操作呢？</p>
</li>
<li><p>比如你的机器内存有限，你应用需要耗费大量的内存，那么你可以引入其他机制，以牺牲少量性能为代价，换取更好的内存回收效果和内存利用率。</p>
</li>
<li><p>最近分配的对象尽量在线性地址上集中在一起，这样缓存命中高，也不易发生缺页。</p>
</li>
<li><p>比如你程序里面某些对象需要被跟踪，你能否直接在分配器上实现对象跟踪机制，跟踪各种泄漏，越界问题？</p>
</li>
<li><p>每个内存分配都在寻求最佳的公平，你在乎的公平是什么？</p>
</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://www.cyningsun.com/07-07-2018/memory-allocator-contrasts.html">ptmalloc、tcmalloc与jemalloc对比分析 (cyningsun.com)</a></p>
<h3 id="内存满了会发生什么"><a href="#内存满了会发生什么" class="headerlink" title="内存满了会发生什么"></a>内存满了会发生什么</h3><h4 id="内存分配过程"><a href="#内存分配过程" class="headerlink" title="内存分配过程"></a>内存分配过程</h4><p>应用程序通过 malloc 函数申请内存的时候，实际上申请的是虚拟内存，此时并不会分配物理内存。</p>
<p>当应用程序读写了这块虚拟内存，CPU 就会去访问这个虚拟内存， 这时会发现这个虚拟内存没有映射到物理内存， CPU 就会产生<strong>缺页中断</strong>，进程会从用户态切换到内核态，并将缺页中断交给内核的 Page Fault Handler （缺页中断函数）处理。</p>
<p>缺页中断处理函数会看是否有空闲的物理内存，如果有，就直接分配物理内存，并建立虚拟内存与物理内存之间的映射关系。</p>
<p>如果没有空闲的物理内存，那么内核就会开始进行<strong>回收内存</strong>的工作，回收的方式主要是两种：直接内存回收和后台内存回收。</p>
<ul>
<li><strong>后台内存回收</strong>（kswapd）：在物理内存紧张的时候，会唤醒 kswapd 内核线程来回收内存，这个回收内存的过程<strong>异步</strong>的，不会阻塞进程的执行。</li>
<li><strong>直接内存回收</strong>（direct reclaim）：如果后台异步回收跟不上进程内存申请的速度，就会开始直接回收，这个回收内存的过程是<strong>同步</strong>的，会阻塞进程的执行。</li>
</ul>
<p>如果直接内存回收后，空闲的物理内存仍然无法满足此次物理内存的申请，那么内核就会放最后的大招了 ——<strong>触发 OOM （Out of Memory）机制</strong>。</p>
<p>OOM Killer 机制会根据算法选择一个占用物理内存较高的进程，然后将其杀死，以便释放内存资源，如果物理内存依然不足，OOM Killer 会继续杀死占用物理内存较高的进程，直到释放足够的内存位置。</p>
<h4 id="哪些内存可以回收"><a href="#哪些内存可以回收" class="headerlink" title="哪些内存可以回收"></a>哪些内存可以回收</h4><p>系统内存紧张的时候，就会进行回收内测的工作，那具体哪些内存是可以被回收的呢？</p>
<p>主要有两类内存可以被回收，而且它们的回收方式也不同。</p>
<ul>
<li><strong>文件页</strong>（File-backed Page）：<strong>文件页指用作缓存的数据</strong>。内核缓存的磁盘数据（Buffer）和内核缓存的文件数据（Cache）都叫作文件页。大部分文件页，都可以直接释放内存，以后有需要时，再从磁盘重新读取就可以了。而那些被应用程序修改过，并且暂时还没写入磁盘的数据（也就是脏页），就得先写入磁盘，然后才能进行内存释放。所以，<strong>回收干净页的方式是直接释放内存，回收脏页的方式是先写回磁盘后再释放内存</strong>。</li>
<li><strong>匿名页</strong>（Anonymous Page）：这部分内存没有实际载体，不像文件缓存有硬盘文件这样一个载体，比如堆、栈数据等。这部分内存很可能还要再次被访问，所以不能直接释放内存，它们<strong>回收的方式是通过 Linux 的 Swap 机制</strong>，Swap 会把不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。</li>
</ul>
<p>文件页和匿名页的回收都是基于 LRU 算法，也就是优先回收不常访问的内存。LRU 回收算法，实际上维护着 active 和 inactive 两个双向链表，其中：</p>
<ul>
<li><strong>active_list</strong> 活跃内存页链表，这里存放的是最近被访问过（活跃）的内存页；</li>
<li><strong>inactive_list</strong> 不活跃内存页链表，这里存放的是很少被访问（非活跃）的内存页；</li>
</ul>
<p>越接近链表尾部，就表示内存页越不常访问。这样，在回收内存时，系统就可以根据活跃程度，优先回收不活跃的内存。</p>
<p>活跃和非活跃的内存页，按照类型的不同，又分别分为文件页和匿名页。可以从 &#x2F;proc&#x2F;meminfo 中，查询它们的大小。</p>
<h4 id="如何保护一个进程不被-OOM-杀掉"><a href="#如何保护一个进程不被-OOM-杀掉" class="headerlink" title="如何保护一个进程不被 OOM 杀掉"></a>如何保护一个进程不被 OOM 杀掉</h4><p>在系统空闲内存不足的情况，进程申请了一个很大的内存，如果直接内存回收都无法回收出足够大的空闲内存，那么就会触发 OOM 机制，内核就会根据算法选择一个进程杀掉。</p>
<p>Linux 到底是根据什么标准来选择被杀的进程呢？这就要提到一个在 Linux 内核里有一个 <code>oom_badness()</code> 函数，它会把系统中可以被杀掉的进程扫描一遍，并对每个进程打分，得分最高的进程就会被首先杀掉。</p>
<pre><code class="c">points = process_pages + oom_score_adj*totalpages/1000
</code></pre>
<p>进程得分的结果受下面这两个方面影响：</p>
<ul>
<li>第一，进程已经使用的物理内存页面数。</li>
<li>第二，每个进程的 OOM 校准值 oom_score_adj。它是可以通过 <code>/proc/[pid]/oom_score_adj</code> 来配置的。我们可以在设置 -1000 到 1000 之间的任意一个数值，调整进程被 OOM Kill 的几率。</li>
</ul>
<p><strong>用「系统总的可用页面数」乘以 「OOM 校准值 oom_score_adj」再除以 1000，最后再加上进程已经使用的物理页面数，计算出来的值越大，那么这个进程被 OOM Kill 的几率也就越大</strong>。</p>
<p>每个进程的 oom_score_adj 默认值都为 0，所以最终得分跟进程自身消耗的内存有关，消耗的内存越大越容易被杀掉。我们可以通过调整 oom_score_adj 的数值，来改成进程的得分结果：</p>
<ul>
<li>如果你不想某个进程被首先杀掉，那你可以调整该进程的 oom_score_adj，从而改变这个进程的得分结果，降低该进程被 OOM 杀死的概率。</li>
<li>如果你想某个进程无论如何都不能被杀掉，那你可以将 oom_score_adj 配置为 -1000。</li>
</ul>
<p>我们最好将一些很重要的系统服务的 oom_score_adj 配置为 -1000，比如 sshd，因为这些系统服务一旦被杀掉，我们就很难再登陆进系统了。</p>
<p>但是，不建议将我们自己的业务程序的 oom_score_adj 设置为 -1000，因为业务程序一旦发生了内存泄漏，而它又不能被杀掉，这就会导致随着它的内存开销变大，OOM killer 不停地被唤醒，从而把其他进程一个个给杀掉。</p>
<h4 id="在-4GB-物理内存的机器上，申请-8G-内存会怎么样"><a href="#在-4GB-物理内存的机器上，申请-8G-内存会怎么样" class="headerlink" title="在 4GB 物理内存的机器上，申请 8G 内存会怎么样"></a>在 4GB 物理内存的机器上，申请 8G 内存会怎么样</h4><p>32 位操作系统和 64 位操作系统的虚拟地址空间大小是不同的，在 Linux 操作系统中，虚拟地址空间的内部又被分为<strong>内核空间和用户空间</strong>两部分，如下所示：</p>
<p><img src="https://img-blog.csdnimg.cn/3a6cb4e3f27241d3b09b4766bb0b1124.png" alt="img"></p>
<p>通过这里可以看出：</p>
<ul>
<li><code>32</code> 位系统的内核空间占用 <code>1G</code>，位于最高处，剩下的 <code>3G</code> 是用户空间；</li>
<li><code>64</code> 位系统的内核空间和用户空间都是 <code>128T</code>，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。</li>
</ul>
<p>因为malloc分配的是虚拟内存，跟实际物理内存无关，经过实验得出结论</p>
<ul>
<li>在 32 位操作系统，因为进程最大只能申请 3 GB 大小的虚拟内存，所以直接申请 8G 内存，会申请失败。</li>
<li>在 64位 位操作系统，因为进程最大只能申请 128 TB 大小的虚拟内存，即使物理内存只有 4GB，申请 8G 内存也是没问题，因为申请的内存是虚拟内存。</li>
</ul>
<p>在实际使用过程中，如果这块虚拟内存被访问了，要看系统有没有 Swap 分区：</p>
<ul>
<li>如果没有 Swap 分区，因为物理空间不够，进程会被操作系统杀掉，原因是 OOM（内存溢出）；</li>
<li>如果有 Swap 分区，即使物理内存只有 4GB，程序也能正常使用 8GB 的内存，进程可以正常运行；</li>
</ul>
<h2 id="进程管理"><a href="#进程管理" class="headerlink" title="进程管理"></a>进程管理</h2><h3 id="进程"><a href="#进程" class="headerlink" title="进程"></a><strong>进程</strong></h3><p>我们编写的代码只是一个存储在硬盘的静态文件，通过编译后就会生成二进制可执行文件，当我们运行这个可执行文件后，它会被装载到内存中，接着 CPU 会执行程序中的每一条指令，那么这个<strong>运行中的程序，就被称为「进程」（Process）</strong>。</p>
<p>现在我们考虑有一个会读取硬盘文件数据的程序被执行了，那么当运行到读取文件的指令时，就会去从硬盘读取数据，但是硬盘的读写速度是非常慢的，那么在这个时候，如果 CPU 傻傻的等硬盘返回数据的话，那 CPU 的利用率是非常低的。</p>
<p>所以，当进程要从硬盘读取数据时，CPU 不需要阻塞等待数据的返回，而是去执行另外的进程。当硬盘数据返回时，CPU 会收到个<strong>中断</strong>，于是 CPU 再继续运行这个进程。</p>
<p>这种<strong>多个程序、交替执行</strong>的思想，就有 CPU 管理多个进程的初步想法。虽然单核的 CPU 在某一个瞬间，只能运行一个进程。但在 1 秒钟期间，它可能会运行多个进程，这样就产生<strong>并行的错觉</strong>，实际上这是<strong>并发</strong>。</p>
<p><strong>为什么引入进程：</strong>为了支持和管理多任务并发执行</p>
<p>一开始操作系统是单道批处理系统，一个作业单独进入内存并独占系统资源，直到运行结束后下一个作业才能进入内存，当作业进行I&#x2F;O操作时，CPU只能处于等待状态，因此，CPU利用率较低。为此需要引入多道程序并发执行。</p>
<p>进程看起来在独占地使用硬件(而实际在和其他进程分时共享资源)，为程序员屏蔽了多道程序并发执行时进程调度和进程切换的细节。这样程序员就无需考虑程序之间切换所需操作的硬件，这些由操作系统的内核进行管理。</p>
<h4 id="进程的状态"><a href="#进程的状态" class="headerlink" title="进程的状态"></a>进程的状态</h4><p>在一个进程的活动期间至少具备三种基本状态，即运行状态、就绪状态、阻塞状态。</p>
<ul>
<li>运行状态（<em>Running</em>）：该时刻进程占用 CPU；</li>
<li>就绪状态（<em>Ready</em>）：可运行，由于其他进程处于运行状态而暂时停止运行；</li>
<li>阻塞状态（<em>Blocked</em>）：该进程正在等待某一事件发生（如等待输入&#x2F;输出操作的完成）而暂时停止运行，这时，即使给它CPU控制权，它也无法运行；</li>
</ul>
<p>当然，进程还有另外两个基本状态：</p>
<ul>
<li>创建状态（<em>new</em>）：进程正在被创建时的状态；</li>
<li>结束状态（<em>Exit</em>）：进程正在从系统中消失时的状态；</li>
</ul>
<p>如果有大量处于阻塞状态的进程，进程可能会占用着物理内存空间，显然不是我们所希望的，毕竟物理内存空间是有限的，被阻塞状态的进程占用着物理内存就一种浪费物理内存的行为。</p>
<p>所以，在虚拟内存管理的操作系统中，通常会把阻塞状态的进程的物理内存空间换出到硬盘，等需要再次运行的时候，再从硬盘换入到物理内存。</p>
<p>那么，就需要一个新的状态，来<strong>描述进程没有占用实际的物理内存空间的情况，这个状态就是挂起状态</strong>。这跟阻塞状态是不一样，阻塞状态是等待某个事件的返回。</p>
<p>另外，挂起状态可以分为两种：</p>
<ul>
<li>阻塞挂起状态：进程在外存（硬盘）并等待某个事件的出现；</li>
<li>就绪挂起状态：进程在外存（硬盘），但只要进入内存，即刻立刻运行；</li>
</ul>
<p><strong>导致进程挂起的原因</strong>不只是因为进程所使用的内存空间不在物理内存，还包括如下情况：</p>
<ul>
<li>通过 sleep 让进程间歇性挂起，其工作原理是设置一个定时器，到期后唤醒进程。</li>
<li>用户希望挂起一个程序的执行，比如在 Linux 中用 <code>Ctrl+Z</code> 挂起进程；</li>
</ul>
<p>这两种挂起状态加上前面的五种状态，就变成了七种状态变迁，见如下图：</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/process-state.jpg" alt="process-state"></p>
<h4 id="进程的控制结构"><a href="#进程的控制结构" class="headerlink" title="进程的控制结构"></a>进程的控制结构</h4><p>在操作系统中，是用<strong>进程控制块</strong>（<em>process control block，PCB</em>）数据结构来描述进程的。</p>
<p><strong>PCB 是进程存在的唯一标识</strong>，这意味着一个进程的存在，必然会有一个 PCB，如果进程消失了，那么 PCB 也会随之消失。</p>
<blockquote>
<p>PCB 具体包含什么信息呢？</p>
</blockquote>
<p><strong>进程描述信息：</strong></p>
<ul>
<li>进程标识符：标识各个进程，每个进程都有一个并且唯一的标识符；</li>
<li>用户标识符：进程归属的用户，用户标识符主要为共享和保护服务；</li>
</ul>
<p><strong>进程控制和管理信息：</strong></p>
<ul>
<li>进程当前状态，如 new、ready、running、waiting 或 blocked 等；</li>
<li>进程优先级：进程抢占 CPU 时的优先级；</li>
</ul>
<p><strong>资源分配清单：</strong></p>
<ul>
<li>有关内存地址空间或虚拟地址空间的信息，所打开文件的列表和所使用的 I&#x2F;O 设备信息。</li>
</ul>
<p><strong>CPU 相关信息：</strong></p>
<ul>
<li>CPU 中各个寄存器的值，当进程被切换时，CPU 的状态信息都会被保存在相应的 PCB 中，以便进程重新执行时，能从断点处继续执行。</li>
</ul>
<blockquote>
<p>每个 PCB 是如何组织的呢？</p>
</blockquote>
<p>通常是通过<strong>链表</strong>的方式进行组织，把具有<strong>相同状态的进程链在一起，组成各种队列</strong>。比如：</p>
<ul>
<li>将所有处于就绪状态的进程链在一起，称为<strong>就绪队列</strong>；</li>
<li>把所有因等待某事件而处于等待状态的进程链在一起就组成各种<strong>阻塞队列</strong>；</li>
<li>另外，对于运行队列在单核 CPU 系统中则只有一个运行指针了，因为单核 CPU 在某个时间，只能运行一个程序。</li>
</ul>
<p>那么，就绪队列和阻塞队列链表的组织形式如下图：</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/PCB-queue.jpg" alt="PCB-queue"></p>
<p><strong>除了链表的组织方式，还有索引方式</strong>，它的工作原理：将同一状态的进程组织在一个索引表中，索引表项指向相应的 PCB，不同状态对应不同的索引表。</p>
<p>一般会选择链表，因为可能面临进程创建，销毁等调度导致进程状态发生变化，所以链表能够更加灵活的插入和删除。</p>
<h4 id="进程控制"><a href="#进程控制" class="headerlink" title="进程控制"></a>进程控制</h4><p>进程的<strong>创建、终止、阻塞、唤醒</strong>的过程就是进程的控制。</p>
<p><strong>01 创建进程</strong></p>
<p>操作系统允许一个进程创建另一个进程，而且允许子进程继承父进程所拥有的资源。</p>
<p>创建进程的过程如下：</p>
<ul>
<li>申请一个空白的 PCB，并向 PCB 中填写一些控制和管理进程的信息，比如进程的唯一标识等；</li>
<li>为该进程分配运行时所必需的资源，比如内存资源；</li>
<li>将 PCB 插入到就绪队列，等待被调度运行；</li>
</ul>
<p><strong>02 终止进程</strong></p>
<p>进程可以有 3 种终止方式：正常结束、异常结束以及外界干预（信号 <code>kill</code> 掉）。</p>
<p>当子进程被终止时，其在父进程处继承的资源应当还给父进程。而当父进程被终止时，该父进程的子进程就变为孤儿进程，会被 1 号进程收养，并由 1 号进程对它们完成状态收集工作。</p>
<p>终止进程的过程如下：</p>
<ul>
<li>查找需要终止的进程的 PCB；</li>
<li>如果处于执行状态，则立即终止该进程的执行，然后将 CPU 资源分配给其他进程；</li>
<li>如果其还有子进程，则应将该进程的子进程交给 1 号进程接管；</li>
<li>将该进程所拥有的全部资源都归还给操作系统；</li>
<li>将其从 PCB 所在队列中删除；</li>
</ul>
<p><strong>03 阻塞进程</strong></p>
<p>当进程需要等待某一事件完成时，它可以调用阻塞语句把自己阻塞等待。而一旦被阻塞等待，它只能由另一个进程唤醒。</p>
<p>阻塞进程的过程如下：</p>
<ul>
<li>找到将要被阻塞进程标识号对应的 PCB；</li>
<li>如果该进程为运行状态，则保护其现场，将其状态转为阻塞状态，停止运行；</li>
<li>将该 PCB 插入到阻塞队列中去；</li>
</ul>
<p><strong>04 唤醒进程</strong></p>
<p>进程由「运行」转变为「阻塞」状态是由于进程必须等待某一事件的完成，所以处于阻塞状态的进程是绝对不可能叫醒自己的。</p>
<p>如果某进程正在等待 I&#x2F;O 事件，需由别的进程发消息给它，则只有当该进程所期待的事件出现时，才由发现者进程用唤醒语句叫醒它。</p>
<p>唤醒进程的过程如下：</p>
<ul>
<li>在该事件的阻塞队列中找到相应进程的 PCB；</li>
<li>将其从阻塞队列中移出，并置其状态为就绪状态；</li>
<li>把该 PCB 插入到就绪队列中，等待调度程序调度；</li>
</ul>
<p>进程的阻塞和唤醒是一对功能相反的语句，如果某个进程调用了阻塞语句，则必有一个与之对应的唤醒语句。</p>
<h4 id="进程的上下文切换"><a href="#进程的上下文切换" class="headerlink" title="进程的上下文切换"></a>进程的上下文切换</h4><p>CPU 寄存器和程序计数是 CPU 在运行任何任务前，所必须依赖的环境，这些环境就叫做 <strong>CPU 上下文</strong>。</p>
<p>CPU 上下文切换就是先把前一个任务的 CPU 上下文（CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。</p>
<p>根据任务的不同，把 CPU 上下文切换分成：<strong>进程上下文切换、线程上下文切换和中断上下文切换</strong>。</p>
<blockquote>
<p>进程的上下文切换到底是切换什么呢？</p>
</blockquote>
<p>进程是由内核管理和调度的，所以进程的切换只能发生在内核态。</p>
<p>所以，<strong>进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。</strong></p>
<p>通常，会把交换的信息保存在进程的 PCB，当要运行另外一个进程的时候，我们需要从这个进程的 PCB 取出上下文，然后恢复到 CPU 中，这使得这个进程可以继续执行。</p>
<blockquote>
<p>发生进程上下文切换有哪些场景？</p>
</blockquote>
<ul>
<li>为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，进程就从运行状态变为就绪状态，系统从就绪队列选择另外一个进程运行；</li>
<li>进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行；</li>
<li>当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度；</li>
<li>当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行；</li>
<li>发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序；</li>
</ul>
<p>以上，就是发生进程上下文切换的常见场景了。</p>
<h3 id="线程"><a href="#线程" class="headerlink" title="线程"></a>线程</h3><p><strong>线程是进程当中的一条执行流程。</strong></p>
<p>同一个进程内多个线程之间可以共享代码段、数据段、打开的文件等资源，但每个线程各自都有一套独立的寄存器和栈，这样可以确保线程的控制流是相对独立的。</p>
<p><strong>为什么引入线程：</strong>为了进一步提高并发度，减少多进程的开销(多核CPU可以并行运行程序，多线程可以提高CPU利用率)</p>
<p>举个例子，假设你要编写一个视频播放器软件，那么该软件功能的核心模块有三个：</p>
<ul>
<li>从视频文件当中读取数据；</li>
<li>对读取的数据进行解压缩；</li>
<li>把解压缩后的视频数据播放出来；</li>
</ul>
<p>对于单进程的实现方式，我想大家都会是以下这个方式：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/14-%E5%8D%95%E7%BA%BF%E7%A8%8Bmp4%E4%BB%A3%E7%A0%81%E5%AE%9E%E4%BE%8B.jpg" alt="单进程实现方式"></p>
<p>对于单进程的这种方式，存在以下问题：</p>
<ul>
<li>播放出来的画面和声音会不连贯，因为当 CPU 能力不够强的时候，<code>Read</code> 的时候可能进程就等在这了，这样就会导致等半天才进行数据解压和播放；</li>
<li>各个函数之间不是并发执行，影响资源的使用效率；</li>
</ul>
<p>那改进成多进程的方式：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/15-%E5%A4%9A%E8%BF%9B%E7%A8%8Bmp4-%E4%BB%A3%E7%A0%81%E5%AE%9E%E4%BE%8B.jpg" alt="多进程实现方式"></p>
<p>对于多进程的这种方式，依然会存在问题：</p>
<ul>
<li>进程之间如何通信，共享数据？</li>
<li>维护进程的系统开销较大，如创建进程时，分配资源、建立 PCB；终止进程时，回收资源、撤销 PCB；进程切换时，保存当前进程的状态信息；</li>
</ul>
<p>那到底如何解决呢？需要有一种新的实体，满足以下特性：</p>
<ul>
<li>实体之间可以并发运行；</li>
<li>实体之间共享相同的地址空间；</li>
</ul>
<p>这个新的实体，就是**线程( Thread )**，线程之间可以并发运行且共享相同的地址空间。</p>
<p>对于，线程相比进程能减少开销，体现在：</p>
<ul>
<li>线程的<strong>创建</strong>时间比进程快，因为进程在创建的过程中，还需要资源管理信息，比如内存管理信息、文件管理信息，而线程在创建的过程中，不会涉及这些资源管理信息，而是共享它们；</li>
<li>线程的<strong>终止</strong>时间比进程快，因为线程释放的资源相比进程少很多；</li>
<li>同一个进程内的线程<strong>切换</strong>比进程切换快，因为线程具有相同的地址空间（虚拟内存共享），这意味着同一个进程的线程都具有同一个页表，那么在切换的时候不需要切换页表。而对于进程之间的切换，切换的时候要把页表给切换掉，而页表的切换过程开销是比较大的；</li>
<li>由于同一进程的各线程间共享内存和文件资源，那么在线程之间<strong>通信</strong>的时候，就不需要经过内核了，这就使得线程之间的数据交互效率更高了；</li>
</ul>
<p>所以，线程比进程不管是时间效率，还是空间效率都要高。</p>
<h4 id="线程的上下文切换"><a href="#线程的上下文切换" class="headerlink" title="线程的上下文切换"></a>线程的上下文切换</h4><p>线程与进程最大的区别在于：<strong>线程是调度的基本单位，而进程则是资源拥有的基本单位</strong>。</p>
<p>所以，所谓操作系统的任务调度，实际上的调度对象是线程，而进程只是给线程提供了虚拟内存、全局变量等资源。</p>
<p>对于线程和进程，我们可以这么理解：</p>
<ul>
<li>当进程只有一个线程时，可以认为进程就等于线程；</li>
<li>当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源，这些资源在上下文切换时是不需要修改的；</li>
</ul>
<p>另外，线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的。</p>
<blockquote>
<p>线程上下文切换的是什么？</p>
</blockquote>
<p>这还得看线程是不是属于同一个进程：</p>
<ul>
<li>当两个线程不是属于同一个进程，则切换的过程就跟进程上下文切换一样；</li>
<li><strong>当两个线程是属于同一个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据</strong>；</li>
</ul>
<p>所以，线程的上下文切换相比进程，开销要小很多。</p>
<h4 id="线程的实现"><a href="#线程的实现" class="headerlink" title="线程的实现"></a>线程的实现</h4><p>主要有三种线程的实现方式：</p>
<ul>
<li><strong>用户线程（User Thread）</strong>：在用户空间实现的线程，不是由内核管理的线程，是由用户态的线程库来完成线程的管理；</li>
<li><strong>内核线程（Kernel Thread）</strong>：在内核中实现的线程，是由内核管理的线程；</li>
<li><strong>轻量级进程（LightWeight Process）</strong>：在内核中来支持用户线程；</li>
</ul>
<p><em>用户线程</em></p>
<p>用户线程是基于用户态的线程管理库来实现的，那么<strong>线程控制块（Thread Control Block, TCB）</strong> 也是在库里面来实现的，对于操作系统而言是看不到这个 TCB 的，它只能看到整个进程的 PCB。</p>
<p>所以，<strong>用户线程的整个线程管理和调度，操作系统是不直接参与的，而是由用户级线程库函数来完成线程的管理，包括线程的创建、终止、同步和调度等。</strong></p>
<p>用户线程和内核线程存在多对一，一对一，多对多的对应关系</p>
<p><strong>多对一线程模型</strong>            </p>
<p>多对一线程模型中，线程的创建、调度、同步的所有细节全部由进程的用户空间线程库来处理。用户态线程的很多操作对内核来说都是透明的，因为不需要内核来接管，这意味不需要内核态和用户态频繁切换。线程的创建、调度、同步处理速度非常快。当然线程的一些其他操作还是要经过内核，如IO读写。这样导致了一个问题：当多线程并发执行时，如果其中一个线程执行IO操作时，内核接管这个操作，如果IO阻塞，用户态的其他线程都会被阻塞，因为这些线程都对应同一个内核调度实体。在多处理器机器上，内核不知道用户态有这些线程，无法把它们调度到其他处理器，也无法通过优先级来调度。这对线程的使用是没有意义的！</p>
<p><strong>一对一线程模型</strong>            </p>
<p>一对一模型中，每个用户线程都对应各自的内核调度实体。内核会对每个线程进行调度，可以调度到其他处理器上面。当然由内核来调度的结果就是：线程的每次操作会在用户态和内核态切换。另外，内核为每个线程都映射调度实体，如果系统出现大量线程，会对系统性能有影响。但该模型的实用性还是高于多对一的线程模型。</p>
<p><strong>多对多线程模型</strong>              </p>
<p>多对多模型中，结合了1：1和M：1的优点，避免了它们的缺点。每个线程可以拥有多个调度实体，也可以多个线程对应一个调度实体。听起来好像非常完美，但线程的调度需要由内核态和用户态一起来实现。可想而知，多个对象操作一个东西时，肯定要一些其他的同步机制。用户态和内核态的分工合作导致实现该模型非常复杂。NPTL曾经也想使用该模型，但它太复杂，要对内核进行大范围改动，所以还是采用了一对一的模型！！！</p>
<p>用户线程的<strong>优点</strong>：</p>
<ul>
<li>每个进程都需要有它私有的线程控制块（TCB）列表，用来跟踪记录它各个线程状态信息（PC、栈指针、寄存器），TCB 由用户级线程库函数来维护，可用于不支持线程技术的操作系统；</li>
<li>用户线程的切换也是由线程库函数来完成的，无需用户态与内核态的切换，所以速度特别快；</li>
</ul>
<p>用户线程的<strong>缺点</strong>：</p>
<ul>
<li>由于操作系统不参与线程的调度，如果一个线程发起了系统调用而阻塞，那进程所包含的用户线程都不能执行了。</li>
<li>当一个线程开始运行后，除非它主动地交出 CPU 的使用权，否则它所在的进程当中的其他线程无法运行，因为用户态的线程没法打断当前运行中的线程，它没有这个特权，只有操作系统才有，但是用户线程不是由操作系统管理的。</li>
<li>由于时间片分配给进程，故与其他进程比，在多线程执行时，每个线程得到的时间片较少，执行会比较慢；</li>
</ul>
<p><em>内核线程</em></p>
<p><strong>内核线程是由操作系统管理的，线程对应的 TCB 自然是放在操作系统里的，这样线程的创建、终止和管理都是由操作系统负责。</strong></p>
<p>内核线程的<strong>优点</strong>：</p>
<ul>
<li>在一个进程当中，如果某个内核线程发起系统调用而被阻塞，并不会影响其他内核线程的运行；</li>
<li>分配给线程，多线程的进程获得更多的 CPU 运行时间；</li>
</ul>
<p>内核线程的<strong>缺点</strong>：</p>
<ul>
<li>在支持内核线程的操作系统中，由内核来维护进程和线程的上下文信息，如 PCB 和 TCB；</li>
<li>线程的创建、终止和切换都是通过系统调用的方式来进行，因此对于系统来说，系统开销比较大；</li>
</ul>
<p><em>轻量级进程</em></p>
<p><strong>轻量级进程（Light-weight process，LWP）是内核支持的用户线程，一个进程可有一个或多个 LWP，每个 LWP 是跟内核线程一对一映射的，也就是 LWP 都是由一个内核线程支持，而且 LWP 是由内核管理并像普通进程一样被调度</strong>。</p>
<p>在大多数系统中，<strong>LWP与普通进程的区别也在于它只有一个最小的执行上下文和调度程序所需的统计信息</strong>。一般来说，一个进程代表程序的一个实例，而 LWP 代表程序的执行线程，因为一个执行线程不像进程那样需要那么多状态信息，所以 LWP 也不带有这样的信息。</p>
<h4 id="Linux线程模型"><a href="#Linux线程模型" class="headerlink" title="Linux线程模型"></a>Linux线程模型</h4><p>Linux刚诞生那时候，还没有“线程”的概念(Linux 2.4及以前的版本)，当然也没办法在操作系统上创建线程。所以，<strong>Linux 2.4内核中不知道什么是“线程”</strong>，只有一个<code>task_struct</code>的数据结构，就是进程。</p>
<p>后来随着科学技术的发展，大家提出线程的概念，于是，我们希望Linux能加入多线程编程。要修改一个操作系统，是很复杂的事情，特别是当操作系统越来越庞大的时候。怎么才能让Linux支持多线程呢？</p>
<p>最简单的，就是不去动操作系统的内核，而是写一个函数库来模拟线程。也就是说，我用C写一个函数，比如 create_thread，这个函数最终在Linux的内核里还是去调用了创建进程的函数去创建了一个进程，好了，这个线程，就是用库函数创建的线程，就是所谓的用户级线程了。这是最初始的<code>LinuxThreads</code>。</p>
<p><code>LinuxThreads</code>与真正的POSIX标准有一些不相容的地方，尤其是在信号处理、进程调度和进程间同步原语方面。<code>LinuxThreads</code>使用信号进行线程之间的通信，必须经过内核，效率较低。</p>
<p>要提高<code>LinuxThreads</code>的效率很明显需要提供内核支持以及必须重写线程函式库。为了解决这个问题出现了两个互相竞争的项目：一个IBM的组的项目叫做NGPT（Next Generation POSIX Threads，下一代POSIX线程），另一个组是由Red Hat程序员组成的叫做NPTL。2003年中NGPT被放弃。</p>
<p>NPTL从Linux内核2.6开始它被纳入内核。目前它完全被结合入GNU C 函式库。NPTL的解决方法与LinuxThreads类似，内核看到的首要抽象依然是一个进程，新线程是通过clone()系统调用产生的。但是NPTL需要特殊的内核支持来解决同步的原始类型之间互相竞争的状况。在这种情况下线程必须能够入眠和再复苏。用来完成这个任务的原始类型叫做futex。有了futex后，可在用户态进行线程同步，提高了效率。</p>
<p>总结：Linux采用的是1:1的线程模型，即一个用户线程对应一个内核线程。同时在Linux看来线程和进程是一样的，在内核中有着同样的数据结构<code>task_struct</code>，只是线程相比进程共享资源更多。</p>
<h4 id="多线程还是多进程"><a href="#多线程还是多进程" class="headerlink" title="多线程还是多进程"></a><strong>多线程还是多进程</strong></h4><p>多进程模式最大的优点就是稳定性高，因为一个进程崩溃了，不会影响其他进程。著名的Apache最早就是采用多进程模式。</p>
<p>多进程模式的缺点是创建进程的代价大，在Unix&#x2F;Linux系统下，用<code>fork</code>调用还行，在Windows下创建进程开销巨大；另外，操作系统能同时运行的进程数也是有限的，在内存和CPU的限制下，如果有几千个进程同时运行，操作系统连调度都会成问题；进程之间通信的性能也很差。</p>
<table>
<thead>
<tr>
<th><strong>对比维度</strong></th>
<th><strong>多进程</strong></th>
<th><strong>多线程</strong></th>
<th><strong>总结</strong></th>
</tr>
</thead>
<tbody><tr>
<td>数据共享、同步</td>
<td>数据共享复杂，需要用IPC；数据是分开的，同步简单</td>
<td>因为共享进程数据，数据共享简单，但也是因为这个原因导致同步复杂</td>
<td>各有优势</td>
</tr>
<tr>
<td>内存、CPU</td>
<td>占用内存多，切换复杂，CPU利用率低</td>
<td>占用内存少，切换简单，CPU利用率高</td>
<td>线程占优</td>
</tr>
<tr>
<td>创建销毁、切换</td>
<td>创建销毁、切换复杂，速度慢</td>
<td>创建销毁、切换简单，速度很快</td>
<td>线程占优</td>
</tr>
<tr>
<td>编程、调试</td>
<td>编程简单，调试简单</td>
<td>编程复杂，调试复杂</td>
<td>进程占优</td>
</tr>
<tr>
<td>可靠性</td>
<td>进程间不会互相影响</td>
<td>一个线程挂掉将导致整个进程挂掉</td>
<td>进程占优</td>
</tr>
<tr>
<td>分布式</td>
<td>适应于多核、多机分布式；如果一台机器不够，扩展到多台机器比较简单</td>
<td>适应于多核分布式</td>
<td>进程占优</td>
</tr>
</tbody></table>
<p><strong>需要频繁创建销毁的优先用线程</strong></p>
<p>这种原则最常见的应用就是Web服务器了，来一个连接建立一个线程，断了就销毁线程，要是用进程，创建和销毁的代价是很难承受的</p>
<p><strong>需要进行大量计算的优先使用线程</strong></p>
<p>所谓大量计算，当然就是要耗费很多CPU，切换频繁了，这种情况下线程是最合适的。</p>
<p>这种原则最常见的是图像处理、算法处理。</p>
<p><strong>强相关的处理用线程，弱相关的处理用进程</strong></p>
<p>一般的Server需要完成如下任务：消息收发、消息处理。“消息收发”和“消息处理”就是弱相关的任务，而“消息处理”里面可能又分为“消息解码”、“业务处理”，这两个任务相对来说相关性就要强多了。因此“消息收发”和“消息处理”可以分进程设计，“消息解码”、“业务处理”可以分线程设计。</p>
<p><strong>可能要扩展到多机分布的用进程，多核分布的用线程</strong></p>
<p>原因请看上面对比。</p>
<h3 id="进程调度"><a href="#进程调度" class="headerlink" title="进程调度"></a>进程调度</h3><p>在进程的生命周期中，当进程从一个运行状态到另外一状态变化的时候，其实会触发一次调度。</p>
<p>比如，以下状态的变化都会触发操作系统的调度：</p>
<ul>
<li><em>从就绪态 -&gt; 运行态</em>：当进程被创建时，会进入到就绪队列，操作系统会从就绪队列选择一个进程运行；</li>
<li><em>从运行态 -&gt; 阻塞态</em>：当进程发生 I&#x2F;O 事件而阻塞时，操作系统必须选择另外一个进程运行；</li>
<li><em>从运行态 -&gt; 结束态</em>：当进程退出结束后，操作系统得从就绪队列选择另外一个进程运行；</li>
</ul>
<p>另外，如果硬件时钟提供某个频率的周期性中断，那么可以根据如何处理时钟中断 ，把调度算法分为两类：</p>
<ul>
<li><strong>非抢占式调度算法</strong>挑选一个进程，然后让该进程运行直到被阻塞，或者直到该进程退出，才会调用另外一个进程，也就是说不会理时钟中断这个事情。</li>
<li><strong>抢占式调度算法</strong>挑选一个进程，然后让该进程只运行某段时间，如果在该时段结束时，该进程仍然在运行时，则会把它挂起，接着调度程序从就绪队列挑选另外一个进程。这种抢占式调度处理，需要在时间间隔的末端发生<strong>时钟中断</strong>，以便把 CPU 控制返回给调度程序进行调度，也就是常说的<strong>时间片机制</strong>。</li>
</ul>
<p><strong>调度原则</strong></p>
<ul>
<li><strong>CPU 利用率</strong>：调度程序应确保 CPU 是始终匆忙的状态，这可提高 CPU 的利用率；</li>
<li><strong>系统吞吐量</strong>：吞吐量表示的是单位时间内 CPU 完成进程的数量，长作业的进程会占用较长的 CPU 资源，因此会降低吞吐量，相反，短作业的进程会提升系统吞吐量；</li>
<li><strong>周转时间</strong>：周转时间是进程运行+阻塞时间+等待时间的总和，一个进程的周转时间越小越好；</li>
<li><strong>等待时间</strong>：这个等待时间不是阻塞状态的时间，而是进程处于就绪队列的时间，等待的时间越长，用户越不满意；</li>
<li><strong>响应时间</strong>：用户提交请求到系统第一次产生响应所花费的时间，在交互式系统中，响应时间是衡量调度算法好坏的主要标准。</li>
</ul>
<h4 id="调度算法"><a href="#调度算法" class="headerlink" title="调度算法"></a>调度算法</h4><p><strong>先来先服务调度算法</strong></p>
<p>最简单的一个调度算法，就是非抢占式的<strong>先来先服务（First Come First Serve, FCFS）算法</strong>了。</p>
<p>顾名思义，先来后到，<strong>每次从就绪队列选择最先进入队列的进程，然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。</strong></p>
<p>这似乎很公平，但是当一个长作业先运行了，那么后面的短作业等待的时间就会很长，不利于短作业。</p>
<p><strong>FCFS 对长作业有利，对CPU 繁忙型作业有利，而不适用于 I&#x2F;O 繁忙型作业(I&#x2F;O繁忙需要经常放弃CPU，并重新排队)。</strong></p>
<p><strong>最短作业优先调度算法</strong></p>
<p><strong>最短作业优先（Shortest Job First, SJF）调度算法</strong>同样也是顾名思义，它会<strong>优先选择运行时间最短的进程来运行</strong>，这有助于提高系统的吞吐量。</p>
<p>这显然对长作业不利，很容易造成一种极端现象。</p>
<p>比如，一个长作业在就绪队列等待运行，而这个就绪队列有非常多的短作业，那么就会使得长作业不断的往后推，周转时间变长，致使长作业长期不会被运行。</p>
<p><strong>高响应比优先调度算法</strong></p>
<p>前面的「先来先服务调度算法」和「最短作业优先调度算法」都没有很好的权衡短作业和长作业。</p>
<p>那么，<strong>高响应比优先 （Highest Response Ratio Next, HRRN）调度算法</strong>主要是权衡了短作业和长作业。</p>
<p><strong>每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行</strong>，「响应比优先级」的计算公式：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/26-%E5%93%8D%E5%BA%94%E6%AF%94%E5%85%AC%E5%BC%8F.jpg" alt="img"></p>
<p>从上面的公式，可以发现：</p>
<ul>
<li>如果两个进程的「等待时间」相同时，「要求的服务时间」越短，「响应比」就越高，这样短作业的进程容易被选中运行；</li>
<li>如果两个进程「要求的服务时间」相同时，「等待时间」越长，「响应比」就越高，这就兼顾到了长作业进程，因为进程的响应比可以随时间等待的增加而提高，当其等待时间足够长时，其响应比便可以升到很高，从而获得运行的机会；</li>
</ul>
<p><strong>因为进程要求服务时间不可预估，所以高响应比优先调度算法是「理想型」的调度算法，现实中是实现不了的。</strong></p>
<p><strong>时间片轮转调度算法</strong></p>
<p>最古老、最简单、最公平且使用最广的算法就是<strong>时间片轮转（Round Robin, RR）调度算法</strong>。</p>
<p><strong>每个进程被分配一个时间段，称为时间片（*Quantum*），即允许该进程在该时间段中运行。</strong></p>
<ul>
<li>如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配给另外一个进程；</li>
<li>如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；</li>
</ul>
<p>另外，时间片的长度就是一个很关键的点：</p>
<ul>
<li>如果时间片设得太短会导致过多的进程上下文切换，降低了 CPU 效率；</li>
<li>如果设得太长又可能引起对短作业进程的响应时间变长。将</li>
</ul>
<p>一般来说，时间片设为 <code>20ms~50ms</code> 通常是一个比较合理的折中值。</p>
<p><strong>最高优先级调度算法</strong></p>
<p>前面的「时间片轮转算法」做了个假设，即让所有的进程同等重要，也不偏袒谁，大家的运行时间都一样。</p>
<p>但是，对于多用户计算机系统就有不同的看法了，它们希望调度是有优先级的，即希望调度程序能<strong>从就绪队列中选择最高优先级的进程进行运行，这称为最高优先级（Highest Priority First，HPF）调度算法</strong>。</p>
<p>进程的优先级可以分为，静态优先级和动态优先级：</p>
<ul>
<li>静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化；</li>
<li>动态优先级：根据进程的动态变化调整优先级，比如如果进程运行时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升高其优先级，也就是<strong>随着时间的推移增加等待进程的优先级</strong>。</li>
</ul>
<p>该算法也有两种处理优先级高的方法，非抢占式和抢占式：</p>
<ul>
<li>非抢占式：当就绪队列中出现优先级高的进程，运行完当前进程，再选择优先级高的进程。</li>
<li>抢占式：当就绪队列中出现优先级高的进程，当前进程挂起，调度优先级高的进程运行。</li>
</ul>
<p>但是依然有缺点，可能会导致低优先级的进程永远不会运行。</p>
<p><strong>多级反馈队列调度算法</strong></p>
<p><strong>多级反馈队列（Multilevel Feedback Queue）调度算法</strong>是「时间片轮转算法」和「最高优先级算法」的综合和发展。</p>
<p>顾名思义：</p>
<ul>
<li>「多级」表示有多个队列，每个队列优先级从高到低，同时优先级越高时间片越短。</li>
<li>「反馈」表示如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列；</li>
</ul>
<p>来看看，它是如何工作的：</p>
<ul>
<li>设置了多个队列，赋予每个队列不同的优先级，每个<strong>队列优先级从高到低</strong>，同时<strong>优先级越高时间片越短</strong>；</li>
<li>新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度，如果在第一级队列规定的时间片没运行完成，则将其转入到第二级队列的末尾，以此类推，直至完成；</li>
<li>当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行</li>
</ul>
<p>可以发现，对于短作业可能可以在第一级队列很快被处理完。对于长作业，如果在第一级队列处理不完，可以移入下次队列等待被执行，虽然等待的时间变长了，但是运行时间也变更长了，所以该算法很好的<strong>兼顾了长短作业，同时有较好的响应时间。</strong></p>
<h3 id="进程通信"><a href="#进程通信" class="headerlink" title="进程通信"></a>进程通信</h3><p>每个进程的用户地址空间都是独立的，一般而言是不能互相访问的，但内核空间是每个进程都共享的，所以进程之间要通信必须通过内核。</p>
<h4 id="管道"><a href="#管道" class="headerlink" title="管道"></a>管道</h4><p>如果你学过 Linux 命令，那你肯定很熟悉「<code>|</code>」这个竖线。</p>
<pre><code class="bash">$ ps auxf | grep mysql
</code></pre>
<p>上面命令行里的「<code>|</code>」竖线就是一个<strong>管道</strong>，它的功能是将前一个命令（<code>ps auxf</code>）的输出，作为后一个命令（<code>grep mysql</code>）的输入，从这功能描述，可以看出<strong>管道传输数据是单向的</strong>，如果想相互通信，我们需要创建两个管道才行。</p>
<p>同时，我们得知上面这种管道是没有名字，所以「<code>|</code>」表示的管道称为<strong>匿名管道</strong>，用完了就销毁。</p>
<p>管道还有另外一个类型是<strong>命名管道</strong>，也被叫做 <code>FIFO</code>，因为数据是先进先出的传输方式。</p>
<p>在使用命名管道前，先需要通过 <code>mkfifo</code> 命令来创建，并且指定管道名字：</p>
<pre><code class="bash">$ mkfifo myPipe
</code></pre>
<p>基于 Linux 一切皆文件的理念，所以管道也是以文件的方式存在，我们可以用 ls 看一下，这个文件的类型是 p，也就是 pipe（管道） 的意思。</p>
<p>接下来，我们往 myPipe 这个管道写入数据：</p>
<pre><code class="bash">$ echo &quot;hello&quot; &gt; myPipe  // 将数据写进管道
                         // 停住了 ...
</code></pre>
<p>你操作了后，你会发现命令执行后就停在这了，这是因为管道里的内容没有被读取，只有当管道里的数据被读完后，命令才可以正常退出。</p>
<p>于是，我们执行另外一个命令来读取这个管道里的数据：</p>
<pre><code class="bash">$ cat &lt; myPipe  // 读取管道里的数据
hello
</code></pre>
<p>可以看到，管道里的内容被读取出来了，并打印在了终端上，另外一方面，echo 那个命令也正常退出了。</p>
<p>我们可以看出，<strong>管道这种通信方式效率低，不适合进程间频繁地交换数据</strong>。当然，它的好处，自然就是简单，同时也我们很容易得知管道里的数据已经被另一个进程读取了。</p>
<h5 id="匿名管道创建原理"><a href="#匿名管道创建原理" class="headerlink" title="匿名管道创建原理"></a>匿名管道创建原理</h5><p>匿名管道的创建，需要通过下面这个系统调用：</p>
<pre><code class="c">int pipe(int fd[2])
</code></pre>
<p>这里表示创建一个匿名管道，并返回了两个描述符，一个是管道的读取端描述符 <code>fd[0]</code>，另一个是管道的写入端描述符 <code>fd[1]</code>。注意，这个匿名管道是特殊的文件，只存在于内存，不存于文件系统中。</p>
<p>其实，<strong>所谓的管道，就是内核里面的一串缓存</strong>。从管道的一段写入的数据，实际上是缓存在内核中的，另一端读取，也就是从内核中读取这段数据。另外，<strong>管道传输的数据是无格式的流且大小受限</strong>。</p>
<p>看到这，你可能会有疑问了，这两个描述符都是在一个进程里面，并没有起到进程间通信的作用，怎么样才能使得管道是跨过两个进程的呢？</p>
<p>我们可以使用 <code>fork</code> 创建子进程，<strong>创建的子进程会复制父进程的文件描述符</strong>，这样就做到了两个进程各有两个「 <code>fd[0]</code> 与 <code>fd[1]</code>」，两个进程就可以通过各自的 fd 写入和读取同一个管道文件实现跨进程通信了。</p>
<p>管道只能一端写入，另一端读出，所以上面这种模式容易造成混乱，因为父进程和子进程都可以同时写入，也都可以读出。那么，<strong>为了避免这种情况，通常的做法是：</strong></p>
<ul>
<li>父进程关闭读取的 fd[0]，只保留写入的 fd[1]；</li>
<li>子进程关闭写入的 fd[1]，只保留读取的 fd[0]；</li>
</ul>
<p>所以说如果需要双向通信，则应该创建两个管道。</p>
<p>到这里，我们仅仅解析了使用管道进行父进程与子进程之间的通信，但是在我们 shell 里面并不是这样的。在 shell 里面执行 <code>A | B</code>命令的时候，A 进程和 B 进程都是 shell 创建出来的子进程，A 和 B 之间不存在父子关系，它俩的父进程都是 shell。</p>
<p>所以说，在 shell 里通过「<code>|</code>」匿名管道将多个命令连接在一起，实际上也就是创建了多个子进程，那么在我们编写 shell 脚本时，能使用一个管道搞定的事情，就不要多用一个管道，这样可以减少创建子进程的系统开销。</p>
<p>我们可以得知，<strong>对于匿名管道，它的通信范围是存在父子关系的进程</strong>。因为管道没有实体，也就是没有管道文件，只能通过 fork 来复制父进程 fd 文件描述符，来达到通信的目的。</p>
<p>另外，<strong>对于命名管道，它可以在不相关的进程间也能相互通信</strong>。因为命令管道，提前创建了一个类型为管道的设备文件，在进程里只要使用这个设备文件，就可以相互通信。</p>
<p>不管是匿名管道还是命名管道，进程写入的数据都是缓存在内核中，另一个进程读取数据时候自然也是从内核中获取，同时通信数据都遵循<strong>先进先出</strong>原则，不支持 lseek 之类的文件定位操作。</p>
<h4 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h4><p>前面说到管道的通信方式是效率低的，因此管道不适合进程间频繁地交换数据。</p>
<p>对于这个问题，<strong>消息队列</strong>的通信模式就可以解决。比如，A 进程要给 B 进程发送消息，A 进程把数据放在对应的消息队列后就可以正常返回了，B 进程需要的时候再去读取数据就可以了。同理，B 进程要给 A 进程发送消息也是如此。</p>
<p>再来，<strong>消息队列是保存在内核中的消息链表</strong>，在发送数据时，会分成一个一个独立的数据单元，也就是消息体（数据块），消息体是用户自定义的数据类型，消息的发送方和接收方要约定好消息体的数据类型，所以每个消息体都是固定大小的存储块，不像管道是无格式的字节流数据。如果进程从消息队列中读取了消息体，内核就会把这个消息体删除。<strong>消息队列</strong>克服了管道通信的数据是无格式的字节流的问题</p>
<p><strong>消息队列生命周期随内核</strong>，如果没有释放消息队列或者没有关闭操作系统，消息队列会一直存在，而前面提到的<strong>匿名管道的生命周期，是随进程的创建而建立，随进程的结束而销毁。</strong></p>
<p>消息这种模型，两个进程之间的通信就像平时发邮件一样，你来一封，我回一封，可以频繁沟通了。</p>
<p>但邮件的通信方式存在不足的地方有两点，<strong>一是通信不及时，二是附件也有大小限制</strong>，这同样也是消息队列通信不足的点。</p>
<p><strong>消息队列不适合比较大数据的传输</strong>，因为在内核中每个消息体都有一个最大长度的限制，同时所有队列所包含的全部消息体的总长度也是有上限。在 Linux 内核中，会有两个宏定义 <code>MSGMAX</code> 和 <code>MSGMNB</code>，它们以字节为单位，分别定义了一条消息的最大长度和一个队列的最大长度。</p>
<p><strong>消息队列通信过程中，存在用户态与内核态之间的数据拷贝开销</strong>，因为进程写入数据到内核中的消息队列时，会发生从用户态拷贝数据到内核态的过程，同理另一进程读取内核中的消息数据时，会发生从内核态拷贝数据到用户态的过程。</p>
<h4 id="共享内存"><a href="#共享内存" class="headerlink" title="共享内存"></a>共享内存</h4><p>消息队列的读取和写入的过程，都会有发生用户态与内核态之间的消息拷贝过程。那<strong>共享内存</strong>的方式，就很好的解决了这一问题。</p>
<p>现代操作系统，对于内存管理，采用的是虚拟内存技术，也就是每个进程都有自己独立的虚拟内存空间，不同进程的虚拟内存映射到不同的物理内存中。所以，即使进程 A 和 进程 B 的虚拟地址是一样的，其实访问的是不同的物理内存地址，对于数据的增删查改互不影响。</p>
<p><strong>共享内存的机制，就是拿出一块虚拟地址空间来，映射到相同的物理内存中</strong>。这样这个进程写入的东西，另外一个进程马上就能看到了，都不需要拷贝来拷贝去，传来传去，大大提高了进程间通信的速度。</p>
<h4 id="信号量"><a href="#信号量" class="headerlink" title="信号量"></a>信号量</h4><p>用了共享内存通信方式，带来新的问题，那就是如果多个进程同时修改同一个共享内存，很有可能就冲突了。例如两个进程都同时写一个地址，那先写的那个进程会发现内容被别人覆盖了。</p>
<p>为了防止多进程竞争共享资源，而造成的数据错乱，所以需要保护机制，使得共享的资源，在任意时刻只能被一个进程访问。正好，<strong>信号量</strong>就实现了这一保护机制。</p>
<p><strong>信号量其实是一个整型的计数器，主要用于实现进程间的互斥与同步，而不是用于缓存进程间通信的数据</strong>。</p>
<p>信号量表示资源的数量，控制信号量的方式有两种原子操作：</p>
<ul>
<li>一个是 <strong>P 操作</strong>，这个操作会把信号量减去 1，相减后如果信号量 &lt; 0，则表明资源已被占用，进程需阻塞等待；相减后如果信号量 &gt;&#x3D; 0，则表明还有资源可使用，进程可正常继续执行。</li>
<li>另一个是 <strong>V 操作</strong>，这个操作会把信号量加上 1，相加后如果信号量 &lt;&#x3D; 0，则表明当前有阻塞中的进程，于是会将该进程唤醒运行；相加后如果信号量 &gt; 0，则表明当前没有阻塞中的进程；</li>
</ul>
<p>P 操作是用在进入共享资源之前，V 操作是用在离开共享资源之后，这两个操作是必须成对出现的。</p>
<p>信号初始化为 <code>1</code>，就代表着是<strong>互斥信号量</strong>，它可以保证共享内存在任何时刻只有一个进程在访问，这就很好的保护了共享内存。</p>
<p>信号初始化为 <code>0</code>，就代表着是<strong>同步信号量</strong>，它可以保证进程 A 应在进程 B 之前执行。</p>
<h4 id="信号"><a href="#信号" class="headerlink" title="信号"></a>信号</h4><p>上面说的进程间通信，都是常规状态下的工作模式。<strong>对于异常情况下的工作模式，就需要用「信号」的方式来通知进程。</strong></p>
<p>信号跟信号量虽然名字相似度 66.66%，但两者用途完全不一样，就好像 Java 和 JavaScript 的区别。</p>
<p>在 Linux 操作系统中， 为了响应各种各样的事件，提供了几十种信号，分别代表不同的意义。我们可以通过 <code>kill -l</code> 命令，查看所有的信号。</p>
<p>运行在 shell 终端的进程，我们可以通过键盘输入某些组合键的时候，给进程发送信号。例如</p>
<ul>
<li>Ctrl+C 产生 <code>SIGINT</code> 信号，表示终止该进程；</li>
<li>Ctrl+Z 产生 <code>SIGTSTP</code> 信号，表示停止该进程，但还未结束；</li>
</ul>
<p>如果进程在后台运行，可以通过 <code>kill</code> 命令的方式给进程发送信号，但前提需要知道运行中的进程 PID 号，例如：</p>
<ul>
<li>kill -9 1050 ，表示给 PID 为 1050 的进程发送 <code>SIGKILL</code> 信号，用来立即结束该进程；</li>
</ul>
<p>所以，<strong>信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令）</strong>。</p>
<p>信号是进程间通信机制中<strong>唯一的异步通信机制</strong>，因为可以在任何时候发送信号给某一进程，一旦有信号产生，我们就有下面这几种，用户进程对信号的处理方式。</p>
<p><strong>1.执行默认操作</strong>。Linux 对每种信号都规定了默认操作，例如，上面列表中的 SIGTERM 信号，就是终止进程的意思。</p>
<p><strong>2.捕捉信号</strong>。我们可以为信号定义一个信号处理函数。当信号发生时，我们就执行相应的信号处理函数。</p>
<p><strong>3.忽略信号</strong>。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。有两个信号是应用进程无法捕捉和忽略的，即 <code>SIGKILL</code> 和 <code>SEGSTOP</code>，它们用于在任何时候中断或结束某一进程。</p>
<h4 id="socket"><a href="#socket" class="headerlink" title="socket"></a>socket</h4><p>前面提到的管道、消息队列、共享内存、信号量和信号都是在同一台主机上进行进程间通信，那要想<strong>跨网络与不同主机上的进程之间通信，就需要 Socket 通信了。</strong></p>
<p>实际上，Socket 通信不仅可以跨网络与不同主机的进程间通信，还可以在同主机上进程间通信。</p>
<h3 id="多线程冲突"><a href="#多线程冲突" class="headerlink" title="多线程冲突"></a>多线程冲突</h3><p>在进程&#x2F;线程并发执行的过程中，进程&#x2F;线程之间存在协作的关系，例如有互斥、同步的关系。</p>
<p>为了实现进程&#x2F;线程间正确的协作，操作系统必须提供实现进程协作的措施和方法，主要的方法有两种：</p>
<ul>
<li><em>锁</em>：加锁、解锁操作；</li>
<li><em>信号量</em>：P、V 操作；</li>
</ul>
<p>这两个都可以方便地实现进程&#x2F;线程互斥，而信号量比锁的功能更强一些，它还可以方便地实现进程&#x2F;线程同步。</p>
<h4 id="锁"><a href="#锁" class="headerlink" title="锁"></a><strong>锁</strong></h4><p>使用加锁操作和解锁操作可以解决并发线程&#x2F;进程的互斥问题。</p>
<p>任何想进入临界区的线程，必须先执行加锁操作。若加锁操作顺利通过，则线程可进入临界区；在完成对临界资源的访问后再执行解锁操作，以释放该临界资源。</p>
<p>根据锁的实现不同，可以分为「忙等待锁」和「无忙等待锁」。</p>
<h5 id="忙等待锁"><a href="#忙等待锁" class="headerlink" title="忙等待锁"></a>忙等待锁</h5><p>在说明「忙等待锁」的实现之前，先介绍现代 CPU 体系结构提供的特殊<strong>原子操作指令 —— 测试和置位（Test-and-Set）指令</strong>。</p>
<p>如果用 C 代码表示 Test-and-Set 指令，形式如下：</p>
<pre><code class="c">int TestAndSet(int *old_ptr, int new)
&#123;
    int old = *old_ptr;
    *old_ptr = new;
    return old;
&#125;
</code></pre>
<p>测试并设置指令做了下述事情:</p>
<ul>
<li>把 <code>old_ptr</code> 更新为 <code>new</code> 的新值</li>
<li>返回 <code>old_ptr</code> 的旧值；</li>
</ul>
<p>当然，<strong>关键是这些代码是原子执行</strong>。因为既可以测试旧值，又可以设置新值，所以我们把这条指令叫作「测试并设置」。</p>
<p>我们可以运用 Test-and-Set 指令来实现「忙等待锁」，代码如下：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/14-%E8%87%AA%E6%97%8B%E9%94%81.jpg" alt="img"></p>
<p>我们来确保理解为什么这个锁能工作：</p>
<ul>
<li>第一个场景是，首先假设一个线程在运行，调用 <code>lock()</code>，没有其他线程持有锁，所以 <code>flag</code> 是 0。当调用 <code>TestAndSet(flag, 1)</code> 方法，返回 0，线程会跳出 while 循环，获取锁。同时也会原子的设置 flag 为1，标志锁已经被持有。当线程离开临界区，调用 <code>unlock()</code> 将 <code>flag</code> 清理为 0。</li>
<li>第二种场景是，当某一个线程已经持有锁（即 <code>flag</code> 为1）。本线程调用 <code>lock()</code>，然后调用 <code>TestAndSet(flag, 1)</code>，这一次返回 1。只要另一个线程一直持有锁，<code>TestAndSet()</code> 会重复返回 1，本线程会一直<strong>忙等</strong>。当 <code>flag</code> 终于被改为 0，本线程会调用 <code>TestAndSet()</code>，返回 0 并且原子地设置为 1，从而获得锁，进入临界区。</li>
</ul>
<p>很明显，当获取不到锁时，线程就会一直 while 循环，不做任何事情，所以就被称为「忙等待锁」，也被称为<strong>自旋锁（spin lock）</strong>。</p>
<p>这是最简单的一种锁，一直自旋，利用 CPU 周期，直到锁可用。在单处理器上，需要抢占式的调度器（即不断通过时钟中断一个线程，运行其他线程）。否则，自旋锁在单 CPU 上无法使用，因为一个自旋的线程永远不会放弃 CPU。</p>
<h5 id="无等待锁"><a href="#无等待锁" class="headerlink" title="无等待锁"></a>无等待锁</h5><p>无等待锁顾名思义就是获取不到锁的时候，不用自旋。<br>既然不想自旋，那当没获取到锁的时候，就把当前线程放入到锁的等待队列，然后执行调度程序，把 CPU 让给其他线程执行。</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/15-%E6%97%A0%E7%AD%89%E5%BE%85%E9%94%81.jpg" alt="img"></p>
<h5 id="常见的锁"><a href="#常见的锁" class="headerlink" title="常见的锁"></a>常见的锁</h5><p>接下来，针对不同的应用场景，谈一谈「<strong>互斥锁、自旋锁、读写锁、乐观锁、悲观锁</strong>」的选择和使用。</p>
<p><strong>互斥锁与自旋锁</strong></p>
<p>最底层的两种就是会「互斥锁和自旋锁」，有很多高级的锁都是基于它们实现的，你可以认为它们是各种锁的地基，所以我们必须清楚它俩之间的区别和应用。</p>
<p>加锁的目的就是保证共享资源在任意时间里，只有一个线程访问，这样就可以避免多线程导致共享数据错乱的问题。</p>
<p>当已经有一个线程加锁后，其他线程加锁则就会失败，互斥锁和自旋锁对于加锁失败后的处理方式是不一样的：</p>
<ul>
<li><strong>互斥锁</strong>加锁失败后，线程会<strong>释放 CPU</strong> ，给其他线程；</li>
<li><strong>自旋锁</strong>加锁失败后，线程会<strong>忙等待</strong>，直到它拿到锁；</li>
</ul>
<p>互斥锁是一种「独占锁」，比如当线程 A 加锁成功后，此时互斥锁已经被线程 A 独占了，只要线程 A 没有释放手中的锁，线程 B 加锁就会失败，于是就会释放 CPU 让给其他线程，<strong>既然线程 B 释放掉了 CPU，自然线程 B 加锁的代码就会被阻塞</strong>。</p>
<p><strong>对于互斥锁加锁失败而阻塞的现象，是由操作系统内核实现的</strong>。当加锁失败时，内核会将线程置为「睡眠」状态，等到锁被释放后，内核会在合适的时机唤醒线程，当这个线程成功获取到锁后，于是就可以继续执行。</p>
<p>所以，互斥锁加锁失败时，会从用户态陷入到内核态，让内核帮我们切换线程，虽然简化了使用锁的难度，但是存在一定的性能开销成本。</p>
<p>那这个开销成本是什么呢？会有<strong>两次线程上下文切换的成本</strong>：</p>
<ul>
<li>当线程加锁失败时，内核会把线程的状态从「运行」状态设置为「睡眠」状态，然后把 CPU 切换给其他线程运行；</li>
<li>接着，当锁被释放时，之前「睡眠」状态的线程会变为「就绪」状态，然后内核会在合适的时间，把 CPU 切换给该线程运行。</li>
</ul>
<p>所以，<strong>如果你能确定被锁住的代码执行时间很短，就不应该用互斥锁，而应该选用自旋锁，否则使用互斥锁。</strong></p>
<p>自旋锁是通过 CPU 提供的 <code>CAS</code> 函数（<em>Compare And Swap</em>），在「用户态」完成加锁和解锁操作，不会主动产生线程上下文切换，所以相比互斥锁来说，会快一些，开销也小一些。</p>
<p>一般加锁的过程，包含两个步骤：</p>
<ul>
<li>第一步，查看锁的状态，如果锁是空闲的，则执行第二步；</li>
<li>第二步，将锁设置为当前线程持有；</li>
</ul>
<p>CAS 函数就把这两个步骤合并成一条硬件级指令，形成<strong>原子指令</strong>，这样就保证了这两个步骤是不可分割的，要么一次性执行完两个步骤，要么两个步骤都不执行。</p>
<p>自旋锁是最比较简单的一种锁，一直自旋，利用 CPU 周期，直到锁可用。<strong>需要注意，在单核 CPU 上，需要抢占式的调度器（即不断通过时钟中断一个线程，运行其他线程）。否则，自旋锁在单 CPU 上无法使用，因为一个自旋的线程永远不会放弃 CPU。</strong></p>
<p>自旋锁开销少，在多核系统下一般不会主动产生线程切换，适合异步、协程等在用户态切换请求的编程方式，但如果被锁住的代码执行时间过长，自旋的线程会长时间占用 CPU 资源，所以自旋的时间和被锁住的代码执行的时间是成「正比」的关系，我们需要清楚的知道这一点。</p>
<p>自旋锁与互斥锁使用层面比较相似，但实现层面上完全不同：<strong>当加锁失败时，互斥锁用「线程切换」来应对，自旋锁则用「忙等待」来应对</strong>。</p>
<p>它俩是锁的最基本处理方式，<strong>更高级的锁都会选择其中一个来实现，比如读写锁既可以选择互斥锁实现，也可以基于自旋锁实现</strong>。</p>
<p><strong>读写锁</strong></p>
<p>读写锁从字面意思我们也可以知道，它由「读锁」和「写锁」两部分构成，如果只读取共享资源用「读锁」加锁，如果要修改共享资源则用「写锁」加锁。</p>
<p>所以，<strong>读写锁适用于能明确区分读操作和写操作的场景</strong>。</p>
<p>读写锁的工作原理是：</p>
<ul>
<li>当「写锁」没有被线程持有时，多个线程能够并发地持有读锁，这大大提高了共享资源的访问效率，因为「读锁」是用于读取共享资源的场景，所以多个线程同时持有读锁也不会破坏共享资源的数据。</li>
<li>但是，一旦「写锁」被线程持有后，读线程的获取读锁的操作会被阻塞，而且其他写线程的获取写锁的操作也会被阻塞。</li>
</ul>
<p>所以说，写锁是独占锁，因为任何时刻只能有一个线程持有写锁，类似互斥锁和自旋锁，而读锁是共享锁，因为读锁可以被多个线程同时持有。</p>
<p>知道了读写锁的工作原理后，我们可以发现，<strong>读写锁在读多写少的场景，能发挥出优势</strong>。</p>
<p>另外，根据实现的不同，读写锁可以分为「读优先锁」和「写优先锁」。</p>
<p>读优先锁期望的是，读锁能被更多的线程持有，以便提高读线程的并发性，它的工作方式是：当读线程 A 先持有了读锁，写线程 B 在获取写锁的时候，会被阻塞，并且在阻塞过程中，后续来的读线程 C 仍然可以成功获取读锁，最后直到读线程 A 和 C 释放读锁后，写线程 B 才可以成功获取写锁。</p>
<p>而「写优先锁」是优先服务写线程，其工作方式是：当读线程 A 先持有了读锁，写线程 B 在获取写锁的时候，会被阻塞，并且在阻塞过程中，后续来的读线程 C 获取读锁时会失败，于是读线程 C 将被阻塞在获取读锁的操作，这样只要读线程 A 释放读锁后，写线程 B 就可以成功获取写锁。</p>
<p>读优先锁对于读线程并发性更好，但也不是没有问题。我们试想一下，如果一直有读线程获取读锁，那么写线程将永远获取不到写锁，这就造成了写线程「饥饿」的现象。</p>
<p>写优先锁可以保证写线程不会饿死，但是如果一直有写线程获取写锁，读线程也会被「饿死」。</p>
<p>既然不管优先读锁还是写锁，对方可能会出现饿死问题，那么我们就不偏袒任何一方，搞个「公平读写锁」。</p>
<p><strong>公平读写锁比较简单的一种方式是：用队列把获取锁的线程排队，不管是写线程还是读线程都按照先进先出的原则加锁即可，这样读线程仍然可以并发，也不会出现「饥饿」的现象。</strong></p>
<p>互斥锁和自旋锁都是最基本的锁，读写锁可以根据场景来选择这两种锁其中的一个进行实现。</p>
<p><strong>乐观锁与悲观锁</strong></p>
<p>前面提到的互斥锁、自旋锁、读写锁，都是属于悲观锁。</p>
<p>悲观锁做事比较悲观，它认为<strong>多线程同时修改共享资源的概率比较高，于是很容易出现冲突，所以访问共享资源前，先要上锁</strong>。</p>
<p>那相反的，如果多线程同时修改共享资源的概率比较低，就可以采用乐观锁。</p>
<p>乐观锁做事比较乐观，它假定冲突的概率很低，它的工作方式是：<strong>先修改完共享资源，再验证这段时间内有没有发生冲突，如果没有其他线程在修改资源，那么操作完成，如果发现有其他线程已经修改过这个资源，就放弃本次操作</strong>。</p>
<p>可见，乐观锁的心态是，不管三七二十一，先改了资源再说。另外，你会发现<strong>乐观锁全程并没有加锁，所以它也叫无锁编程</strong>。</p>
<p>这里举一个场景例子：在线文档。</p>
<p>我们都知道在线文档可以同时多人编辑的，如果使用了悲观锁，那么只要有一个用户正在编辑文档，此时其他用户就无法打开相同的文档了，这用户体验当然不好了。</p>
<p>那实现多人同时编辑，实际上是用了乐观锁，它允许多个用户打开同一个文档进行编辑，编辑完提交之后才验证修改的内容是否有冲突。</p>
<p>服务端要怎么验证是否冲突了呢？通常方案如下：</p>
<ul>
<li>由于发生冲突的概率比较低，所以先让用户编辑文档，但是浏览器在下载文档时会记录下服务端返回的文档版本号；</li>
<li>当用户提交修改时，发给服务端的请求会带上原始文档版本号，服务器收到后将它与当前版本号进行比较，如果版本号一致则修改成功，否则提交失败。</li>
</ul>
<p>实际上，我们常见的 SVN 和 Git 也是用了乐观锁的思想，先让用户编辑代码，然后提交的时候，通过版本号来判断是否产生了冲突，发生了冲突的地方，需要我们自己修改后，再重新提交。</p>
<p>乐观锁虽然去除了加锁解锁的操作，但是一旦发生冲突，重试的成本非常高，所以<strong>只有在冲突概率非常低，且加锁成本非常高的场景时，才考虑使用乐观锁。</strong></p>
<h4 id="信号量-1"><a href="#信号量-1" class="headerlink" title="信号量"></a>信号量</h4><p>信号量是操作系统提供的一种协调共享资源访问的方法。</p>
<p>通常<strong>信号量表示资源的数量</strong>，对应的变量是一个整型（<code>sem</code>）变量。</p>
<p>另外，还有<strong>两个原子操作的系统调用函数来控制信号量的</strong>，分别是：</p>
<ul>
<li><em>P 操作</em>：将 <code>sem</code> 减 <code>1</code>，相减后，如果 <code>sem &lt; 0</code>，则进程&#x2F;线程进入阻塞等待，否则继续，表明 P 操作可能会阻塞；</li>
<li><em>V 操作</em>：将 <code>sem</code> 加 <code>1</code>，相加后，如果 <code>sem &lt;= 0</code>，唤醒一个等待中的进程&#x2F;线程，表明 V 操作不会阻塞；</li>
</ul>
<p>P 操作是用在进入临界区之前，V 操作是用在离开临界区之后，这两个操作是必须成对出现的。</p>
<blockquote>
<p>操作系统是如何实现 PV 操作的呢？</p>
</blockquote>
<p>信号量数据结构与 PV 操作的算法描述如下图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/17-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9FPV%E7%AE%97%E6%B3%95%E6%8F%8F%E8%BF%B0.jpg" alt="PV 操作的算法描述"></p>
<p>PV 操作的函数是由操作系统管理和实现的，所以操作系统已经使得执行 PV 函数时是具有原子性的。</p>
<h4 id="生产者消费者问题"><a href="#生产者消费者问题" class="headerlink" title="生产者消费者问题"></a>生产者消费者问题</h4><p>生产者-消费者问题描述：</p>
<ul>
<li><strong>生产者</strong>在生成数据后，放在一个缓冲区中；</li>
<li><strong>消费者</strong>从缓冲区取出数据处理；</li>
<li>任何时刻，<strong>只能有一个</strong>生产者或消费者可以访问缓冲区；</li>
</ul>
<p>我们对问题分析可以得出：</p>
<ul>
<li>任何时刻只能有一个线程操作缓冲区，说明操作缓冲区是临界代码，<strong>需要互斥</strong>；</li>
<li>缓冲区空时，消费者必须等待生产者生成数据；缓冲区满时，生产者必须等待消费者取出数据。说明生产者和消费者<strong>需要同步</strong>。</li>
</ul>
<p>那么我们需要三个信号量，分别是：</p>
<ul>
<li>互斥信号量 <code>mutex</code>：用于互斥访问缓冲区，初始化值为 1；</li>
<li>资源信号量 <code>fullBuffers</code>：用于消费者询问缓冲区是否有数据，有数据则读取数据，初始化值为 0（表明缓冲区一开始为空）；</li>
<li>资源信号量 <code>emptyBuffers</code>：用于生产者询问缓冲区是否有空位，有空位则生成数据，初始化值为 n （缓冲区大小）；</li>
</ul>
<h4 id="哲学家就餐问题"><a href="#哲学家就餐问题" class="headerlink" title="哲学家就餐问题"></a>哲学家就餐问题</h4><p>先来看看哲学家就餐的问题描述：</p>
<ul>
<li><code>5</code> 个老大哥哲学家，闲着没事做，围绕着一张圆桌吃面；</li>
<li>巧就巧在，这个桌子只有 <code>5</code> 支叉子，每两个哲学家之间放一支叉子；</li>
<li>哲学家围在一起先思考，思考中途饿了就会想进餐；</li>
<li><strong>奇葩的是，这些哲学家要两支叉子才愿意吃面，也就是需要拿到左右两边的叉子才进餐</strong>；</li>
<li><strong>吃完后，会把两支叉子放回原处，继续思考</strong>；</li>
</ul>
<p>那么问题来了，如何保证哲 学家们的动作有序进行，而不会出现有人永远拿不到叉子呢？</p>
<blockquote>
<p>方案一</p>
</blockquote>
<p>我们用信号量的方式，也就是 PV 操作来尝试解决它，哲学家先拿左边叉子，再拿右边叉子，拿完两个叉子后进餐，吃完之后放回叉子。</p>
<p>不过，这种解法存在一个极端的问题：<strong>假设五位哲学家同时拿起左边的叉子，桌面上就没有叉子了， 这样就没有人能够拿到他们右边的叉子，也就说每一位哲学家都会在 <code>P(fork[(i + 1) % N ])</code> 这条语句阻塞了，很明显这发生了死锁的现象</strong>。</p>
<blockquote>
<p>方案二</p>
</blockquote>
<p>既然「方案一」会发生同时竞争左边叉子导致死锁的现象，那么我们就在拿叉子前，加个互斥信号量，代码如下：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/26-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%BA%8C%E7%A4%BA%E4%BE%8B.jpg" alt="img"></p>
<p>上面程序中的互斥信号量的作用就在于，<strong>只要有一个哲学家进入了「临界区」，也就是准备要拿叉子时，其他哲学家都不能动，只有这位哲学家用完叉子了，才能轮到下一个哲学家进餐。</strong></p>
<p>方案二虽然能让哲学家们按顺序吃饭，但是每次进餐只能有一位哲学家，而桌面上是有 5 把叉子，按道理是能可以有两个哲学家同时进餐的，所以从效率角度上，这不是最好的解决方案。</p>
<blockquote>
<p>方案三</p>
</blockquote>
<p>那既然方案二使用互斥信号量，会导致只能允许一个哲学家就餐，那么我们就不用它。</p>
<p>另外，方案一的问题在于，会出现所有哲学家同时拿左边刀叉的可能性，那我们就避免哲学家可以同时拿左边的刀叉，采用分支结构，根据哲学家的编号的不同，而采取不同的动作。</p>
<p><strong>即让偶数编号的哲学家「先拿左边的叉子后拿右边的叉子」，奇数编号的哲学家「先拿右边的叉子后拿左边的叉子」。</strong></p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/28-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%B8%89%E7%A4%BA%E4%BE%8B.jpg" alt="img"></p>
<p>上面的程序，在 P 操作时，根据哲学家的编号不同，拿起左右两边叉子的顺序不同。另外，V 操作是不需要分支的，因为 V 操作是不会阻塞的。</p>
<p>方案三即不会出现死锁，也可以两人同时进餐。</p>
<blockquote>
<p>方案四</p>
</blockquote>
<p>在这里再提出另外一种可行的解决方案，我们<strong>用一个数组 state 来记录每一位哲学家的三个状态，分别是在进餐状态、思考状态、饥饿状态（正在试图拿叉子）。</strong></p>
<p>那么，<strong>一个哲学家只有在两个邻居都没有进餐时，才可以进入进餐状态。</strong></p>
<p>第 <code>i</code> 个哲学家的左邻右舍，则由宏 <code>LEFT</code> 和 <code>RIGHT</code> 定义：</p>
<ul>
<li><em>LEFT</em> : ( i + 5 - 1 ) % 5</li>
<li><em>RIGHT</em> : ( i + 1 ) % 5</li>
</ul>
<p>比如 i 为 2，则 <code>LEFT</code> 为 1，<code>RIGHT</code> 为 3。</p>
<p>具体代码实现如下：<br><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/30-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E5%9B%9B%E7%A4%BA%E4%BE%8B.jpg" alt="img"></p>
<p>上面的程序使用了一个信号量数组，每个信号量对应一位哲学家，这样在所需的叉子被占用时，想进餐的哲学家就被阻塞。</p>
<p>注意，每个进程&#x2F;线程将 <code>smart_person</code> 函数作为主代码运行，而其他 <code>take_forks</code>、<code>put_forks</code> 和 <code>test</code> 只是普通的函数，而非单独的进程&#x2F;线程。</p>
<p>方案四同样不会出现死锁，也可以两人同时进餐。</p>
<h4 id="读者-写者问题"><a href="#读者-写者问题" class="headerlink" title="读者-写者问题"></a>读者-写者问题</h4><p>前面的「哲学家进餐问题」对于互斥访问有限的竞争问题（如 I&#x2F;O 设备）一类的建模过程十分有用。</p>
<p>另外，还有个著名的问题是「读者-写者」，它为数据库访问建立了一个模型。</p>
<p>读者只会读取数据，不会修改数据，而写者即可以读也可以修改数据。</p>
<p>读者-写者的问题描述：</p>
<ul>
<li>「读-读」允许：同一时刻，允许多个读者同时读</li>
<li>「读-写」互斥：没有写者时读者才能读，没有读者时写者才能写</li>
<li>「写-写」互斥：没有其他写者时，写者才能写</li>
</ul>
<p>接下来，提出几个解决方案来分析分析。</p>
<blockquote>
<p>方案一</p>
</blockquote>
<p>使用信号量的方式来尝试解决：</p>
<ul>
<li>信号量 <code>wMutex</code>：控制写操作的互斥信号量，初始值为 1 ；</li>
<li>读者计数 <code>rCount</code>：正在进行读操作的读者个数，初始化为 0；</li>
<li>信号量 <code>rCountMutex</code>：控制对 rCount 读者计数器的互斥修改，初始值为 1；</li>
</ul>
<p>接下来看看代码的实现：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/32-%E8%AF%BB%E8%80%85%E5%86%99%E8%80%85-%E6%96%B9%E6%A1%88%E4%B8%80%E7%A4%BA%E4%BE%8B.jpg" alt="img"></p>
<p>上面的这种实现，是读者优先的策略，因为只要有读者正在读的状态，后来的读者都可以直接进入，如果读者持续不断进入，则写者会处于饥饿状态。</p>
<blockquote>
<p>方案二</p>
</blockquote>
<p>那既然有读者优先策略，自然也有写者优先策略：</p>
<ul>
<li>只要有写者准备要写入，写者应尽快执行写操作，后来的读者就必须阻塞；</li>
<li>如果有写者持续不断写入，则读者就处于饥饿；</li>
</ul>
<p>在方案一的基础上新增如下变量：</p>
<ul>
<li>信号量 <code>rMutex</code>：控制读者进入的互斥信号量，初始值为 1；</li>
<li>信号量 <code>wDataMutex</code>：控制写者写操作的互斥信号量，初始值为 1；</li>
<li>写者计数 <code>wCount</code>：记录写者数量，初始值为 0；</li>
<li>信号量 <code>wCountMutex</code>：控制 wCount 互斥修改，初始值为 1；</li>
</ul>
<p>具体实现如下代码：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/33-%E8%AF%BB%E8%80%85%E5%86%99%E8%80%85-%E6%96%B9%E6%A1%88%E4%BA%8C%E7%A4%BA%E4%BE%8B.jpg" alt="img"></p>
<p>注意，这里 <code>rMutex</code> 的作用，开始有多个读者读数据，它们全部进入读者队列，此时来了一个写者，执行了 <code>P(rMutex)</code> 之后，后续的读者由于阻塞在 <code>rMutex</code> 上，都不能再进入读者队列，而写者到来，则可以全部进入写者队列，因此保证了写者优先。</p>
<p>同时，第一个写者执行了 <code>P(rMutex)</code> 之后，也不能马上开始写，必须等到所有进入读者队列的读者都执行完读操作，通过 <code>V(wDataMutex)</code> 唤醒写者的写操作。</p>
<blockquote>
<p>方案三</p>
</blockquote>
<p>既然读者优先策略和写者优先策略都会造成饥饿的现象，那么我们就来实现一下公平策略。</p>
<p>公平策略：</p>
<ul>
<li>优先级相同；</li>
<li>写者、读者互斥访问；</li>
<li>只能一个写者访问临界区；</li>
<li>可以有多个读者同时访问临界资源；</li>
</ul>
<p>具体代码实现：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/34-%E8%AF%BB%E8%80%85%E5%86%99%E8%80%85-%E6%96%B9%E6%A1%88%E4%B8%89%E7%A4%BA%E4%BE%8B.jpg" alt="img"></p>
<p>看完代码不知你是否有这样的疑问，为什么加了一个信号量 <code>flag</code>，就实现了公平竞争？</p>
<p>对比方案一的读者优先策略，可以发现，读者优先中只要后续有读者到达，读者就可以进入读者队列， 而写者必须等待，直到没有读者到达。</p>
<p>没有读者到达会导致读者队列为空，即 <code>rCount==0</code>，此时写者才可以进入临界区执行写操作。</p>
<p>而这里 <code>flag</code> 的作用就是阻止读者的这种特殊权限（特殊权限是只要读者到达，就可以进入读者队列）。</p>
<p>比如：开始来了一些读者读数据，它们全部进入读者队列，此时来了一个写者，执行 <code>P(flag)</code> 操作，使得后续到来的读者都阻塞在 <code>flag</code> 上，不能进入读者队列，这会使得读者队列逐渐为空，即 <code>rCount</code> 减为 0。</p>
<p>这个写者也不能立马开始写（因为此时读者队列不为空），会阻塞在信号量 <code>wDataMutex</code> 上，读者队列中的读者全部读取结束后，最后一个读者进程执行 <code>V(wDataMutex)</code>，唤醒刚才的写者，写者则继续开始进行写操作。</p>
<h4 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h4><p><strong>死锁（Deadlock）：</strong>死锁问题的产生是由两个或者以上线程并行执行的时候，争夺资源而互相等待造成的。若无外力作用，它们都将无法推进下去。称此时系统处于死锁状态或系统产生了死锁。</p>
<p>死锁只有<strong>同时满足</strong>以下四个条件才会发生：</p>
<ul>
<li>互斥条件：<strong>多个线程不能同时使用同一个资源</strong>。</li>
<li>持有并等待条件：当线程 A 已经持有了资源 1，又想申请资源 2，而资源 2 已经被线程 B 持有了，所以线程 A 就会处于等待状态，但是<strong>线程 A 在等待资源 2 的同时并不会释放自己已经持有的资源 1</strong>。</li>
<li>不可剥夺条件：当线程已经持有了资源 ，<strong>在自己使用完之前不能被其他线程获取</strong>，线程 B 如果也想使用此资源，则只能在线程 A 使用完并释放后才能获取。</li>
<li>环路等待条件：在死锁发生的时候，<strong>多个线程获取资源的顺序构成了环形链</strong>。</li>
</ul>
<h5 id="排查死锁"><a href="#排查死锁" class="headerlink" title="排查死锁"></a>排查死锁</h5><p>在 Linux 下，我们可以使用 <code>pstack</code> + <code>gdb</code> 工具来定位死锁问题。</p>
<p>pstack 命令可以显示每个线程的栈跟踪信息（函数调用过程），它的使用方式也很简单，只需要 <code>pstack &lt;pid&gt;</code> 就可以了。</p>
<p>那么，在定位死锁问题时，我们可以多次执行 pstack 命令查看线程的函数调用过程，多次对比结果，确认哪几个线程一直没有变化，且是因为在等待锁，那么大概率是由于死锁问题导致的。</p>
<p>但是，还不能够确认这两个线程是在互相等待对方的锁的释放，因为我们看不到它们是等在哪个锁对象，于是我们可以使用 gdb 工具进一步确认。gdb可以查看函数栈帧的信息，以及查看对象的信息。</p>
<h5 id="避免死锁"><a href="#避免死锁" class="headerlink" title="避免死锁"></a>避免死锁</h5><p>前面我们提到，产生死锁的四个必要条件是：互斥条件、持有并等待条件、不可剥夺条件、环路等待条件。</p>
<p>那么避免死锁问题就只需要破环其中一个条件就可以，最常见的并且可行的就是<strong>使用资源有序分配法，来破环环路等待条件</strong>。</p>
<p>那什么是资源有序分配法呢？</p>
<p>线程 A 和 线程 B 获取资源的顺序要一样，当线程 A 是先尝试获取资源 A，然后尝试获取资源 B 的时候，线程 B 同样也是先尝试获取资源 A，然后尝试获取资源 B。也就是说，线程 A 和 线程 B 总是以相同的顺序申请自己想要的资源。</p>
<h3 id="一个进程最多可以创建多少个线程"><a href="#一个进程最多可以创建多少个线程" class="headerlink" title="一个进程最多可以创建多少个线程"></a>一个进程最多可以创建多少个线程</h3><p>这个问题跟两个东西有关系：</p>
<ul>
<li><strong>进程的虚拟内存空间上限</strong>，因为创建一个线程，操作系统需要为其分配一个栈空间，如果线程数量越多，所需的栈空间就要越大，那么虚拟内存就会占用的越多。</li>
<li><strong>系统参数限制</strong>，虽然 Linux 并没有内核参数来控制单个进程创建的最大线程个数，但是有系统级别的参数来控制整个系统的最大线程个数。</li>
</ul>
<p>我们可以执行 ulimit -a 这条命令，查看进程创建线程时默认分配的栈空间大小。</p>
<p>下面这三个内核参数的大小，都会影响创建线程的上限：</p>
<ul>
<li><em><strong>&#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;threads-max</strong></em>，表示系统支持的最大线程数，默认值是 <code>14553</code>；</li>
<li><em><strong>&#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;pid_max</strong></em>，表示系统全局的 PID 号数值的限制，每一个进程或线程都有 ID，ID 的值超过这个数，进程或线程就会创建失败，默认值是 <code>32768</code>；</li>
<li><em><strong>&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;max_map_count</strong></em>，表示限制一个进程可以拥有的VMA(虚拟内存区域)的数量，具体什么意思我也没搞清楚，反正如果它的值很小，也会导致创建线程失败，默认值是 <code>65530</code>。</li>
</ul>
<p>除此之外创建线程还受到cpu的限制，如果更改内核参数到极大值，创建线程过多时会导致cpu被占满，服务器被卡死无法创建新的线程。</p>
<p><strong>总结：</strong></p>
<ul>
<li>32 位系统，用户态的虚拟空间只有 3G，如果创建线程时分配的栈空间是 10M，那么一个进程最多只能创建 300 个左右的线程。</li>
<li>64 位系统，用户态的虚拟空间大到有 128T，如果按创建一个线程需占用 10M 栈空间的情况来算，那么理论上可以创建 128T&#x2F;10M 个线程，也就是 1000多万个线程，理论上不会受虚拟内存大小的限制，而是会受系统参数和cpu性能限制。</li>
</ul>
<h3 id="线程崩溃了，进程也会崩溃吗？"><a href="#线程崩溃了，进程也会崩溃吗？" class="headerlink" title="线程崩溃了，进程也会崩溃吗？"></a>线程崩溃了，进程也会崩溃吗？</h3><p>一般来说如果线程是因为非法访问内存引起的崩溃，那么进程肯定会崩溃，为什么系统要让进程崩溃呢，这主要是因为在进程中，<strong>各个线程的地址空间是共享的</strong>，既然是共享，那么某个线程对地址的非法访问就会导致内存的不确定性，进而可能会影响到其他线程，这种操作是危险的，操作系统会认为这很可能导致一系列严重的后果，于是干脆让整个进程崩溃。</p>
<p>线程共享代码段，数据段，地址空间，文件非法访问内存有以下几种情况，我们以 C 语言举例来看看。</p>
<p>1.、针对只读内存写入数据</p>
<pre><code class="c">   #include &lt;stdio.h&gt;
   #include &lt;stdlib.h&gt;
   
   int main() &#123;
      char *s = &quot;hello world&quot;;
      // 向只读内存写入数据，崩溃
      s[1] = &#39;H&#39;; 
   &#125;
</code></pre>
<p>2、访问了进程没有权限访问的地址空间（比如内核空间）</p>
<pre><code class="c">   #include &lt;stdio.h&gt;
   #include &lt;stdlib.h&gt;

   int main() &#123;
      int *p = (int *)0xC0000fff;
      // 针对进程的内核空间写入数据，崩溃
      *p = 10; 
   &#125;
</code></pre>
<p>在 32 位虚拟地址空间中，p 指向的是内核空间，显然不具有写入权限，所以上述赋值操作会导致崩溃</p>
<p>3、访问了不存在的内存，比如：</p>
<pre><code class="c">   #include &lt;stdio.h&gt;
   #include &lt;stdlib.h&gt;
   
   int main() &#123;
      int *a = NULL;
      *a = 1;     
   &#125;
</code></pre>
<p>以上错误都是访问内存时的错误，所以统一会报 Segment Fault 错误（即段错误），这些都会导致进程崩溃</p>
<h4 id="进程是如何崩溃的-信号机制简介"><a href="#进程是如何崩溃的-信号机制简介" class="headerlink" title="进程是如何崩溃的-信号机制简介"></a>进程是如何崩溃的-信号机制简介</h4><p>那么线程崩溃后，进程是如何崩溃的呢，这背后的机制到底是怎样的，答案是<strong>信号</strong>。</p>
<p>大家想想要干掉一个正在运行的进程是不是经常用 kill -9 pid 这样的命令，这里的 kill 其实就是给指定 pid 发送终止信号的意思，其中的 9 就是信号。</p>
<p>其实信号有很多类型的，在 Linux 中可以通过 <code>kill -l</code>查看所有可用的信号：</p>
<p>当然了发 kill 信号必须具有一定的权限，否则任意进程都可以通过发信号来终止其他进程，那显然是不合理的，实际上 kill 执行的是系统调用，将控制权转移给了内核（操作系统），由内核来给指定的进程发送信号</p>
<p>那么发个信号进程怎么就崩溃了呢，这背后的原理到底是怎样的？</p>
<p>其背后的机制如下</p>
<ol>
<li>CPU 执行正常的进程指令</li>
<li>调用 kill 系统调用向进程发送信号</li>
<li>进程收到操作系统发的信号，CPU 暂停当前程序运行，并将控制权转交给操作系统</li>
<li>调用 kill 系统调用向进程发送信号（假设为 11，即 SIGSEGV，一般非法访问内存报的都是这个错误）</li>
<li><strong>操作系统根据情况执行相应的信号处理程序（函数），一般执行完信号处理程序逻辑后会让进程退出</strong></li>
</ol>
<p>注意上面的第五步，如果进程没有注册自己的信号处理函数，那么操作系统会执行默认的信号处理程序（一般最后会让进程退出），但如果注册了，则会执行自己的信号处理函数，这样的话就给了进程一个垂死挣扎的机会，它收到 kill 信号后，可以调用 exit() 来退出，<strong>但也可以使用 sigsetjmp，siglongjmp 这两个函数来恢复进程的执行</strong></p>
<pre><code class="c">// 自定义信号处理函数示例

#include &lt;stdio.h&gt;
#include &lt;signal.h&gt;
#include &lt;stdlib.h&gt;
// 自定义信号处理函数，处理自定义逻辑后再调用 exit 退出
void sigHandler(int sig) &#123;
  printf(&quot;Signal %d catched!\n&quot;, sig);
  exit(sig);
&#125;
int main(void) &#123;
  signal(SIGSEGV, sigHandler);
  int *p = (int *)0xC0000fff;
  *p = 10; // 针对不属于进程的内核空间写入数据，崩溃
&#125;

// 以上结果输出: Signal 11 catched!
</code></pre>
<p><strong>如代码所示</strong>：注册信号处理函数后，当收到 SIGSEGV 信号后，先执行相关的逻辑再退出</p>
<p>另外当进程接收信号之后也可以不定义自己的信号处理函数，而是选择忽略信号，如下</p>
<pre><code class="c">#include &lt;stdio.h&gt;
#include &lt;signal.h&gt;
#include &lt;stdlib.h&gt;

int main(void) &#123;
  // 忽略信号
  signal(SIGSEGV, SIG_IGN);

  // 产生一个 SIGSEGV 信号
  raise(SIGSEGV);

  printf(&quot;正常结束&quot;);
&#125;
</code></pre>
<p>也就是说虽然给进程发送了 kill 信号，但如果进程自己定义了信号处理函数或者无视信号就有机会逃出生天，当然了 kill -9 命令例外，不管进程是否定义了信号处理函数，都会马上被干掉。</p>
<h4 id="为什么线程崩溃不会导致-JVM-进程崩溃"><a href="#为什么线程崩溃不会导致-JVM-进程崩溃" class="headerlink" title="为什么线程崩溃不会导致 JVM 进程崩溃"></a>为什么线程崩溃不会导致 JVM 进程崩溃</h4><p>现在我们再来看看开头这个问题，相信你多少会心中有数，想想看在 Java 中有哪些是常见的由于非法访问内存而产生的 Exception 或 error 呢，常见的是大家熟悉的 StackoverflowError 或者 NPE（NullPointerException）,NPE 我们都了解，属于是访问了不存在的内存。</p>
<p>那么 stackoverflow 是怎么发生的呢？</p>
<p>进程每调用一个函数，都会分配一个栈桢，然后在栈桢里会分配函数里定义的各种局部变量。</p>
<p>假设现在调用了一个无限递归的函数，那就会持续分配栈帧，但 stack 的大小是有限的（Linux 中默认为 8 M，可以通过 ulimit -a 查看），如果无限递归很快栈就会分配完了，此时再调用函数试图分配超出栈的大小内存，就会发生段错误，也就是 stackoverflowError。</p>
<p>那问题来了，既然 StackoverflowError 或者 NPE 都属于非法访问内存， JVM 为什么不会崩溃呢？</p>
<p>有了上一节的铺垫，相信你不难回答，其实就是<strong>因为 JVM 自定义了自己的信号处理函数，拦截了 SIGSEGV 信号，针对这两者不让它们崩溃，而是自己内部作了额外的处理，其实是恢复了线程的执行，并抛出 StackoverflowError 和 NPE，这就是为什么 JVM 不会崩溃且我们能捕获这两个错误&#x2F;异常的原因</strong>。</p>
<p><strong>如果 JVM 不对信号做额外的处理，最后会自己退出并产生 crash 文件 hs_err_pid_xxx.log（可以通过 -XX:ErrorFile&#x3D;&#x2F;var&#x2F;*log*&#x2F;hs_err.log 这样的方式指定），这个文件记录了虚拟机崩溃的重要原因</strong>。</p>
<p>所以也可以说，虚拟机是否崩溃只要看它是否会产生此崩溃日志文件</p>
<h3 id="调度算法-1"><a href="#调度算法-1" class="headerlink" title="调度算法"></a>调度算法</h3><h4 id="内存页面置换算法"><a href="#内存页面置换算法" class="headerlink" title="内存页面置换算法"></a>内存页面置换算法</h4><p>在了解内存页面置换算法前，我们得先谈一下<strong>缺页异常（缺页中断）</strong>。</p>
<p>当 CPU 访问的页面不在物理内存时，便会产生一个缺页中断，请求操作系统将所缺页调入到物理内存。那它与一般中断的主要区别在于：</p>
<ul>
<li>缺页中断在指令执行「期间」产生和处理中断信号，而一般中断在一条指令执行「完成」后检查和处理中断信号。</li>
<li>缺页中断返回到该指令的开始重新执行「该指令」，而一般中断返回回到该指令的「下一个指令」执行。</li>
</ul>
<p>缺页中断的处理流程：</p>
<ol>
<li>在 CPU 里访问一条 Load M 指令，然后 CPU 会去找 M 所对应的页表项。</li>
<li>如果该页表项的状态位是「有效的」，那 CPU 就可以直接去访问物理内存了，如果状态位是「无效的」，则 CPU 则会发送缺页中断请求。</li>
<li>操作系统收到了缺页中断，则会执行缺页中断处理函数，先会查找该页面在磁盘中的页面的位置。</li>
<li>找到磁盘中对应的页面后，需要把该页面换入到物理内存中，但是在换入前，需要在物理内存中找空闲页，如果找到空闲页，就把页面换入到物理内存中。</li>
<li>页面从磁盘换入到物理内存完成后，则把页表项中的状态位修改为「有效的」。</li>
<li>最后，CPU 重新执行导致缺页异常的指令。</li>
</ol>
<p>上面所说的过程，第 4 步是能在物理内存找到空闲页的情况，那如果找不到呢？</p>
<p>找不到空闲页的话，就说明此时内存已满了，这时候，就需要「页面置换算法」选择一个物理页，如果该物理页有被修改过（脏页），则把它换出到磁盘，然后把该被置换出去的页表项的状态改成「无效的」，最后把正在访问的页面装入到这个物理页中。</p>
<p>这里提一下，页表项通常有如下图的字段：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E9%A1%B5%E8%A1%A8%E9%A1%B9%E5%AD%97%E6%AE%B5.png" alt="img"></p>
<p>那其中：</p>
<ul>
<li><em>状态位</em>：用于表示该页是否有效，也就是说是否在物理内存中，供程序访问时参考。</li>
<li><em>访问字段</em>：用于记录该页在一段时间被访问的次数，供页面置换算法选择出页面时参考。</li>
<li><em>修改位</em>：表示该页在调入内存后是否有被修改过，由于内存中的每一页都在磁盘上保留一份副本，因此，如果没有修改，在置换该页时就不需要将该页写回到磁盘上，以减少系统的开销；如果已经被修改，则将该页重写到磁盘上，以保证磁盘中所保留的始终是最新的副本。</li>
<li><em>硬盘地址</em>：用于指出该页在硬盘上的地址，通常是物理块号，供调入该页时使用。</li>
</ul>
<p>所以，页面置换算法的功能是，<strong>当出现缺页异常，需调入新页面而内存已满时，选择被置换的物理页面</strong>，也就是说选择一个物理页面换出到磁盘，然后把需要访问的页面换入到物理页。</p>
<h5 id="最佳页面置换算法"><a href="#最佳页面置换算法" class="headerlink" title="最佳页面置换算法"></a>最佳页面置换算法</h5><p>最佳页面置换算法基本思路是，<strong>置换在「未来」最长时间不访问的页面</strong>。</p>
<p>所以，该算法实现需要计算内存中每个逻辑页面的「下一次」访问时间，然后比较，选择未来最长时间不访问的页面。</p>
<p>这很理想，但是实际系统中无法实现，因为程序访问页面时是动态的，我们是无法预知每个页面在「下一次」访问前的等待时间。</p>
<p>所以，最佳页面置换算法作用是为了衡量你的算法的效率，你的算法效率越接近该算法的效率，那么说明你的算法是高效的。</p>
<h5 id="先进先出置换算法"><a href="#先进先出置换算法" class="headerlink" title="先进先出置换算法"></a>先进先出置换算法</h5><p>既然我们无法预知页面在下一次访问前所需的等待时间，那我们可以<strong>选择在内存驻留时间最长的页面进行中置换</strong>，这个就是「先进先出置换」算法的思想。</p>
<p>因为最早的页面并不一定是缺少访问的页面，所以先进先出效率很低。</p>
<h5 id="最近最久未使用的置换算法"><a href="#最近最久未使用的置换算法" class="headerlink" title="最近最久未使用的置换算法"></a>最近最久未使用的置换算法</h5><p>最近最久未使用（<em>LRU</em>）的置换算法的基本思路是，发生缺页时，<strong>选择最长时间没有被访问的页面进行置换</strong>，也就是说，该算法假设已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用。</p>
<p>这种算法近似最优置换算法，最优置换算法是通过「未来」的使用情况来推测要淘汰的页面，而 LRU 则是通过「历史」的使用情况来推测要淘汰的页面。</p>
<p>虽然 LRU 在理论上是可以实现的，但代价很高。为了完全实现 LRU，需要在内存中维护一个所有页面的链表，最近最多使用的页面在表头，最近最少使用的页面在表尾。</p>
<p>困难的是，在每次访问内存时都必须要更新「整个链表」。在链表中找到一个页面，删除它，然后把它移动到表头是一个非常费时的操作。</p>
<p>所以，LRU 虽然看上去不错，但是由于开销比较大，实际应用中比较少使用。</p>
<h5 id="时钟页面置换算法"><a href="#时钟页面置换算法" class="headerlink" title="时钟页面置换算法"></a>时钟页面置换算法</h5><p>那有没有一种即能优化置换的次数，也能方便实现的算法呢？</p>
<p>时钟页面置换算法就可以两者兼得，它跟 LRU 近似，又是对 FIFO 的一种改进。</p>
<p>该算法的思路是，把所有的页面都保存在一个类似钟面的「环形链表」中，一个表针指向最老的页面。</p>
<p>当发生缺页中断时，算法顺时针遍历环形链表：</p>
<ul>
<li>如果它的访问位位是 0 就淘汰该页面，并把新的页面插入这个位置，然后把表针前移一个位置；</li>
<li>如果访问位是 1 就清除访问位，并把表针前移一个位置，重复这个过程直到找到了一个访问位为 0 的页面为止；</li>
</ul>
<p>时钟算法会淘汰掉过去一段时间没有访问过的页面，跟LRU类似。区别在于时钟算法不需要把最近访问过的页面移动到表头，开销小了一些。</p>
<h5 id="最不常用算法"><a href="#最不常用算法" class="headerlink" title="最不常用算法"></a>最不常用算法</h5><p>最不常用（<em>LFU</em>）算法，这名字听起来很调皮，但是它的意思不是指这个算法不常用，而是<strong>当发生缺页中断时，选择「访问次数」最少的那个页面，并将其淘汰</strong>。</p>
<p>它的实现方式是，对每个页面设置一个「访问计数器」，每当一个页面被访问时，该页面的访问计数器就累加 1。在发生缺页中断时，淘汰计数器值最小的那个页面。</p>
<p>看起来很简单，每个页面加一个计数器就可以实现了，但是在操作系统中实现的时候，我们需要考虑效率和硬件成本的。</p>
<p>要增加一个计数器来实现，这个硬件成本是比较高的，另外如果要对这个计数器查找哪个页面访问次数最小，查找链表本身，如果链表长度很大，是非常耗时的，效率不高。</p>
<p>但还有个问题，LFU 算法只考虑了频率问题，没考虑时间的问题，比如有些页面在过去时间里访问的频率很高，但是现在已经没有访问了，而当前频繁访问的页面由于没有这些页面访问的次数高，在发生缺页中断时，就会可能会误伤当前刚开始频繁访问，但访问次数还不高的页面。</p>
<p>那这个问题的解决的办法还是有的，可以定期减少访问的次数，比如当发生时间中断时，把过去时间访问的页面的访问次数除以 2，也就说，随着时间的流失，以前的高访问次数的页面会慢慢减少，相当于加大了被置换的概率。</p>
<h4 id="磁盘调度算法"><a href="#磁盘调度算法" class="headerlink" title="磁盘调度算法"></a>磁盘调度算法</h4><p>磁盘调度算法的目的很简单，就是为了提高磁盘的访问性能，一般是通过优化磁盘的访问请求顺序来做到的。</p>
<p>寻道的时间是磁盘访问最耗时的部分，如果请求顺序优化的得当，必然可以节省一些不必要的寻道时间，从而提高磁盘的访问性能。</p>
<h5 id="先来先服务"><a href="#先来先服务" class="headerlink" title="先来先服务"></a>先来先服务</h5><p>先来先服务（<em>First-Come，First-Served，FCFS</em>），顾名思义，先到来的请求，先被服务，按照请求序列的顺序依次移动磁头到磁道。</p>
<p>这种算法，比较简单粗暴，如果大量进程竞争使用磁盘，请求访问的磁道可能会很分散，那先来先服务算法在性能上就会显得很差，因为寻道时间过长。</p>
<h5 id="最短寻道时间优先"><a href="#最短寻道时间优先" class="headerlink" title="最短寻道时间优先"></a>最短寻道时间优先</h5><p>最短寻道时间优先（<em>Shortest Seek First，SSF</em>）算法的工作方式是，优先选择从当前磁头位置所需寻道时间最短的请求。</p>
<p>相比先来先服务性能提高了不少，但这个算法可能存在某些请求的<strong>饥饿</strong>，假设是一个动态的请求，如果后续来的请求都比之前的请求更近，那么之前的请求可能永远不会被响应，于是就产生了饥饿现象，这里<strong>产生饥饿的原因是磁头在一小块区域来回移动</strong>。</p>
<h5 id="扫描算法"><a href="#扫描算法" class="headerlink" title="扫描算法"></a>扫描算法</h5><p>最短寻道时间优先算法会产生饥饿的原因在于：磁头有可能再一个小区域内来回得移动。</p>
<p>为了防止这个问题，可以规定：<strong>磁头在一个方向上移动，访问所有未完成的请求，直到磁头到达该方向上的最后的磁道，才调换方向，这就是扫描（Scan）算法</strong>。</p>
<p><strong>这种算法也叫做电梯算法</strong>，比如电梯保持按一个方向移动，直到在那个方向上没有请求为止，然后改变方向。</p>
<p>磁头先响应一边的请求(比如左边)，直到到达最左端（ 0 磁道）后，才开始反向移动，响应右边的请求。</p>
<p>扫描调度算法性能较好，不会产生饥饿现象，但是存在这样的问题，中间部分的磁道会比较占便宜，中间部分相比其他部分响应的频率会比较多，也就是说每个磁道的响应频率存在差异。</p>
<h5 id="循环扫描算法"><a href="#循环扫描算法" class="headerlink" title="循环扫描算法"></a>循环扫描算法</h5><p>扫描算法使得每个磁道响应的频率存在差异，那么要优化这个问题的话，可以总是按相同的方向进行扫描，使得每个磁道的响应频率基本一致。</p>
<p>循环扫描（<em>Circular Scan, CSCAN</em> ）规定：只有磁头朝某个特定方向移动时(如向右)，才处理磁道访问请求，移动到最右端后需要复位磁头，也就是直接快速移动至最靠左的磁道，这个过程是很快的，并且<strong>复位中途不处理任何请求</strong>，该算法的特点，就是<strong>磁道只响应一个方向上的请求</strong>。</p>
<p>以这个序列为例子，磁头的初始位置是 53：</p>
<p>98，183，37，122，14，124，65，67</p>
<p>那么，假设循环扫描调度算先朝磁道增加的方向移动，具体请求会是下列从左到右的顺序：</p>
<p>65，67，98，122，124，183，<code>199</code>，<code>0</code>，14，37</p>
<p>循环扫描算法相比于扫描算法，对于各个位置磁道响应频率相对比较平均。</p>
<h5 id="LOOK-与-C-LOOK算法"><a href="#LOOK-与-C-LOOK算法" class="headerlink" title="LOOK 与 C-LOOK算法"></a>LOOK 与 C-LOOK算法</h5><p>我们前面说到的扫描算法和循环扫描算法，都是磁头移动到磁盘「最始端或最末端」才开始调换方向。</p>
<p>那这其实是可以优化的，优化的思路就是<strong>磁头在移动到「最远的请求」位置，然后立即反向移动。</strong></p>
<p>那针对扫描算法的优化则叫 LOOK 算法，它的工作方式，磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，<strong>反向移动的途中会响应请求</strong>。</p>
<p>而针对循环扫描算法的优化则叫 C-LOOK，它的工作方式，磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，<strong>反向移动的途中不会响应请求</strong>。</p>
<h2 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h2><p>文件系统是管理磁盘的软件系统，它简化了用户对磁盘空间的使用方法，并降低了磁盘空间的使用难度，通过更加形象的方式将磁盘中的数据展示给用户。</p>
<p>文件系统实现对磁盘空间的统一管理，<strong>一方面文件系统对磁盘空间进行统一规划，另外一方面文件系统提供给普通用户人性化的接口</strong>。就好比仓库中的货架，将空间进行规划和编排，这样根据编号可以方便的找到具体的货物。而文件系统也是类似，将磁盘空间进行规划和编号处理，<strong>这样通过文件名就可以找到具体的数据</strong>，而不用关心数据到底是怎么存储的。如果没有文件系统，数据被毫无规律的放到磁盘上，最后查找的时候会非常费劲，甚至可能找不到需要的数据。</p>
<p>没有文件系统的磁盘就像一个空仓库，数据被杂乱的堆放；有了文件系统相当于给仓库加了货架，将空间统一规划和编排。</p>
<p>Linux 最经典的一句话是：「<strong>一切皆文件</strong>」，不仅普通的文件和目录，就连块设备、管道、socket 等，也都是统一交给文件系统管理的。</p>
<p>Linux 文件系统会为每个文件分配两个数据结构：<strong>索引节点（index node）和目录项（directory entry）</strong>，它们主要用来记录文件的元信息和目录层次结构。</p>
<ul>
<li>索引节点，也就是 <em>inode</em>，用来记录文件的元信息，比如 inode 编号、文件大小、访问权限、创建时间、修改时间、<strong>数据在磁盘的位置</strong>等等。索引节点是文件的<strong>唯一</strong>标识，它们之间一一对应，也同样都会被存储在硬盘中，所以<strong>索引节点同样占用磁盘空间</strong>。</li>
<li>目录项，也就是 <em>dentry</em>，用来记录文件的名字、<strong>索引节点指针</strong>以及与其他目录项的层级关联关系。多个目录项关联起来，就会形成目录结构，但它与索引节点不同的是，<strong>目录项是由内核维护的一个数据结构，不存放于磁盘，而是缓存在内存</strong>。</li>
</ul>
<p>由于索引节点唯一标识一个文件，而目录项记录着文件的名字，所以目录项和索引节点的关系是多对一，也就是说，<strong>一个文件可以有多个别名。</strong>比如，硬链接的实现就是多个目录项中的索引节点指向同一个文件。</p>
<p>注意，目录也是文件，也是用索引节点唯一标识，持久化存储在磁盘。和普通文件不同的是，普通文件在磁盘里面保存的是文件数据，而目录文件在磁盘里面保存子目录或文件；目录项是内核一个数据结构，缓存在内存。</p>
<p>如果查询目录频繁从磁盘读，效率会很低，所以内核会把已经读过的目录用目录项这个数据结构缓存在内存，下次再次读到相同的目录时，只需从内存读就可以，大大提高了文件系统的效率。</p>
<p>注意，目录项这个数据结构不只是表示目录，也是可以表示文件的。</p>
<blockquote>
<p>那文件数据是如何存储在磁盘的呢？</p>
</blockquote>
<p>磁盘读写的最小单位是<strong>扇区</strong>，扇区的大小只有 <code>512B</code> 大小，很明显，如果每次读写都以这么小为单位，那这读写的效率会非常低。</p>
<p>所以，文件系统把多个扇区组成了一个<strong>逻辑块</strong>，每次读写的最小单位就是逻辑块（数据块），Linux 中的逻辑块大小为 <code>4KB</code>，也就是一次性读写 8 个扇区，这将大大提高了磁盘的读写的效率。</p>
<p>以上就是索引节点、目录项以及文件数据的关系，下面这个图就很好的展示了它们之间的关系：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%9B%AE%E5%BD%95%E9%A1%B9%E5%92%8C%E7%B4%A2%E5%BC%95%E5%85%B3%E7%B3%BB%E5%9B%BE.png" alt="img"></p>
<p>索引节点是存储在硬盘上的数据，那么为了加速文件的访问，通常会把索引节点加载到内存中。</p>
<p>另外，磁盘进行格式化的时候，会被分成三个存储区域，分别是超级块、索引节点区和数据块区。</p>
<ul>
<li><em>超级块</em>，用来存储文件系统的详细信息，比如块个数、块大小、空闲块等等。</li>
<li><em>索引节点区</em>，用来存储索引节点；</li>
<li><em>数据块区</em>，用来存储文件或目录数据；</li>
</ul>
<p>我们不可能把超级块和索引节点区全部加载到内存，这样内存肯定撑不住，所以只有当需要使用的时候，才将其加载进内存，它们加载进内存的时机是不同的：</p>
<ul>
<li>超级块：当文件系统挂载时进入内存；</li>
<li>索引节点区：当文件被访问时进入内存；</li>
</ul>
<h3 id="虚拟文件系统"><a href="#虚拟文件系统" class="headerlink" title="虚拟文件系统"></a>虚拟文件系统</h3><p>文件系统的种类众多，而操作系统希望<strong>对用户提供一个统一的接口</strong>，于是在用户层与文件系统层引入了中间层，这个中间层就称为<strong>虚拟文件系统（Virtual File System，VFS）。</strong></p>
<p>VFS 定义了一组所有文件系统都支持的数据结构和标准接口，这样程序员不需要了解文件系统的工作原理，只需要了解 VFS 提供的统一接口即可。</p>
<p>在 Linux 文件系统中，用户空间、系统调用、虚拟文件系统、缓存、文件系统以及存储之间的关系如下图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E8%99%9A%E6%8B%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F.png" alt="img"></p>
<p>Linux 支持的文件系统也不少，根据存储位置的不同，可以把文件系统分为三类：</p>
<ul>
<li><em>磁盘的文件系统</em>，它是直接把数据存储在磁盘中，比如 Ext 2&#x2F;3&#x2F;4、XFS 等都是这类文件系统。</li>
<li><em>内存的文件系统</em>，这类文件系统的数据不是存储在硬盘的，而是占用内存空间，我们经常用到的 <code>/proc</code> 和 <code>/sys</code> 文件系统都属于这一类，读写这类文件，实际上是读写内核中相关的数据。</li>
<li><em>网络的文件系统</em>，用来访问其他计算机主机数据的文件系统，比如 NFS、SMB 等等。</li>
</ul>
<p>文件系统首先要先挂载到某个目录才可以正常使用，比如 Linux 系统在启动时，会把文件系统挂载到根目录。</p>
<h3 id="文件的使用"><a href="#文件的使用" class="headerlink" title="文件的使用"></a>文件的使用</h3><p>我们从用户角度来看文件的话，就是我们要怎么使用文件？首先，我们得通过系统调用来打开一个文件。</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%86%99%E5%88%B0%E7%A3%81%E7%9B%98%E8%BF%87%E7%A8%8B.png" alt="write 的过程"></p>
<pre><code class="c">fd = open(name, flag); # 打开文件
...
write(fd,...);         # 写数据
...
close(fd);             # 关闭文件
</code></pre>
<p>上面简单的代码是读取一个文件的过程：</p>
<ul>
<li>首先用 <code>open</code> 系统调用打开文件，<code>open</code> 的参数中包含文件的路径名和文件名。</li>
<li>使用 <code>write</code> 写数据，其中 <code>write</code> 使用 <code>open</code> 所返回的<strong>文件描述符</strong>，并不使用文件名作为参数。</li>
<li>使用完文件后，要用 <code>close</code> 系统调用关闭文件，避免资源的泄露。</li>
</ul>
<p>我们打开了一个文件后，操作系统会跟踪进程打开的所有文件，所谓的跟踪呢，就是操作系统为每个进程维护一个打开文件表，文件表里的每一项代表「<strong>文件描述符</strong>」，所以说文件描述符是打开文件的标识。</p>
<p>操作系统在打开文件表中维护着打开文件的状态和信息：</p>
<ul>
<li>文件指针：系统跟踪上次读写位置作为当前文件位置指针，这种指针对打开文件的某个进程来说是唯一的；</li>
<li>文件打开计数器：文件关闭时，操作系统必须重用其打开文件表条目，否则表内空间不够用。因为多个进程可能打开同一个文件，所以系统在删除打开文件条目之前，必须等待最后一个进程关闭文件，该计数器跟踪打开和关闭的数量，当该计数为 0 时，系统关闭文件，删除该条目；</li>
<li>文件磁盘位置：绝大多数文件操作都要求系统修改文件数据，该信息保存在内存中，以免每个操作都从磁盘中读取；</li>
<li>访问权限：每个进程打开文件都需要有一个访问模式（创建、只读、读写、添加等），该信息保存在进程的打开文件表中，以便操作系统能允许或拒绝之后的 I&#x2F;O 请求；</li>
</ul>
<p>用户和操作系统对文件的读写操作是有差异的，用户习惯以字节的方式读写文件，而操作系统则是以数据块来读写文件，那屏蔽掉这种差异的工作就是文件系统了。</p>
<p>我们来分别看一下，读文件和写文件的过程：</p>
<ul>
<li>当用户进程从文件读取 1 个字节大小的数据时，文件系统则需要获取字节所在的数据块，再返回数据块对应的用户进程所需的数据部分。</li>
<li>当用户进程把 1 个字节大小的数据写进文件时，文件系统则找到需要写入数据的数据块的位置，然后修改数据块中对应的部分，最后再把数据块写回磁盘。</li>
</ul>
<p>所以说，<strong>文件系统的基本操作单位是数据块</strong>。</p>
<h3 id="文件的存储"><a href="#文件的存储" class="headerlink" title="文件的存储"></a>文件的存储</h3><p>文件的数据是要存储在硬盘上面的，数据在磁盘上的存放方式，就像程序在内存中存放的方式那样，有以下两种：</p>
<ul>
<li>连续空间存放方式</li>
<li>非连续空间存放方式</li>
</ul>
<p>其中，非连续空间存放方式又可以分为「链表方式」和「索引方式」。</p>
<h4 id="连续空间存放方式"><a href="#连续空间存放方式" class="headerlink" title="连续空间存放方式"></a>连续空间存放方式</h4><p>连续空间存放方式顾名思义，<strong>文件存放在磁盘「连续的」物理空间中</strong>。这种模式下，文件的数据都是紧密相连，<strong>读写效率很高</strong>，因为一次磁盘寻道就可以读出整个文件。</p>
<p>使用连续存放的方式有一个前提，必须先知道一个文件的大小，这样文件系统才会根据文件的大小在磁盘上找到一块连续的空间分配给文件。</p>
<p>所以，<strong>文件头里需要指定「起始块的位置」和「长度」</strong>，有了这两个信息就可以很好的表示文件存放方式是一块连续的磁盘空间。</p>
<p>注意，此处说的文件头，就类似于 Linux 的 inode。</p>
<p>连续空间存放的方式虽然读写效率高，<strong>但是有「外部碎片」和「文件长度不易扩展」的缺陷。</strong>使用非连续空间存放方式可以解决这些缺陷。</p>
<h4 id="非连续空间存放方式"><a href="#非连续空间存放方式" class="headerlink" title="非连续空间存放方式"></a>非连续空间存放方式</h4><p>非连续空间存放方式分为「链表方式」和「索引方式」。</p>
<blockquote>
<p>我们先来看看链表的方式。</p>
</blockquote>
<p>链表的方式存放是<strong>离散的，不用连续的</strong>，于是就可以<strong>消除磁盘碎片</strong>，可大大提高磁盘空间的利用率，同时<strong>文件的长度可以动态扩展</strong>。根据实现的方式的不同，链表可分为「<strong>隐式链表</strong>」和「<strong>显式链接</strong>」两种形式。</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%9D%9E%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E5%AD%98%E6%94%BE%E6%96%B9%E5%BC%8F-%E9%93%BE%E8%A1%A8%E6%96%B9%E5%BC%8F.png" alt="隐式链表"></p>
<p>文件要以「<strong>隐式链表</strong>」的方式存放的话，<strong>实现的方式是文件头要包含「第一块」和「最后一块」的位置，并且每个数据块里面留出一个指针空间，用来存放下一个数据块的位置</strong>，这样一个数据块连着一个数据块，从链头开始就可以顺着指针找到所有的数据块，所以存放的方式可以是不连续的。</p>
<p>隐式链表的存放方式的<strong>缺点在于无法直接访问数据块，只能通过指针顺序访问文件，以及数据块指针消耗了一定的存储空间</strong>。隐式链接分配的<strong>稳定性较差</strong>，系统在运行过程中由于软件或者硬件错误<strong>导致链表中的指针丢失或损坏，会导致文件数据的丢失。</strong></p>
<p>如果取出每个磁盘块的指针，把它放在内存的一个表中，就可以解决上述隐式链表的两个不足。那么，这种实现方式是「<strong>显式链接</strong>」，它指<strong>把用于链接文件各数据块的指针，显式地存放在内存的一张链接表中</strong>，该表在整个磁盘仅设置一张，<strong>每个表项中存放链接指针，指向下一个数据块号</strong>。</p>
<p>对于显式链接的工作方式，我们举个例子，文件 A 依次使用了磁盘块 4、7、2、10 和 12 ，文件 B 依次使用了磁盘块 6、3、11 和 14 。利用下图中的表，可以从第 4 块开始，顺着链走到最后，找到文件 A 的全部磁盘块。同样，从第 6 块开始，顺着链走到最后，也能够找出文件 B 的全部磁盘块。最后，这两个链都以一个不属于有效磁盘编号的特殊标记（如 -1 ）结束。内存中的这样一个表格称为<strong>文件分配表（File Allocation Table，FAT）</strong>。</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E5%88%86%E9%85%8D%E8%A1%A8.png" alt="显式链接"></p>
<p>由于查找记录的过程是在内存中进行的，因而不仅显著地<strong>提高了检索速度</strong>，而且<strong>大大减少了访问磁盘的次数</strong>。但也正是整个表都存放在内存中的关系，它的主要的缺点是<strong>不适用于大磁盘</strong>。</p>
<p>比如，对于 200GB 的磁盘和 1KB 大小的块，这张表需要有 2 亿项，每一项对应于这 2 亿个磁盘块中的一个块，每项如果需要 4 个字节，那这张表要占用 800MB 内存，很显然 FAT 方案对于大磁盘而言不太合适。</p>
<blockquote>
<p>接下来，我们来看看索引的方式。</p>
</blockquote>
<p>链表的方式解决了连续分配的磁盘碎片和文件动态扩展的问题，但是不能有效支持直接访问（FAT除外），索引的方式可以解决这个问题。</p>
<p>索引的实现是为每个文件创建一个「<strong>索引数据块</strong>」，里面存放的是<strong>指向文件数据块的指针列表</strong>，说白了就像书的目录一样，要找哪个章节的内容，看目录查就可以。</p>
<p>另外，<strong>文件头需要包含指向「索引数据块」的指针</strong>，这样就可以通过文件头知道索引数据块的位置，再通过索引数据块里的索引信息找到对应的数据块。</p>
<p>创建文件时，索引块的所有指针都设为空。当首次写入第 i 块时，先从空闲空间中取得一个块，再将其地址写到索引块的第 i 个条目。</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%9D%9E%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E5%AD%98%E6%94%BE%E6%96%B9%E5%BC%8F-%E7%B4%A2%E5%BC%95%E6%96%B9%E5%BC%8F.png" alt="索引的方式"></p>
<p>索引的方式优点在于：</p>
<ul>
<li>文件的创建、增大、缩小很方便；</li>
<li>不会有碎片的问题；</li>
<li>支持顺序读写和随机读写；</li>
</ul>
<p>由于索引数据也是存放在磁盘块的，如果文件很小，明明只需一块就可以存放的下，但还是需要额外分配一块来存放索引数据，所以缺陷之一就是存储索引带来的开销。</p>
<p>如果文件很大，大到一个索引数据块放不下索引信息，这时又要如何处理大文件的存放呢？我们可以通过组合的方式，来处理大文件的存。</p>
<p>先来看看链表 + 索引的组合，这种组合称为「<strong>链式索引块</strong>」，它的实现方式是<strong>在索引数据块留出一个存放下一个索引数据块的指针</strong>，于是当一个索引数据块的索引信息用完了，就可以通过指针的方式，找到下一个索引数据块的信息。那这种方式也会出现前面提到的链表方式的问题，万一某个指针损坏了，后面的数据也就会无法读取了。</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%93%BE%E5%BC%8F%E7%B4%A2%E5%BC%95%E5%9D%97.png" alt="链式索引块"></p>
<p>还有另外一种组合方式是索引 + 索引的方式，这种组合称为「<strong>多级索引块</strong>」，实现方式是<strong>通过一个索引块来存放多个索引数据块</strong>，一层套一层索引，像极了俄罗斯套娃是吧。</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E7%BA%A7%E7%B4%A2%E5%BC%95%E5%9D%97.png" alt="多级索引块"></p>
<h4 id="Unix-文件的实现方式"><a href="#Unix-文件的实现方式" class="headerlink" title="Unix 文件的实现方式"></a>Unix 文件的实现方式</h4><p>我们先把前面提到的文件实现方式，做个比较：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%96%B9%E5%BC%8F%E6%AF%94%E8%BE%83.png" alt="img"></p>
<p>那早期 Unix 文件系统是组合了前面的文件存放方式的优点，如下图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/Unix%20%E5%A4%9A%E7%BA%A7%E7%B4%A2%E5%BC%95.png" alt="早期 Unix 文件系统"></p>
<p>它是根据文件的大小，存放的方式会有所变化：</p>
<ul>
<li>如果存放文件所需的数据块小于 10 块，则采用直接查找的方式；</li>
<li>如果存放文件所需的数据块超过 10 块，则采用一级间接索引方式；</li>
<li>如果前面两种方式都不够存放大文件，则采用二级间接索引方式；</li>
<li>如果二级间接索引也不够存放大文件，这采用三级间接索引方式；</li>
</ul>
<p>那么，文件头（<em>Inode</em>）就需要包含 13 个指针：</p>
<ul>
<li>10 个指向数据块的指针；</li>
<li>第 11 个指向索引块的指针；</li>
<li>第 12 个指向二级索引块的指针；</li>
<li>第 13 个指向三级索引块的指针；</li>
</ul>
<p>所以，这种方式能很灵活地支持小文件和大文件的存放：</p>
<ul>
<li>对于小文件使用直接查找的方式可减少索引数据块的开销；</li>
<li>对于大文件则以多级索引的方式来支持，所以大文件在访问数据块时需要大量查询；</li>
</ul>
<p>这个方案就用在了 Linux Ext 2&#x2F;3 文件系统里，虽然解决大文件的存储，但是对于大文件的访问，需要大量的查询，效率比较低。</p>
<p>为了解决这个问题，Ext 4 做了一定的改变，具体怎么解决的，本文就不展开了。</p>
<h3 id="空闲空间管理"><a href="#空闲空间管理" class="headerlink" title="空闲空间管理"></a>空闲空间管理</h3><p>前面说到的文件的存储是针对已经被占用的数据块组织和管理，接下来的问题是，如果我要保存一个数据块，我应该放在硬盘上的哪个位置呢？难道需要将所有的块扫描一遍，找个空的地方随便放吗？</p>
<p>那这种方式效率就太低了，所以针对磁盘的空闲空间也是要引入管理的机制，接下来介绍几种常见的方法：</p>
<ul>
<li>空闲表法</li>
<li>空闲链表法</li>
<li>位图法</li>
</ul>
<h4 id="空闲表法"><a href="#空闲表法" class="headerlink" title="空闲表法"></a>空闲表法</h4><p>空闲表法就是为所有空闲空间建立一张表，表内容包括空闲区的第一个块号和该空闲区的块个数，注意，这个方式是连续分配的。如下图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%A9%BA%E9%97%B2%E8%A1%A8%E6%B3%95.png" alt="空闲表法"></p>
<p>当请求分配磁盘空间时，系统依次扫描空闲表里的内容，直到找到一个合适的空闲区域为止。当用户撤销一个文件时，系统回收文件空间。这时，也需顺序扫描空闲表，寻找一个空闲表条目并将释放空间的第一个物理块号及它占用的块数填到这个条目中。</p>
<p>这种方法仅当有少量的空闲区时才有较好的效果。因为，如果存储空间中有着大量的小的空闲区，则空闲表变得很大，这样查询效率会很低。另外，这种分配技术适用于建立连续文件。</p>
<h4 id="空闲链表法"><a href="#空闲链表法" class="headerlink" title="空闲链表法"></a>空闲链表法</h4><p>我们也可以使用「链表」的方式来管理空闲空间，每一个空闲块里有一个指针指向下一个空闲块，这样也能很方便的找到空闲块并管理起来。如下图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%A9%BA%E9%97%B2%E5%9D%97%E9%93%BE%E8%A1%A8.png" alt="空闲链表法"></p>
<p>当创建文件需要一块或几块时，就从链头上依次取下一块或几块。反之，当回收空间时，把这些空闲块依次接到链头上。</p>
<p>这种技术只要在主存中保存一个指针，令它指向第一个空闲块。其特点是简单，但不能随机访问，工作效率低，因为每当在链上增加或移动空闲块时需要做很多 I&#x2F;O 操作，同时数据块的指针消耗了一定的存储空间。</p>
<p>空闲表法和空闲链表法都不适合用于大型文件系统，因为这会使空闲表或空闲链表太大。</p>
<h4 id="位图法"><a href="#位图法" class="headerlink" title="位图法"></a>位图法</h4><p>位图是利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。</p>
<p>当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配。它形式如下：</p>
<pre><code class="text">1111110011111110001110110111111100111 ...
</code></pre>
<p>在 Linux 文件系统就采用了位图的方式来管理空闲空间，不仅用于数据空闲块的管理，还用于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，自然也要有对其管理。</p>
<h3 id="文件系统的结构"><a href="#文件系统的结构" class="headerlink" title="文件系统的结构"></a>文件系统的结构</h3><p>前面提到 Linux 是用位图的方式管理空闲空间，用户在创建一个新文件时，Linux 内核会通过 inode 的位图找到空闲可用的 inode，并进行分配。要存储数据时，会通过块的位图找到空闲的块，并分配，但仔细计算一下还是有问题的。</p>
<p>数据块的位图是放在磁盘块里的，假设是放在一个块里，一个块 4K，每位表示一个数据块，共可以表示 <code>4 * 1024 * 8 = 2^15</code> 个空闲块，由于 1 个数据块是 4K 大小，那么最大可以表示的空间为 <code>2^15 * 4 * 1024 = 2^27</code> 个 byte，也就是 128M。</p>
<p>也就是说按照上面的结构，如果采用「一个块的位图 + 一系列的块」，外加「一个块的 inode 的位图 + 一系列的 inode 的结构」能表示的最大空间也就 128M，这太少了，现在很多文件都比这个大。</p>
<p>在 Linux 文件系统，把这个结构称为一个<strong>块组</strong>，那么有 N 多的块组，就能够表示 N 大的文件。</p>
<p>下图给出了 Linux Ext2 整个文件系统的结构和块组的内容，文件系统都由大量块组组成，在硬盘上相继排布：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E7%BB%84.png" alt="img"></p>
<p>最前面的第一个块是引导块，在系统启动时用于启用引导，接着后面就是一个一个连续的块组了，块组的内容如下：</p>
<ul>
<li><em>超级块</em>，包含的是文件系统的重要信息，比如 inode 总个数、块总个数、每个块组的 inode 个数、每个块组的块个数等等。</li>
<li><em>块组描述符</em>，包含文件系统中各个块组的状态，比如块组中空闲块和 inode 的数目等，每个块组都包含了文件系统中「所有块组的组描述符信息」。</li>
<li><em>数据位图和 inode 位图</em>， 用于表示对应的数据块或 inode 是空闲的，还是被使用中。</li>
<li><em>inode 列表</em>，包含了块组中所有的 inode，inode 用于保存文件系统中与各个文件和目录相关的所有元数据。</li>
<li><em>数据块</em>，包含文件的有用数据。</li>
</ul>
<p>你可以会发现每个块组里有很多重复的信息，比如<strong>超级块和块组描述符表，这两个都是全局信息，而且非常的重要</strong>，这么做是有两个原因：</p>
<ul>
<li>如果系统崩溃破坏了超级块或块组描述符，有关文件系统结构和内容的所有信息都会丢失。如果有冗余的副本，该信息是可能恢复的。</li>
<li>通过使文件和管理数据尽可能接近，减少了磁头寻道和旋转，这可以提高文件系统的性能。</li>
</ul>
<p>不过，Ext2 的后续版本采用了稀疏技术。该做法是，超级块和块组描述符表不再存储到文件系统的每个块组中，而是只写入到块组 0、块组 1 和其他 ID 可以表示为 3、 5、7 的幂的块组中。</p>
<h3 id="目录的存储"><a href="#目录的存储" class="headerlink" title="目录的存储"></a>目录的存储</h3><p>在前面，我们知道了一个普通文件是如何存储的，但还有一个特殊的文件，经常用到的目录，它是如何保存的呢？</p>
<p>基于 Linux 一切皆文件的设计思想，目录其实也是个文件，你甚至可以通过 <code>vim</code> 打开它，它也有 inode，inode 里面也是指向一些块。</p>
<p>和普通文件不同的是，<strong>普通文件的块里面保存的是文件数据，而目录文件的块里面保存的是目录里面一项一项的文件信息。</strong></p>
<p>在目录文件的块中，最简单的保存格式就是<strong>列表</strong>，就是一项一项地将目录下的文件信息（如文件名、文件 inode、文件类型等）列在表里。</p>
<p>列表中每一项就代表该目录下的文件的文件名和对应的 inode，通过这个 inode，就可以找到真正的文件。</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%9B%AE%E5%BD%95%E5%93%88%E5%B8%8C%E8%A1%A8.png" alt="目录格式哈希表"></p>
<p>通常，第一项是「<code>.</code>」，表示当前目录，第二项是「<code>..</code>」，表示上一级目录，接下来就是一项一项的文件名和 inode。</p>
<p>如果一个目录有超级多的文件，我们要想在这个目录下找文件，按照列表一项一项的找，效率就不高了。</p>
<p>于是，保存目录的格式改成<strong>哈希表</strong>，对文件名进行哈希计算，把哈希值保存起来，如果我们要查找一个目录下面的文件名，可以通过名称取哈希。如果哈希能够匹配上，就说明这个文件的信息在相应的块里面。</p>
<p>Linux 系统的 ext 文件系统就是采用了哈希表，来保存目录的内容，这种方法的优点是查找非常迅速，插入和删除也较简单，不过需要一些预备措施来避免哈希冲突。</p>
<p>目录查询是通过在磁盘上反复搜索完成，需要不断地进行 I&#x2F;O 操作，开销较大。所以，为了减少 I&#x2F;O 操作，把当前使用的文件目录缓存在内存，以后要使用该文件时只要在内存中操作，从而降低了磁盘操作次数，提高了文件系统的访问速度。</p>
<h3 id="软链接和硬链接"><a href="#软链接和硬链接" class="headerlink" title="软链接和硬链接"></a>软链接和硬链接</h3><p>有时候我们希望给某个文件取个别名，那么在 Linux 中可以通过<strong>硬链接（Hard Link）</strong> 和<strong>软链接（Symbolic Link）</strong> 的方式来实现，它们都是比较特殊的文件，但是实现方式也是不相同的。</p>
<p>硬链接是<strong>多个目录项中的「索引节点」指向一个文件</strong>，也就是指向同一个 inode，但是 inode 是不可能跨越文件系统的，每个文件系统都有各自的 inode 数据结构和列表，所以<strong>硬链接是不可用于跨文件系统的</strong>。由于多个目录项都是指向一个 inode，那么<strong>只有删除文件的所有硬链接以及源文件时，系统才会彻底删除该文件。</strong></p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%A1%AC%E9%93%BE%E6%8E%A5-2.png" alt="硬链接"></p>
<p>软链接相当于重新创建一个文件，这个文件有<strong>独立的 inode</strong>，但是这个<strong>文件的内容是另外一个文件的路径</strong>，所以访问软链接的时候，实际上相当于访问到了另外一个文件，所以<strong>软链接是可以跨文件系统的</strong>，甚至<strong>目标文件被删除了，链接文件还是在的，只不过指向的文件找不到了而已。</strong></p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E8%BD%AF%E9%93%BE%E6%8E%A5.png" alt="软链接"></p>
<h3 id="文件-I-x2F-O"><a href="#文件-I-x2F-O" class="headerlink" title="文件 I&#x2F;O"></a>文件 I&#x2F;O</h3><p>文件的读写方式各有千秋，对于文件的 I&#x2F;O 分类也非常多，常见的有</p>
<ul>
<li>缓冲与非缓冲 I&#x2F;O</li>
<li>直接与非直接 I&#x2F;O</li>
<li>阻塞与非阻塞 I&#x2F;O VS 同步与异步 I&#x2F;O</li>
</ul>
<p>接下来，分别对这些分类讨论。</p>
<h4 id="缓冲与非缓冲-I-x2F-O"><a href="#缓冲与非缓冲-I-x2F-O" class="headerlink" title="缓冲与非缓冲 I&#x2F;O"></a>缓冲与非缓冲 I&#x2F;O</h4><p>文件操作的标准库是可以实现数据的缓存，那么<strong>根据「是否利用标准库缓冲」，可以把文件 I&#x2F;O 分为缓冲 I&#x2F;O 和非缓冲 I&#x2F;O</strong>：</p>
<ul>
<li>缓冲 I&#x2F;O，利用的是标准库的缓存实现文件的加速访问，而标准库再通过系统调用访问文件。</li>
<li>非缓冲 I&#x2F;O，直接通过系统调用访问文件，不经过标准库缓存。</li>
</ul>
<p>这里所说的「缓冲」特指标准库内部实现的缓冲。</p>
<p>比方说，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来，这样做的目的是，减少系统调用的次数，毕竟系统调用是有 CPU 上下文切换的开销的。</p>
<h4 id="直接与非直接-I-x2F-O"><a href="#直接与非直接-I-x2F-O" class="headerlink" title="直接与非直接 I&#x2F;O"></a>直接与非直接 I&#x2F;O</h4><p>我们都知道磁盘 I&#x2F;O 是非常慢的，所以 Linux 内核为了减少磁盘 I&#x2F;O 次数，在系统调用后，会把用户数据拷贝到内核中缓存起来，这个内核缓存空间也就是「页缓存」，只有当缓存满足某些条件的时候，才发起磁盘 I&#x2F;O 的请求。</p>
<p>那么，<strong>根据是「否利用操作系统的缓存」，可以把文件 I&#x2F;O 分为直接 I&#x2F;O 与非直接 I&#x2F;O</strong>：</p>
<ul>
<li>直接 I&#x2F;O，不会发生内核缓存和用户程序之间数据复制，而是直接经过文件系统访问磁盘。</li>
<li>非直接 I&#x2F;O，读操作时，数据从内核缓存中拷贝给用户程序，写操作时，数据从用户程序拷贝给内核缓存，再由内核决定什么时候写入数据到磁盘。</li>
</ul>
<p>如果你在使用文件操作类的系统调用函数时，指定了 <code>O_DIRECT</code> 标志，则表示使用直接 I&#x2F;O。如果没有设置过，默认使用的是非直接 I&#x2F;O。</p>
<blockquote>
<p>如果用了非直接 I&#x2F;O 进行写数据操作，内核什么情况下才会把缓存数据写入到磁盘？</p>
</blockquote>
<p>以下几种场景会触发内核缓存的数据写入磁盘：</p>
<ul>
<li>在调用 <code>write</code> 的最后，当发现内核缓存的数据太多的时候，内核会把数据写到磁盘上；</li>
<li>用户主动调用 <code>sync</code>，内核缓存会刷到磁盘上；</li>
<li>当内存十分紧张，无法再分配页面时，也会把内核缓存的数据刷到磁盘上；</li>
<li>内核缓存的数据的缓存时间超过某个时间时，也会把数据刷到磁盘上；</li>
</ul>
<h4 id="阻塞与非阻塞-I-x2F-O-VS-同步与异步-I-x2F-O"><a href="#阻塞与非阻塞-I-x2F-O-VS-同步与异步-I-x2F-O" class="headerlink" title="阻塞与非阻塞 I&#x2F;O VS 同步与异步 I&#x2F;O"></a>阻塞与非阻塞 I&#x2F;O VS 同步与异步 I&#x2F;O</h4><p>为什么把阻塞 &#x2F; 非阻塞与同步与异步放一起说的呢？因为它们确实非常相似，也非常容易混淆，不过它们之间的关系还是有点微妙的。</p>
<p>先来看看<strong>阻塞 I&#x2F;O</strong>，当用户程序执行 <code>read</code> ，线程会被阻塞，一直等到内核数据准备好，并把数据从内核缓冲区拷贝到应用程序的缓冲区中，当拷贝过程完成，<code>read</code> 才会返回。</p>
<p>注意，<strong>阻塞等待的是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程</strong>。过程如下图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%98%BB%E5%A1%9E%20I_O.png" alt="阻塞 I/O"></p>
<p>知道了阻塞 I&#x2F;O ，来看看<strong>非阻塞 I&#x2F;O</strong>，非阻塞的 read 请求在数据未准备好的情况下立即返回，可以继续往下执行，此时应用程序不断轮询内核，直到数据准备好，内核将数据拷贝到应用程序缓冲区，<code>read</code> 调用才可以获取到结果。过程如下图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%9D%9E%E9%98%BB%E5%A1%9E%20I_O%20.png" alt="非阻塞 I/O"></p>
<p>注意，<strong>这里最后一次 read 调用，获取数据的过程，是一个同步的过程，是需要等待的过程。这里的同步指的是内核态的数据拷贝到用户程序的缓存区这个过程。</strong></p>
<p>举个例子，访问管道或 socket 时，如果设置了 <code>O_NONBLOCK</code> 标志，那么就表示使用的是非阻塞 I&#x2F;O 的方式访问，而不做任何设置的话，默认是阻塞 I&#x2F;O。</p>
<p>应用程序每次轮询内核的 I&#x2F;O 是否准备好，感觉有点傻乎乎，因为轮询的过程中，应用程序啥也做不了，只是在循环。</p>
<p>为了解决这种傻乎乎轮询方式，于是 <strong>I&#x2F;O 多路复用</strong>技术就出来了，如 select、poll，它是通过 I&#x2F;O 事件分发，当内核数据准备好时，再以事件通知应用程序进行操作。</p>
<p>这个做法大大改善了 CPU 的利用率，因为当调用了 I&#x2F;O 多路复用接口，如果没有事件发生，那么当前线程就会发生阻塞，这时 CPU 会切换其他线程执行任务，等内核发现有事件到来的时候，会唤醒阻塞在 I&#x2F;O 多路复用接口的线程，然后用户可以进行后续的事件处理。</p>
<p>整个流程要比阻塞 IO 要复杂，似乎也更浪费性能。但 <strong>I&#x2F;O 多路复用接口最大的优势在于，用户可以在一个线程内同时处理多个 socket 的 IO 请求</strong>。用户可以注册多个 socket，然后不断地调用 I&#x2F;O 多路复用接口读取被激活的 socket，即可达到在同一个线程内同时处理多个 IO 请求的目的。而在同步阻塞模型中，必须通过多线程的方式才能达到这个目的。</p>
<p>下图是使用 select I&#x2F;O 多路复用过程。注意，<code>read</code> 获取数据的过程（数据从内核态拷贝到用户态的过程），也是一个<strong>同步的过程</strong>，需要等待：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9F%BA%E4%BA%8E%E9%9D%9E%E9%98%BB%E5%A1%9E%20I_O%20%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8.png" alt="I/O 多路复用"></p>
<p>实际上，无论是阻塞 I&#x2F;O、非阻塞 I&#x2F;O，还是基于非阻塞 I&#x2F;O 的多路复用<strong>都是同步调用。因为它们在 read 调用时，内核将数据从内核空间拷贝到应用程序空间，过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷贝效率不高，read 调用就会在这个同步过程中等待比较长的时间。</strong></p>
<p>而真正的<strong>异步 I&#x2F;O</strong> 是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程都不用等待。</p>
<p>当我们发起 <code>aio_read</code> 之后，就立即返回，内核自动将数据从内核空间拷贝到应用程序空间，这个拷贝过程同样是异步的，内核自动完成的，和前面的同步操作不一样，应用程序并不需要主动发起拷贝动作。过程如下图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%BC%82%E6%AD%A5%20I_O.png" alt="异步 I/O"></p>
<p>下面这张图，总结了以上几种 I&#x2F;O 模型：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%90%8C%E6%AD%A5VS%E5%BC%82%E6%AD%A5IO.png" alt="img"></p>
<p>在前面我们知道了，I&#x2F;O 是分为两个过程的：</p>
<ol>
<li>数据准备的过程</li>
<li>数据从内核空间拷贝到用户进程缓冲区的过程</li>
</ol>
<p>阻塞 I&#x2F;O 会阻塞在「过程 1 」和「过程 2」，而非阻塞 I&#x2F;O 和基于非阻塞 I&#x2F;O 的多路复用只会阻塞在「过程 2」，所以这三个都可以认为是同步 I&#x2F;O。</p>
<p>异步 I&#x2F;O 则不同，「过程 1 」和「过程 2 」都不会阻塞。</p>
<h4 id="五大IO模型"><a href="#五大IO模型" class="headerlink" title="五大IO模型"></a>五大IO模型</h4><p>1.阻塞IO</p>
<p>2.非阻塞IO</p>
<p>3.IO多路复用</p>
<p>4.信号驱动IO:信号驱动IO不是像select用循环请求询问的方式去监控数据就绪状态，而是在调用sigaction时候建立一个SIGIO的信号联系，当内核数据准备好之后再通过SIGIO信号通知线程数据准备好后的可读状态，当线程收到可读状态的信号后，此时再向内核发起recvfrom读取数据的请求，因为信号驱动IO的模型下应用线程在发出信号监控后即可返回，不会阻塞，所以这样的方式下，一个应用线程也可以同时监控多个fd。</p>
<p>5.异步IO</p>
<h2 id="设备管理"><a href="#设备管理" class="headerlink" title="设备管理"></a>设备管理</h2><h3 id="I-x2F-O控制器"><a href="#I-x2F-O控制器" class="headerlink" title="I&#x2F;O控制器"></a>I&#x2F;O控制器</h3><p>我们的电脑设备可以接非常多的输入输出设备，比如键盘、鼠标、显示器、网卡、硬盘、打印机、音响等等，每个设备的用法和功能都不同，那操作系统是如何把这些输入输出设备统一管理的呢?</p>
<p>为了屏蔽设备之间的差异，每个设备都有一个叫<strong>设备控制器（Device Control）</strong> 的组件，比如硬盘有硬盘控制器、显示器有视频控制器等。</p>
<p>因为这些控制器都很清楚的知道对应设备的用法和功能，所以 CPU 是通过设备控制器来和设备打交道的。</p>
<p>设备控制器里有芯片，它可执行自己的逻辑，也有自己的寄存器，用来与 CPU 进行通信，比如：</p>
<ul>
<li>通过写入这些寄存器，操作系统可以命令设备发送数据、接收数据、开启或关闭，或者执行某些其他操作。</li>
<li>通过读取这些寄存器，操作系统可以了解设备的状态，是否准备好接收一个新的命令等。</li>
</ul>
<p>实际上，控制器是有三类寄存器，它们分别是<strong>状态寄存器（Status Register）</strong>、 <strong>命令寄存器（Command Register）<em>以及</em>数据寄存器（Data Register）</strong>，如下图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/%E8%AE%BE%E5%A4%87%E6%8E%A7%E5%88%B6%E5%99%A8.png" alt="img"></p>
<p>这三个寄存器的作用：</p>
<ul>
<li><em>数据寄存器</em>，CPU 向 I&#x2F;O 设备写入需要传输的数据，比如要打印的内容是「Hello」，CPU 就要先发送一个 H 字符给到对应的 I&#x2F;O 设备。</li>
<li><em>命令寄存器</em>，CPU 发送一个命令，告诉 I&#x2F;O 设备，要进行输入&#x2F;输出操作，于是就会交给 I&#x2F;O 设备去工作，任务完成后，会把状态寄存器里面的状态标记为完成。</li>
<li><em>状态寄存器</em>，目的是告诉 CPU ，现在已经在工作或工作已经完成，如果已经在工作状态，CPU 再发送数据或者命令过来，都是没有用的，直到前面的工作已经完成，状态寄存标记成已完成，CPU 才能发送下一个字符和命令。</li>
</ul>
<p>CPU 通过读写设备控制器中的寄存器控制设备，这可比 CPU 直接控制输入输出设备，要方便和标准很多。</p>
<p>另外， 输入输出设备可分为两大类 ：<strong>块设备（Block Device）<em>和</em>字符设备（Character Device）</strong>。</p>
<ul>
<li><em>块设备</em>，把数据存储在固定大小的块中，每个块有自己的地址，硬盘、USB 是常见的块设备。</li>
<li><em>字符设备</em>，以字符为单位发送或接收一个字符流，字符设备是不可寻址的，也没有任何寻道操作，鼠标是常见的字符设备。</li>
</ul>
<p>块设备通常传输的数据量会非常大，于是控制器设立了一个可读写的<strong>数据缓冲区</strong>。</p>
<ul>
<li>CPU 写入数据到控制器的缓冲区时，当缓冲区的数据囤够了一部分，才会发给设备。</li>
<li>CPU 从控制器的缓冲区读取数据时，也需要缓冲区囤够了一部分，才拷贝到内存。</li>
</ul>
<p>这样做是为了，减少对设备的频繁操作。</p>
<p>那 CPU 是如何与设备的控制寄存器和数据缓冲区进行通信的？存在两个方法：</p>
<ul>
<li><em>端口 I&#x2F;O</em>，每个控制寄存器被分配一个 I&#x2F;O 端口，可以通过特殊的汇编指令操作这些寄存器，比如 <code>in/out</code> 类似的指令。</li>
<li><em>内存映射 I&#x2F;O</em>，将所有控制寄存器映射到内存空间中，这样就可以像读写内存一样读写数据缓冲区。</li>
</ul>
<h3 id="I-x2F-O-控制方式"><a href="#I-x2F-O-控制方式" class="headerlink" title="I&#x2F;O 控制方式"></a>I&#x2F;O 控制方式</h3><p>在前面我知道，每种设备都有一个设备控制器，控制器相当于一个小 CPU，它可以自己处理一些事情，但有个问题是，当 CPU 给设备发送了一个指令，让设备控制器去读设备的数据，它读完的时候，要怎么通知 CPU 呢？</p>
<p>控制器的寄存器一般会有状态标记位，用来标识输入或输出操作是否完成。于是，我们想到第一种<strong>轮询等待</strong>的方法，让 CPU 一直查寄存器的状态，直到状态标记为完成，很明显，这种方式非常的傻瓜，它会占用 CPU 的全部时间。</p>
<p>那我们就想到第二种方法 —— <strong>中断</strong>，通知操作系统数据已经准备好了。我们一般会有一个硬件的<strong>中断控制器</strong>，当设备完成任务后触发中断到中断控制器，中断控制器就通知 CPU，一个中断产生了，CPU 需要停下当前手里的事情来处理中断。</p>
<p>另外，中断有两种，一种<strong>软中断</strong>，例如代码调用 <code>INT</code> 指令触发，一种是<strong>硬件中断</strong>，就是硬件通过中断控制器触发的。</p>
<p>但中断的方式对于频繁读写数据的磁盘，并不友好，这样 CPU 容易经常被打断，会占用 CPU 大量的时间。对于这一类设备的问题的解决方法是使用 <strong>DMA（Direct Memory Access）</strong> 功能，它可以使得设备在 CPU 不参与的情况下，能够自行完成把设备 I&#x2F;O 数据放入到内存。那要实现 DMA 功能要有 「DMA 控制器」硬件的支持。</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/DMA%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.png" alt="img"></p>
<p>DMA 的工作方式如下：</p>
<ul>
<li>CPU 需对 DMA 控制器下发指令，告诉它想读取多少数据，读完的数据放在内存的某个地方就可以了；</li>
<li>接下来，DMA 控制器会向磁盘控制器发出指令，通知它从磁盘读数据到其内部的缓冲区中，接着磁盘控制器将缓冲区的数据传输到内存；</li>
<li>当磁盘控制器把数据传输到内存的操作完成后，磁盘控制器在总线上发出一个确认成功的信号到 DMA 控制器；</li>
<li>DMA 控制器收到信号后，DMA 控制器发中断通知 CPU 指令完成，CPU 就可以直接用内存里面现成的数据了；</li>
</ul>
<p>可以看到， CPU 当要读取磁盘数据的时候，只需给 DMA 控制器发送指令，然后返回去做其他事情，当磁盘数据拷贝到内存后，DMA 控制机器通过中断的方式，告诉 CPU 数据已经准备好了，可以从内存读数据了。仅仅在传送开始和结束时需要 CPU 干预。</p>
<h3 id="设备驱动程序"><a href="#设备驱动程序" class="headerlink" title="设备驱动程序"></a>设备驱动程序</h3><p>虽然设备控制器屏蔽了设备的众多细节，但每种设备的控制器的寄存器、缓冲区等使用模式都是不同的，所以为了屏蔽「设备控制器」的差异，引入了<strong>设备驱动程序</strong>。<br><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F.png" alt="img"></p>
<p>设备控制器不属于操作系统范畴，它是属于硬件，而设备驱动程序属于操作系统的一部分，操作系统的内核代码可以像本地调用代码一样使用设备驱动程序的接口，而设备驱动程序是面向设备控制器的代码，它发出操控设备控制器的指令后，才可以操作设备控制器。</p>
<p>不同的设备控制器虽然功能不同，但是<strong>设备驱动程序会提供统一的接口给操作系统</strong>，这样不同的设备驱动程序，就可以以相同的方式接入操作系统。</p>
<p>前面提到了不少关于中断的事情，设备完成了事情，则会发送中断来通知操作系统。那操作系统就需要有一个地方来处理这个中断，这个地方也就是在设备驱动程序里，它会及时响应控制器发来的中断请求，并根据这个中断的类型调用响应的<strong>中断处理程序</strong>进行处理。</p>
<p>通常，设备驱动程序初始化的时候，要先注册一个该设备的中断处理函数。</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/%E4%B8%AD%E6%96%AD%E5%B7%A5%E4%BD%9C%E8%BF%87%E7%A8%8B.png" alt="img"></p>
<p>我们来看看，中断处理程序的处理流程：</p>
<ol>
<li>在 I&#x2F;O 时，设备控制器如果已经准备好数据，则会通过中断控制器向 CPU 发送中断请求；</li>
<li>保护被中断进程的 CPU 上下文；</li>
<li>转入相应的设备中断处理函数；</li>
<li>进行中断处理；</li>
<li>恢复被中断进程的上下文；</li>
</ol>
<h3 id="通用块层"><a href="#通用块层" class="headerlink" title="通用块层"></a>通用块层</h3><p>对于块设备，为了减少不同块设备的差异带来的影响，Linux 通过一个统一的<strong>通用块层</strong>，来管理不同的块设备。</p>
<p>通用块层是处于文件系统和磁盘驱动中间的一个块设备抽象层，它主要有两个功能：</p>
<ul>
<li>第一个功能，向上为文件系统和应用程序，提供访问块设备的标准接口，向下把各种不同的磁盘设备抽象为统一的块设备，并在内核层面，提供一个框架来管理这些设备的驱动程序；</li>
<li>第二功能，通用层还会给文件系统和应用程序发来的 I&#x2F;O 请求排队，接着会对队列重新排序、请求合并等方式，也就是 I&#x2F;O 调度，主要目的是为了提高磁盘读写的效率。</li>
</ul>
<p>Linux 内存支持 5 种 I&#x2F;O 调度算法，分别是：</p>
<ul>
<li>没有调度算法</li>
<li>先入先出调度算法</li>
<li>完全公平调度算法</li>
<li>优先级调度</li>
<li>最终期限调度算法</li>
</ul>
<p>第一种，没有调度算法，是的，你没听错，它不对文件系统和应用程序的 I&#x2F;O 做任何处理，这种算法常用在虚拟机 I&#x2F;O 中，此时磁盘 I&#x2F;O 调度算法交由物理机系统负责。</p>
<p>第二种，先入先出调度算法，这是最简单的 I&#x2F;O 调度算法，先进入 I&#x2F;O 调度队列的 I&#x2F;O 请求先发生。</p>
<p>第三种，完全公平调度算法，大部分系统都把这个算法作为默认的 I&#x2F;O 调度器，它为每个进程维护了一个 I&#x2F;O 调度队列，并按照时间片来均匀分布每个进程的 I&#x2F;O 请求。</p>
<p>第四种，优先级调度算法，顾名思义，优先级高的 I&#x2F;O 请求先发生， 它适用于运行大量进程的系统，像是桌面环境、多媒体应用等。</p>
<p>第五种，最终期限调度算法，分别为读、写请求创建了不同的 I&#x2F;O 队列，这样可以提高机械磁盘的吞吐量，并确保达到最终期限的请求被优先处理，适用于在 I&#x2F;O 压力比较大的场景，比如数据库等。</p>
<h3 id="存储系统-I-x2F-O-软件分层"><a href="#存储系统-I-x2F-O-软件分层" class="headerlink" title="存储系统 I&#x2F;O 软件分层"></a>存储系统 I&#x2F;O 软件分层</h3><p>前面说到了不少东西，设备、设备控制器、驱动程序、通用块层，现在再结合文件系统原理，我们来看看 Linux 存储系统的 I&#x2F;O 软件分层。</p>
<p>可以把 Linux 存储系统的 I&#x2F;O 由上到下可以分为三个层次，分别是文件系统层、通用块层、设备层。他们整个的层次关系如下图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/I_O%E8%BD%AF%E4%BB%B6%E5%88%86%E5%B1%82.png" alt="img"></p>
<p>这三个层次的作用是：</p>
<ul>
<li>文件系统层，包括虚拟文件系统和其他文件系统的具体实现，它向上为应用程序统一提供了标准的文件访问接口，向下会通过通用块层来存储和管理磁盘数据。</li>
<li>通用块层，包括块设备的 I&#x2F;O 队列和 I&#x2F;O 调度器，它会对文件系统的 I&#x2F;O 请求进行排队，再通过 I&#x2F;O 调度器，选择一个 I&#x2F;O 发给下一层的设备层。</li>
<li>设备层，包括硬件设备、设备控制器和驱动程序，负责最终物理设备的 I&#x2F;O 操作。</li>
</ul>
<p>有了文件系统接口之后，不但可以通过文件系统的命令行操作设备，也可以通过应用程序，调用 <code>read</code>、<code>write</code> 函数，就像读写文件一样操作设备，所以说设备在 Linux 下，也只是一个特殊的文件。</p>
<p>但是，除了读写操作，还需要有检查特定于设备的功能和属性。于是，需要 <code>ioctl</code> 接口，它表示输入输出控制接口，是用于配置和修改特定设备属性的通用接口。</p>
<p>另外，存储系统的 I&#x2F;O 是整个系统最慢的一个环节，所以 Linux 提供了不少缓存机制来提高 I&#x2F;O 的效率。</p>
<ul>
<li>为了提高文件访问的效率，会使用<strong>页缓存、索引节点缓存、目录项缓存</strong>等多种缓存机制，目的是为了减少对块设备的直接调用。</li>
<li>为了提高块设备的访问效率， 会使用<strong>缓冲区</strong>，来缓存块设备的数据。</li>
</ul>
<h3 id="键盘敲入字母时，期间发生了什么？"><a href="#键盘敲入字母时，期间发生了什么？" class="headerlink" title="键盘敲入字母时，期间发生了什么？"></a>键盘敲入字母时，期间发生了什么？</h3><p>看完前面的内容，相信你对输入输出设备的管理有了一定的认识，那接下来就从操作系统的角度回答「键盘敲入字母时，操作系统期间发生了什么？」</p>
<p>我们先来看看 CPU 的硬件架构图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/CPU%20%E7%A1%AC%E4%BB%B6%E6%80%BB%E7%BA%BF%E5%9B%BE.png" alt="CPU 的硬件架构图"></p>
<p>CPU 里面的内存接口，直接和系统总线通信，然后系统总线再接入一个 I&#x2F;O 桥接器，这个 I&#x2F;O 桥接器，另一边接入了内存总线，使得 CPU 和内存通信。再另一边，又接入了一个 I&#x2F;O 总线，用来连接 I&#x2F;O 设备，比如键盘、显示器等。</p>
<p>那当用户输入了键盘字符，<strong>键盘控制器</strong>就会产生扫描码数据，并将其缓冲在键盘控制器的寄存器中，紧接着键盘控制器通过总线给 CPU 发送<strong>中断请求</strong>。</p>
<p>CPU 收到中断请求后，操作系统会<strong>保存被中断进程的 CPU 上下文</strong>，然后调用键盘的<strong>中断处理程序</strong>。</p>
<p>键盘的中断处理程序是在<strong>键盘驱动程序</strong>初始化时注册的，那键盘<strong>中断处理函数</strong>的功能就是从键盘控制器的寄存器的缓冲区读取扫描码，再根据扫描码找到用户在键盘输入的字符，如果输入的字符是显示字符，那就会把扫描码翻译成对应显示字符的 ASCII 码，比如用户在键盘输入的是字母 A，是显示字符，于是就会把扫描码翻译成 A 字符的 ASCII 码。</p>
<p>得到了显示字符的 ASCII 码后，就会把 ASCII 码放到「读缓冲区队列」，接下来就是要把显示字符显示屏幕了，显示设备的驱动程序会定时从「读缓冲区队列」读取数据放到「写缓冲区队列」，最后把「写缓冲区队列」的数据一个一个写入到显示设备的控制器的寄存器中的数据缓冲区，最后将这些数据显示在屏幕里。</p>
<p>显示出结果后，<strong>恢复被中断进程的上下文</strong>。</p>
<h2 id="文件传输优化"><a href="#文件传输优化" class="headerlink" title="文件传输优化"></a>文件传输优化</h2><h3 id="DMA"><a href="#DMA" class="headerlink" title="DMA"></a>DMA</h3><p>在没有 DMA 技术前，I&#x2F;O 的过程是这样的：</p>
<ul>
<li>CPU 发出对应的指令给磁盘控制器，然后返回；</li>
<li>磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到磁盘控制器的内部缓冲区中，然后产生一个<strong>中断</strong>；</li>
<li>CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 CPU 是无法执行其他任务的。</li>
</ul>
<p>为了方便你理解，我画了一副图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/I_O%20%E4%B8%AD%E6%96%AD.png" alt="img"></p>
<p>可以看到，整个数据的传输过程，都要需要 CPU 亲自参与搬运数据的过程，而且这个过程，CPU 是不能做其他事情的。</p>
<p>简单的搬运几个字符数据那没问题，但是如果我们用千兆网卡或者硬盘传输大量数据的时候，都用 CPU 来搬运的话，肯定忙不过来。</p>
<p>计算机科学家们发现了事情的严重性后，于是就发明了 DMA 技术，也就是<strong>直接内存访问（Direct Memory Access）</strong> 技术。</p>
<p>什么是 DMA 技术？简单理解就是，<strong>在进行 I&#x2F;O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务</strong>。</p>
<p>那使用 DMA 控制器进行数据传输的过程究竟是什么样的呢？下面我们来具体看看。</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/DRM%20I_O%20%E8%BF%87%E7%A8%8B.png" alt="img"></p>
<p>具体过程：</p>
<ul>
<li>用户进程调用 read 方法，向操作系统发出 I&#x2F;O 请求，请求读取数据到自己的内存缓冲区中，进程进入阻塞状态；</li>
<li>操作系统收到请求后，进一步将 I&#x2F;O 请求发送 DMA，然后让 CPU 执行其他任务；</li>
<li>DMA 进一步将 I&#x2F;O 请求发送给磁盘；</li>
<li>磁盘收到 DMA 的 I&#x2F;O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知自己缓冲区已满；</li>
<li><strong>DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，此时不占用 CPU，CPU 可以执行其他任务</strong>；</li>
<li>当 DMA 读取了足够多的数据，就会发送中断信号给 CPU；</li>
<li>CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷贝到用户空间，系统调用返回；</li>
</ul>
<p>可以看到， 整个数据传输的过程，CPU 不再参与数据搬运的工作，而是全程由 DMA 完成，但是 CPU 在这个过程中也是必不可少的，因为传输什么数据，从哪里传输到哪里，都需要 CPU 来告诉 DMA 控制器。</p>
<p>早期 DMA 只存在在主板上，如今由于 I&#x2F;O 设备越来越多，数据传输的需求也不尽相同，所以<strong>每个 I&#x2F;O 设备里面都有自己的 DMA 控制器。</strong></p>
<h3 id="传统的文件传输有多糟糕？"><a href="#传统的文件传输有多糟糕？" class="headerlink" title="传统的文件传输有多糟糕？"></a>传统的文件传输有多糟糕？</h3><p>如果服务端要提供文件传输的功能，我们能想到的最简单的方式是：将磁盘上的文件读取出来，然后通过网络协议发送给客户端。</p>
<p>传统 I&#x2F;O 的工作方式是，数据读取和写入是从用户空间到内核空间来回复制，而内核空间的数据是通过操作系统层面的 I&#x2F;O 接口从磁盘读取或写入。</p>
<p>代码通常如下，一般会需要两个系统调用：</p>
<pre><code class="c">read(file, tmp_buf, len);
write(socket, tmp_buf, len);
</code></pre>
<p>代码很简单，虽然就两行代码，但是这里面发生了不少的事情。</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E4%BC%A0%E7%BB%9F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93.png" alt="img"></p>
<p>首先，期间共<strong>发生了 4 次用户态与内核态的上下文切换</strong>，因为发生了两次系统调用，一次是 <code>read()</code> ，一次是 <code>write()</code>，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。</p>
<p>上下文切换到成本并不小，一次切换需要耗时几十纳秒到几微秒，虽然时间看上去很短，但是在高并发的场景下，这类时间容易被累积和放大，从而影响系统的性能。</p>
<p>其次，还<strong>发生了 4 次数据拷贝</strong>，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的，下面说一下这个过程：</p>
<ul>
<li><em>第一次拷贝</em>，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。</li>
<li><em>第二次拷贝</em>，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。</li>
<li><em>第三次拷贝</em>，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。</li>
<li><em>第四次拷贝</em>，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的。</li>
</ul>
<p>我们回过头看这个文件传输的过程，我们只是搬运一份数据，结果却搬运了 4 次，过多的数据拷贝无疑会消耗 CPU 资源，大大降低了系统性能。</p>
<p>这种简单又传统的文件传输方式，存在冗余的上文切换和数据拷贝，在高并发系统里是非常糟糕的，多了很多不必要的开销，会严重影响系统性能。</p>
<p>所以，<strong>要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数</strong>。</p>
<h3 id="如何优化文件传输的性能？"><a href="#如何优化文件传输的性能？" class="headerlink" title="如何优化文件传输的性能？"></a>如何优化文件传输的性能？</h3><blockquote>
<p>先来看看，如何减少「用户态与内核态的上下文切换」的次数呢？</p>
</blockquote>
<p>读取磁盘数据的时候，之所以要发生上下文切换，这是因为用户空间没有权限操作磁盘或网卡，内核的权限最高，这些操作设备的过程都需要交由操作系统内核来完成，所以一般要通过内核去完成某些任务的时候，就需要使用操作系统提供的系统调用函数。</p>
<p>而一次系统调用必然会发生 2 次上下文切换：首先从用户态切换到内核态，当内核执行完任务后，再切换回用户态交由进程代码执行。</p>
<p>所以，<strong>要想减少上下文切换到次数，就要减少系统调用的次数</strong>。</p>
<blockquote>
<p>再来看看，如何减少「数据拷贝」的次数？</p>
</blockquote>
<p>在前面我们知道了，传统的文件传输方式会历经 4 次数据拷贝，而且这里面，「从内核的读缓冲区拷贝到用户的缓冲区里，再从用户的缓冲区里拷贝到 socket 的缓冲区里」，这个过程是没有必要的。</p>
<p>因为文件传输的应用场景中，在用户空间我们并不会对数据「再加工」，所以数据实际上可以不用搬运到用户空间，因此<strong>用户的缓冲区是没有必要存在的</strong>。</p>
<h3 id="如何实现零拷贝？"><a href="#如何实现零拷贝？" class="headerlink" title="如何实现零拷贝？"></a>如何实现零拷贝？</h3><p>零拷贝技术实现的方式通常有 2 种：</p>
<ul>
<li>mmap + write</li>
<li>sendfile</li>
</ul>
<p>下面就谈一谈，它们是如何减少「上下文切换」和「数据拷贝」的次数。</p>
<h4 id="mmap-write"><a href="#mmap-write" class="headerlink" title="mmap + write"></a>mmap + write</h4><p>在前面我们知道，<code>read()</code> 系统调用的过程中会把内核缓冲区的数据拷贝到用户的缓冲区里，于是为了减少这一步开销，我们可以用 <code>mmap()</code> 替换 <code>read()</code> 系统调用函数。</p>
<pre><code class="c">buf = mmap(file, len);
write(sockfd, buf, len);
</code></pre>
<p><code>mmap()</code> 系统调用函数会直接把内核缓冲区里的数据「<strong>映射</strong>」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/mmap%20%2B%20write%20%E9%9B%B6%E6%8B%B7%E8%B4%9D.png" alt="img"></p>
<p>具体过程如下：</p>
<ul>
<li>应用进程调用了 <code>mmap()</code> 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区；</li>
<li>应用进程再调用 <code>write()</code>，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据；</li>
<li>最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。</li>
</ul>
<p>我们可以得知，通过使用 <code>mmap()</code> 来代替 <code>read()</code>， 可以减少一次数据拷贝的过程。</p>
<p>但这还不是最理想的零拷贝，因为仍然需要通过 CPU 把内核缓冲区的数据拷贝到 socket 缓冲区里，而且仍然需要 4 次上下文切换，因为系统调用还是 2 次。</p>
<h4 id="sendfile"><a href="#sendfile" class="headerlink" title="sendfile"></a>sendfile</h4><p>在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 <code>sendfile()</code>，函数形式如下：</p>
<pre><code class="c">#include &lt;sys/socket.h&gt;
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
</code></pre>
<p>它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。</p>
<p>首先，它可以替代前面的 <code>read()</code> 和 <code>write()</code> 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。</p>
<p>其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。如下图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-3%E6%AC%A1%E6%8B%B7%E8%B4%9D.png" alt="img"></p>
<p>但是这还不是真正的零拷贝技术，如果网卡支持 SG-DMA（<em>The Scatter-Gather Direct Memory Access</em>）技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。</p>
<p>你可以在你的 Linux 系统通过下面这个命令，查看网卡是否支持 scatter-gather 特性：</p>
<pre><code class="bash">$ ethtool -k eth0 | grep scatter-gather
scatter-gather: on
</code></pre>
<p>于是，从 Linux 内核 <code>2.4</code> 版本开始起，对于支持网卡支持 SG-DMA 技术的情况下， <code>sendfile()</code> 系统调用的过程发生了点变化，具体过程如下：</p>
<ul>
<li>第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里；</li>
<li>第二步，缓冲区描述符和数据长度传到 socket 缓冲区，这样网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就减少了一次数据拷贝；</li>
</ul>
<p>所以，这个过程之中，只进行了 2 次数据拷贝，如下图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-%E9%9B%B6%E6%8B%B7%E8%B4%9D.png" alt="img"></p>
<p>这就是所谓的<strong>零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。</strong>。</p>
<p>零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，<strong>只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。</strong></p>
<p>所以，总体来看，<strong>零拷贝技术可以把文件传输的性能提高至少一倍以上</strong>。</p>
<p>kafka和nginx都使用了零拷贝技术。</p>
<h3 id="PageCache-有什么作用？"><a href="#PageCache-有什么作用？" class="headerlink" title="PageCache 有什么作用？"></a>PageCache 有什么作用？</h3><p>回顾前面说道文件传输过程，其中第一步都是先需要先把磁盘文件数据拷贝「内核缓冲区」里，这个「内核缓冲区」实际上是<strong>磁盘高速缓存（PageCache）</strong>。</p>
<p>由于零拷贝使用了 PageCache 技术，可以使得零拷贝进一步提升了性能，我们接下来看看 PageCache 是如何做到这一点的。</p>
<p>读写磁盘相比读写内存的速度慢太多了，所以我们应该想办法把「读写磁盘」替换成「读写内存」。于是，我们会通过 DMA 把磁盘里的数据搬运到内存里，这样就可以用读内存替换读磁盘。</p>
<p>但是，内存空间远比磁盘要小，内存注定只能拷贝磁盘里的一小部分数据。</p>
<p>那问题来了，选择哪些磁盘数据拷贝到内存呢？</p>
<p>我们都知道程序运行的时候，具有「局部性」，所以通常，刚被访问的数据在短时间内再次被访问的概率很高，于是我们可以用 <strong>PageCache 来缓存最近被访问的数据</strong>，当空间不足时淘汰最久未被访问的缓存。</p>
<p>所以，读磁盘数据的时候，优先在 PageCache 找，如果数据存在则可以直接返回；如果没有，则从磁盘中读取，然后缓存 PageCache 中。</p>
<p>还有一点，读取磁盘数据的时候，需要找到数据所在的位置，但是对于机械磁盘来说，就是通过磁头旋转到数据所在的扇区，再开始「顺序」读取数据，但是旋转磁头这个物理动作是非常耗时的，为了降低它的影响，<strong>PageCache 使用了「预读功能」</strong>。</p>
<p>比如，假设 read 方法每次只会读 <code>32 KB</code> 的字节，虽然 read 刚开始只会读 0 ～ 32 KB 的字节，但内核会把其后面的 32～64 KB 也读取到 PageCache，这样后面读取 32～64 KB 的成本就很低，如果在 32～64 KB 淘汰出 PageCache 前，进程读取到它了，收益就非常大。</p>
<p>所以，PageCache 的优点主要是两个：</p>
<ul>
<li>缓存最近被访问的数据；</li>
<li>预读功能；</li>
</ul>
<p>这两个做法，将大大提高读写磁盘的性能。</p>
<p><strong>但是，在传输大文件（GB 级别的文件）的时候，PageCache 会不起作用，那就白白浪费 DMA 多做的一次数据拷贝，造成性能的降低，即使使用了 PageCache 的零拷贝也会损失性能</strong></p>
<p>这是因为如果你有很多 GB 级别文件需要传输，每当用户访问这些大文件的时候，内核就会把它们载入 PageCache 中，于是 PageCache 空间很快被这些大文件占满。</p>
<p>另外，由于文件太大，可能某些部分的文件数据被再次访问的概率比较低，这样就会带来 2 个问题：</p>
<ul>
<li>PageCache 由于长时间被大文件占据，其他「热点」的小文件可能就无法充分使用到 PageCache，于是这样磁盘读写的性能就会下降了；</li>
<li>PageCache 中的大文件数据，由于没有享受到缓存带来的好处，但却耗费 DMA 多拷贝到 PageCache 一次；</li>
</ul>
<p>所以，针对大文件的传输，不应该使用 PageCache，也就是说不应该使用零拷贝技术，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，这样在高并发的环境下，会带来严重的性能问题。</p>
<h3 id="大文件传输用什么方式实现？"><a href="#大文件传输用什么方式实现？" class="headerlink" title="大文件传输用什么方式实现？"></a>大文件传输用什么方式实现？</h3><p>那针对大文件的传输，我们应该使用什么方式呢？</p>
<p>我们先来看看最初的例子，当调用 read 方法读取文件时，进程实际上会阻塞在 read 方法调用，因为要等待磁盘数据的返回，如下图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E9%98%BB%E5%A1%9E%20IO%20%E7%9A%84%E8%BF%87%E7%A8%8B.png" alt="img"></p>
<p>具体过程：</p>
<ul>
<li>当调用 read 方法时，会阻塞着，此时内核会向磁盘发起 I&#x2F;O 请求，磁盘收到请求后，便会寻址，当磁盘数据准备好后，就会向内核发起 I&#x2F;O 中断，告知内核磁盘数据已经准备好；</li>
<li>内核收到 I&#x2F;O 中断后，就将数据从磁盘控制器缓冲区拷贝到 PageCache 里；</li>
<li>最后，内核再把 PageCache 中的数据拷贝到用户缓冲区，于是 read 调用就正常返回了。</li>
</ul>
<p>对于阻塞的问题，可以用异步 I&#x2F;O 来解决，它工作方式如下图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E5%BC%82%E6%AD%A5%20IO%20%E7%9A%84%E8%BF%87%E7%A8%8B.png" alt="img"></p>
<p>它把读操作分为两部分：</p>
<ul>
<li>前半部分，内核向磁盘发起读请求，但是可以<strong>不等待数据就位就可以返回</strong>，于是进程此时可以处理其他任务；</li>
<li>后半部分，当内核将磁盘中的数据拷贝到进程缓冲区后，进程将接收到内核的<strong>通知</strong>，再去处理数据；</li>
</ul>
<p>而且，我们可以发现，异步 I&#x2F;O 并没有涉及到 PageCache，所以使用异步 I&#x2F;O 就意味着要绕开 PageCache。</p>
<p>绕开 PageCache 的 I&#x2F;O 叫直接 I&#x2F;O，使用 PageCache 的 I&#x2F;O 则叫缓存 I&#x2F;O。<strong>通常，对于磁盘，异步 I&#x2F;O 只支持直接 I&#x2F;O。</strong></p>
<p>前面也提到，大文件的传输不应该使用 PageCache，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache。</p>
<p>于是，<strong>在高并发的场景下，针对大文件的传输的方式，应该使用「异步 I&#x2F;O + 直接 I&#x2F;O」来替代零拷贝技术</strong>。</p>
<p>直接 I&#x2F;O 应用场景常见的两种：</p>
<ul>
<li>应用程序已经实现了磁盘数据的缓存，那么可以不需要 PageCache 再次缓存，减少额外的性能损耗。在 MySQL 数据库中，可以通过参数设置开启直接 I&#x2F;O，默认是不开启；</li>
<li>传输大文件的时候，由于大文件难以命中 PageCache 缓存，而且会占满 PageCache 导致「热点」文件无法充分利用缓存，从而增大了性能开销，因此，这时应该使用直接 I&#x2F;O。</li>
</ul>
<p>另外，由于直接 I&#x2F;O 绕过了 PageCache，就无法享受内核的这两点的优化：</p>
<ul>
<li>内核的 I&#x2F;O 调度算法会缓存尽可能多的 I&#x2F;O 请求在 PageCache 中，最后「<strong>合并</strong>」成一个更大的 I&#x2F;O 请求再发给磁盘，这样做是为了减少磁盘的寻址操作；</li>
<li>内核也会「<strong>预读</strong>」后续的 I&#x2F;O 请求放在 PageCache 中，一样是为了减少对磁盘的操作；</li>
</ul>
<p>于是，传输大文件的时候，使用「异步 I&#x2F;O + 直接 I&#x2F;O」了，就可以无阻塞地读取文件了。</p>
<p>所以，传输文件的时候，我们要根据文件的大小来使用不同的方式：</p>
<ul>
<li>传输大文件的时候，使用「异步 I&#x2F;O + 直接 I&#x2F;O」；</li>
<li>传输小文件的时候，则使用「零拷贝技术」；</li>
</ul>

      
       
    </div>
</article>







    




    </div>
    <div class="copyright">
        <p class="footer-entry">
    ©2017 hufei
</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full" data-title="切换全屏 快捷键 s"><span class="min "></span></button>
<a class="" id="rocket" ></a>

    </div>
</div>

</body>
<script src="/js/jquery.pjax.js?v=1.1.0" ></script>

<script src="/js/script.js?v=1.1.0" ></script>
<script>
    var img_resize = 'default';
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $("#post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        

        /*高亮代码块行号*/
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
        
    }

    /*打赏页面隐藏与展示*/
    

</script>

<!--加入行号的高亮代码块样式-->

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 542px;
    }
    .nav.fullscreen {
        margin-left: -542px;
    }
    .nav-left {
        width: 120px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 492px;
        }
        .nav.fullscreen {
            margin-left: -492px;
        }
        .nav-left {
            width: 100px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 492px;
            margin-left: -492px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #e2e0e0;
    }
    
    

    /*列表样式*/
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    

    
</style>







</html>
