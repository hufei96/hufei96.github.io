<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="计算机网络网络分层为什么要有通用协议和分层一开始不同公司都推出属于自身的私有网络协议，相互之间无法兼容。 于是ISO(国际标准化组织)提出要制定通用的网络协议，这样就可以兼容多种多样的设备。于是ISO制订了一堆协议即OSI协议栈， 至于为什么要分层，这跟其他复杂系统分层的原因一样，可以将复杂系统简单化。  各层之间相互独立：各层之间相互独立，各层之间不需要关心其他层是如何实现的，只需要知道自己如何">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2023/01/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="计算机网络网络分层为什么要有通用协议和分层一开始不同公司都推出属于自身的私有网络协议，相互之间无法兼容。 于是ISO(国际标准化组织)提出要制定通用的网络协议，这样就可以兼容多种多样的设备。于是ISO制订了一堆协议即OSI协议栈， 至于为什么要分层，这跟其他复杂系统分层的原因一样，可以将复杂系统简单化。  各层之间相互独立：各层之间相互独立，各层之间不需要关心其他层是如何实现的，只需要知道自己如何">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://github.com/hufei96/Image/blob/main/linuxnetcommunicate.png?raw=true">
<meta property="og:image" content="https://github.com/hufei96/Image/blob/main/HTTPcode.png?raw=true">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/%E6%95%B0%E5%AD%97%E8%AF%81%E4%B9%A6%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/%E7%A6%BB%E6%95%A3%E5%AF%B9%E6%95%B0.png">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/tcphead.png">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/tcpthreehand.png">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/tcpthreehand2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/threehand3.png">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/mtuandmss.png">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/socket.png">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/sack.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/flowcontrol.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/fastrecover.png">
<meta property="og:image" content="c:\Users\hufei\Desktop\udp.png">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/http3-over-quic-protocol-works.png">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/packet%20header.png">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/QUICPACKET.png">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/frame%20header.png">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/ipheader.png">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/ipaddress.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/nat.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/dhcprelay.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/icmp.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/igmp.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/ipv4andipv6.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/hufei96/Image/main/vpn.png">
<meta property="og:image" content="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/epoll.png">
<meta property="og:image" content="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%8D%95%E8%BF%9B%E7%A8%8B.png">
<meta property="og:image" content="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%A4%9A%E7%BA%BF%E7%A8%8B.png">
<meta property="og:image" content="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E4%B8%BB%E4%BB%8EReactor%E5%A4%9A%E7%BA%BF%E7%A8%8B.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/0ea3960fef48d4cbaeb4bec4345301e7.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/83d7f363643353c92d252e34f1d4f687.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/30c2c70721c12f9c140358fbdc5f2282.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/f8909edef2f3949f8945bb99380baab3.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/31485046f1303b57d8aaeaab103ea7ab.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/d528bae6fcec2357ba2eb8f324ad9fd5.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/dbb57b8d6071d011d05eeadd93269e13.png">
<meta property="og:image" content="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%BD%91%E7%BB%9C/showsocket.png">
<meta property="og:image" content="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%BD%91%E7%BB%9C/sar.png">
<meta property="article:published_time" content="2023-01-22T03:54:01.983Z">
<meta property="article:modified_time" content="2022-10-19T16:07:24.354Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/hufei96/Image/blob/main/linuxnetcommunicate.png?raw=true">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-计算机网络" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/01/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" class="article-date">
  <time class="dt-published" datetime="2023-01-22T03:54:01.983Z" itemprop="datePublished">2023-01-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="计算机网络"><a href="#计算机网络" class="headerlink" title="计算机网络"></a>计算机网络</h1><h2 id="网络分层"><a href="#网络分层" class="headerlink" title="网络分层"></a>网络分层</h2><h3 id="为什么要有通用协议和分层"><a href="#为什么要有通用协议和分层" class="headerlink" title="为什么要有通用协议和分层"></a>为什么要有通用协议和分层</h3><p>一开始不同公司都推出属于自身的私有网络协议，相互之间无法兼容。</p>
<p>于是ISO(国际标准化组织)提出要制定通用的网络协议，这样就可以兼容多种多样的设备。于是ISO制订了一堆协议即OSI协议栈，</p>
<p>至于为什么要分层，这跟其他复杂系统分层的原因一样，可以将复杂系统简单化。</p>
<ol>
<li><strong>各层之间相互独立</strong>：各层之间相互独立，各层之间不需要关心其他层是如何实现的，只需要知道自己如何调用下层提供好的功能就可以了（可以简单理解为接口调用）。<strong>这个和我们对开发时系统进行分层是一个道理</strong>。</li>
<li><strong>提高了整体灵活性</strong> ：每一层都可以随时替换具体的实现，你只需要保证你提供的功能以及暴露的接口的规则没有改变就行了。<strong>这个和我们平时开发系统的时候要求的高内聚、低耦合的原则也是可以对应上的。</strong></li>
<li><strong>大问题化小</strong> ：分层可以将复杂的网络间题分解为许多比较小的、界线比较清晰简单的小问题来处理和解决。这样使得复杂的计算机网络系统变得易于设计，实现和标准化。 <strong>这个和我们平时开发的时候，一般会将系统功能分解，然后将复杂的问题分解为容易理解的更小的问题是相对应的，这些较小的问题具有更好的边界（目标和接口）定义。</strong></li>
</ol>
<h3 id="OSI七层模型"><a href="#OSI七层模型" class="headerlink" title="OSI七层模型"></a>OSI七层模型</h3><p><strong>应用层</strong>：</p>
<p>提供为应用软件而设计的接口，软件应用程序（如 Web 浏览器和电子邮件客户端）依靠应用层发起通信。</p>
<p>基于TCP协议：HTTP，HTTPS，SMTP，POP3，IMAP，NFS（也可以用UDP）</p>
<p>基于UDP协议：DNS（响应报文过大时会改用TCP，此外主从域名服务器之间通信同步数据时使用TCP），DHCP，RTP</p>
<p><strong>表示层：</strong> </p>
<p>把数据转换为能与接收者的系统格式兼容并适合传输的格式，确保数据可供应用程序使用。如加密解密、转换翻译、压缩解压缩。</p>
<p>两台相互通信的设备可能使用不同的编码方法，因此第 6 层负责将传入数据转换为接收设备应用程序层可以理解的语法。</p>
<p>如果设备通过加密连接进行通信，则第 6 层负责发送端加密和接收端解密，以便向应用程序层呈现非加密可读数据。</p>
<p>最后，表示层还负责压缩从应用程序层接收的数据，然后将数据传递到第 5 层。这有助于尽量减少要传输的数据量，从而提高通信速度和效率。</p>
<p>协议：SSL、TLS</p>
<p><strong>会话层：</strong></p>
<p>负责打开和关闭两个设备之间的通信。通信打开与关闭之间的时间称为会话。会话层用于确保会话保持打开的时长足以传输所有交换数据，而后立即关闭会话以避免浪费资源。</p>
<p>会话层还负责同步数据传输与检查点。例如，如果传输一个 100MB 的文件，会话层可以每 5MB 设置一个检查点。如果在传输了 52MB 后连接断开或崩溃，可以从最后一个检查点恢复会话，也就是只需再传输 50MB 数据。若未设置检查点，则必须从头开始传输整个文件。</p>
<p>协议：</p>
<p><strong>传输层：</strong></p>
<p>第 4 层负责两个设备间的端到端通信(进程到进程)。包括从会话层提取数据，将数据分解为多个数据段（因为网络层有传输容量要求），然后再发送到第 3 层。接收设备传输层负责重组数据段，确保数据可供会话层使用。</p>
<p>传输层还负责进行流量控制和错误控制。流量控制用于确定最佳传输速度，避免采用快速连接的发送方压垮采用慢速连接的接收方。传输层通过确保接收数据的完整性（如果不完整，则请求重新传输）来对接收端进行错误控制。</p>
<p>协议：TCP、UDP</p>
<p><strong>网络层：</strong></p>
<p>网络层负责促进两个不同网络之间主机到主机之间的数据传输。如果两个通信设备位于同一网络，则不需要使用网络层。如果数据太大而无法在数据链路层上从一个节点传输到另一个节点，则网络层可以将数据段拆分为多个片段（称为数据包），独立发送数据包，并在接收设备上重新组装数据包来实现数据传递。</p>
<p>网络层还要确定数据到达目的地所需的最佳物理路径，称为路由。</p>
<p>协议：IP、ICMP</p>
<p><strong>数据链路层：</strong></p>
<p>数据链路层与网络层十分相似，但数据链路层用于促进<strong>同一网络</strong>上两个设备之间的数据传输。数据链路层从网络层提取数据包并将数据包分解成更小的部分（称为帧）。它提供错误控制。它定义了在两个物理连接的设备之间建立和终止连接的协议。它还定义了它们之间的流量控制协议。</p>
<p>协议：MAC，LLC，PPP</p>
<p><strong>物理层：</strong></p>
<p>物理层负责在设备（例如网络接口控制器、以太网集线器或网络交换机）与物理传输介质之间传输和接收原始数据(0&#x2F;1)。它将0&#x2F;1转换为电、无线电或光信号。层规范定义了诸如电压电平、电压变化时间、物理数据速率、最大传输距离、调制方案、信道访问方法和物理连接器等特性。物理层规范包含在普遍存在的蓝牙、以太网和 USB 标准的规范中。</p>
<p>物理层还指定如何在物理信号（例如电压或光脉冲）上进行编码。例如，铜线上的 1 位可能通过从 0 伏到 5 伏信号的转换来表示，而 0 位可能通过从 5 伏信号到 0 伏信号的转换来表示。</p>
<p>协议：CAN，RS232，RJ45</p>
<h3 id="TCP-x2F-IP-网络模型"><a href="#TCP-x2F-IP-网络模型" class="headerlink" title="TCP&#x2F;IP 网络模型"></a>TCP&#x2F;IP 网络模型</h3><p>TCP&#x2F;IP 出现时间更早于OSI，但一开始只有协议没有模型，后来OSI出现后与OSI七层模型进行了相对应，划分了四层模型。因为在OSI出现之时已有大量商家支持TCP&#x2F;IP，加上OSI功能太多，太过复杂，TCP&#x2F;IP成了实际使用的网络模型。</p>
<p><strong>应用层</strong>：</p>
<p>对应OSI中的应用、表示、会话层</p>
<p><strong>传输层：</strong></p>
<p>对应OSI中的传输层</p>
<p><strong>网络层：</strong></p>
<p>对应OSI中的网络层</p>
<p><strong>网络接口层：</strong></p>
<p>对应OSI中的数据链路层和物理层</p>
<h2 id="键入网址到网页显示期间发生了什么？"><a href="#键入网址到网页显示期间发生了什么？" class="headerlink" title="键入网址到网页显示期间发生了什么？"></a>键入网址到网页显示期间发生了什么？</h2><p>1.解析 URL</p>
<p>首先浏览器做的第一步工作就是要对 <code>URL</code> 进行解析，从而生成发送给 <code>Web</code> 服务器的请求信息。</p>
<p>2.生成http请求</p>
<p>对 <code>URL</code> 进行解析之后，浏览器确定了 Web 服务器和文件名，接下来就是根据这些信息来生成 HTTP 请求消息了。</p>
<p>3.DNS查询目的地IP地址</p>
<p>通过浏览器解析 URL 并生成 HTTP 消息后，需要委托操作系统将消息发送给 <code>Web</code> 服务器。</p>
<p>但在发送之前，还有一项工作需要完成，那就是<strong>查询服务器域名对应的 IP 地址</strong>，因为委托操作系统发送消息时，必须提供通信对象的 IP 地址。</p>
<p>浏览器会先看自身有没有对这个域名的缓存，如果有，就直接返回，如果没有，就去问操作系统，操作系统也会去看自己的缓存，如果有，就直接返回，如果没有，再去 hosts 文件看，也没有，才会去问「本地 DNS 服务器」。</p>
<p>客户端会发出一个 DNS 请求，问 <a target="_blank" rel="noopener" href="http://www.server.com/">www.server.com</a> 的 IP 是啥，并发给本地 DNS 服务器（也就是客户端的 TCP&#x2F;IP 设置中填写的 DNS 服务器地址）。</p>
<p>本地域名服务器收到客户端的请求后，如果缓存能找到 <a target="_blank" rel="noopener" href="http://www.server.com,则它直接返回/">www.server.com，则它直接返回</a> IP 地址。如果没有，本地 DNS 会去问它的根域名服务器</p>
<p>根域名服务器收到来自本地 DNS 的请求后，发现后置是 .com，返回.com 顶级域名服务器地址</p>
<p>本地 DNS 收到顶级域名服务器的地址后，向顶级域名服务器发起请求问<a target="_blank" rel="noopener" href="http://www.server.com/">www.server.com</a> 的 IP 地址，依次迭代直到查询到服务器对应的IP地址</p>
<p>4.将HTTP请求报文传给TCP&#x2F;IP协议栈</p>
<p>TCP&#x2F;IP协议栈实现了客户端和服务器进程到进程之间的数据传输</p>
<p>报文依次加上TCP首部、IP首部、MAC首部后从网卡发送出去</p>
<p>5.网卡将报文转换为电信号从网线发送出去</p>
<p>网络包只是存放在内存中的一串二进制数字信息，没有办法直接发送给对方。因此，我们需要将<strong>数字信息转换为电信号</strong>，才能在网线上传输，也就是说，这才是真正的数据发送过程。</p>
<p>负责执行这一操作的是网卡，要控制网卡还需要靠网卡驱动程序。</p>
<p>网卡驱动获取网络包之后，会将其复制到网卡内的缓存区中，接着会在其开头加上报头和起始帧分界符，在末尾加上用于检测错误的帧校验序列。</p>
<p>最后网卡会将包转为电信号，通过网线发送出去。</p>
<p>6.通过交换机和路由器层层转发将报文传输到服务器</p>
<p>交换机用于同一网络下设备间的数据传输，基于MAC地址转发。分割了冲突域但没分割广播域，同一网络下的设备之间可以自由通信，交换机无法控制</p>
<p>而路由器用于不同网络设备间的数据传输，基于IP地址转发。分割了广播域，不同的广播域的通信必须流经路由器，路由器可以控制数据能不能通过；同时路由器的不同端口可以分别连接不同的网络，如一个连以太网，另一个连PPP</p>
<p>7.服务器的 HTTP 进程把客户端请求资源封装在 HTTP 响应报文里发送给客户端</p>
<p>8.客户端收到响应报文后，将页面交给浏览器渲染</p>
<h2 id="LINUX收发数据流程"><a href="#LINUX收发数据流程" class="headerlink" title="LINUX收发数据流程"></a>LINUX收发数据流程</h2><h3 id="Linux-接收网络包的流程"><a href="#Linux-接收网络包的流程" class="headerlink" title="Linux 接收网络包的流程"></a>Linux 接收网络包的流程</h3><p>网卡是计算机里的一个硬件，专门负责接收和发送网络包，当网卡接收到一个网络包后，会通过 DMA 技术，将网络包放入到 Ring Buffer，这个是一个环形缓冲区。</p>
<p>当有网络包到达时，网卡发起硬件中断，于是会执行网卡硬件中断处理函数，中断处理函数处理完需要<strong>暂时屏蔽中断</strong>，然后唤醒<strong>软中断</strong>来<strong>轮询处理数据</strong>，直到没有新数据时才恢复中断，这样<strong>一次中断处理多个网络包</strong>，于是就可以降低网卡中断带来的性能开销。</p>
<p>那软中断是怎么处理网络包的呢？它会从 Ring Buffer 中拷贝数据到内核 struct sk_buff 缓冲区中，从而可以作为一个网络包交给网络协议栈进行逐层处理。</p>
<p>首先，会先进入到网络接口层，在这一层会检查报文的合法性，如果不合法则丢弃，合法则会找出该网络包的上层协议的类型，比如是 IPv4，还是 IPv6，接着再去掉帧头和帧尾，然后交给网络层。</p>
<p>到了网络层，则取出 IP 包，判断网络包下一步的走向，比如是交给上层处理还是转发出去。当确认这个网络包要发送给本机后，就会从 IP 头里看看上一层协议的类型是 TCP 还是 UDP，接着去掉 IP 头，然后交给传输层。</p>
<p>传输层取出 TCP 头或 UDP 头，根据四元组「源 IP、源端口、目的 IP、目的端口」 作为标识，找出对应的 Socket，并把数据拷贝到 Socket 的接收缓冲区。</p>
<p>最后，应用层程序调用 Socket 接口，从内核的 Socket 接收缓冲区读取新到来的数据到应用层。</p>
<h3 id="Linux-发送网络包的流程"><a href="#Linux-发送网络包的流程" class="headerlink" title="Linux 发送网络包的流程"></a>Linux 发送网络包的流程</h3><p>发送网络包的流程正好和接收流程相反。</p>
<p>首先，应用程序会调用 Socket 发送数据包的接口，由于这个是系统调用，所以会从用户态陷入到内核态中的 Socket 层，Socket 层会将应用层数据拷贝到 Socket 发送缓冲区中。</p>
<p>接下来，网络协议栈从 Socket 发送缓冲区中取出数据包，并按照 TCP&#x2F;IP 协议栈从上到下逐层处理。</p>
<p>如果使用的是 TCP 传输协议发送数据，那么会在传输层增加 TCP 包头，然后交给网络层，网络层会给数据包增加 IP 包，然后通过查询路由表确认下一跳的 IP，并按照 MTU 大小进行分片。</p>
<p>分片后的网络包，就会被送到网络接口层，在这里会通过 ARP 协议获得下一跳的 MAC 地址，然后增加帧头和帧尾，放到发包队列中。</p>
<p>这一些准备好后，会触发软中断告诉网卡驱动程序，这里有新的网络包需要发送，最后驱动程序通过 DMA，从发包队列中读取网络包，将其放入到硬件网卡的队列中，随后物理网卡再将它发送出去。</p>
<p><img src="https://github.com/hufei96/Image/blob/main/linuxnetcommunicate.png?raw=true" alt="linuxnetcommunicate.png"></p>
<h2 id="HTTP"><a href="#HTTP" class="headerlink" title="HTTP"></a>HTTP</h2><p>HTTP 是超文本传输协议，也就是<strong>H</strong>yperText <strong>T</strong>ransfer <strong>P</strong>rotocol。超文本就是<strong>超越了普通文本的文本</strong>，它是文字、图片、视频等的混合体，最关键有超链接，能从一个超文本跳转到另外一个超文本。</p>
<p>HTTP 是一个在计算机世界里专门在<strong>两点</strong>之间传输文字、图片、音频、视频等<strong>超文本</strong>数据的<strong>约定和规范</strong>。</p>
<p>浏览器（browser），不管是chrome还是IE，它们不仅要能访问自家公司的服务器（server），还需要访问其他公司的网站服务器，因此它们需要有个统一的标准，不然大家没法交流。于是，HTTP就是那个时代用于统一 browser&#x2F;server (b&#x2F;s) 的协议。</p>
<p>tcp协议只是字节流，接收方收到数据后不一定能理解发送方的意思。http协议相当于定义了说话的规则。HTTP定义了请求对象和响应对象，各种头字段，消息的边界，有了http发送方和接收方才能相互明白。</p>
<p>Http被设计为客户端发送请求，服务器给出响应的一问一答形式进行通信。<strong>这就意味着如果客户端不主动地向服务器发送消息，服务器就无法主动给客户端推送消息。</strong>这是http的一大缺陷。</p>
<h3 id="常用状态码"><a href="#常用状态码" class="headerlink" title="常用状态码"></a>常用状态码</h3><p><img src="https://github.com/hufei96/Image/blob/main/HTTPcode.png?raw=true" alt="HTTPcode.png"></p>
<p><code>1xx</code> 类状态码属于<strong>提示信息</strong>，是协议处理中的一种中间状态，实际用到的比较少。</p>
<p><code>2xx</code> 类状态码表示服务器<strong>成功</strong>处理了客户端的请求，也是我们最愿意看到的状态。</p>
<ul>
<li>「<strong>200 OK</strong>」是最常见的成功状态码，表示一切正常。如果是非 <code>HEAD</code> 请求，服务器返回的响应头都会有 body 数据。</li>
<li>「<strong>204 No Content</strong>」也是常见的成功状态码，与 200 OK 基本相同，但响应头没有 body 数据。</li>
<li>「<strong>206 Partial Content</strong>」是应用于 HTTP 分块下载或断点续传，表示响应返回的 body 数据并不是资源的全部，而是其中的一部分，也是服务器处理成功的状态。</li>
</ul>
<p><code>3xx</code> 类状态码表示客户端请求的资源发生了变动，需要客户端用新的 URL 重新发送请求获取资源，也就是<strong>重定向</strong>。</p>
<ul>
<li>「<strong>301 Moved Permanently</strong>」表示永久重定向，说明请求的资源已经不存在了，需改用新的 URL 再次访问。</li>
<li>「<strong>302 Found</strong>」表示临时重定向，说明请求的资源还在，但暂时需要用另一个 URL 来访问。</li>
</ul>
<p>301 和 302 都会在响应头里使用字段 <code>Location</code>，指明后续要跳转的 URL，浏览器会自动重定向新的 URL。</p>
<ul>
<li>「<strong>304 Not Modified</strong>」不具有跳转的含义，表示资源未修改，重定向已存在的缓冲文件，也称缓存重定向，也就是告诉客户端可以继续使用缓存资源，用于缓存控制。</li>
</ul>
<p><code>4xx</code> 类状态码表示客户端发送的<strong>报文有误</strong>，服务器无法处理，也就是错误码的含义。</p>
<ul>
<li>「<strong>400 Bad Request</strong>」表示客户端请求的报文有错误，但只是个笼统的错误。</li>
<li>「<strong>403 Forbidden</strong>」表示服务器禁止访问资源，并不是客户端的请求出错。</li>
<li>「<strong>404 Not Found</strong>」表示请求的资源在服务器上不存在或未找到，所以无法提供给客户端。</li>
</ul>
<p><code>5xx</code> 类状态码表示客户端请求报文正确，但是<strong>服务器处理时内部发生了错误</strong>，属于服务器端的错误码。</p>
<ul>
<li>「<strong>500 Internal Server Error</strong>」与 400 类型，是个笼统通用的错误码，服务器发生了什么错误，我们并不知道。</li>
<li>「<strong>501 Not Implemented</strong>」表示客户端请求的功能还不支持，类似“即将开业，敬请期待”的意思。</li>
<li>「<strong>502 Bad Gateway</strong>」通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，访问后端服务器发生了错误。</li>
<li>「<strong>503 Service Unavailable</strong>」表示服务器当前很忙，暂时无法响应客户端，类似“网络服务正忙，请稍后重试”的意思。</li>
</ul>
<h3 id="报文常用字段"><a href="#报文常用字段" class="headerlink" title="报文常用字段"></a>报文常用字段</h3><p><em>Host</em> 字段</p>
<p>客户端发送请求时，用来指定服务器的域名。有了 <code>Host</code> 字段，就可以将请求发往「同一台」服务器上的不同网站。</p>
<p><em>Content-Length 字段</em></p>
<p>服务器在返回数据时，会有 <code>Content-Length</code> 字段，表明本次回应的数据长度。</p>
<p><em>Connection 字段</em></p>
<p><code>Connection</code> 字段最常用于客户端要求服务器使用 TCP 持久连接，以便其他请求复用。HTTP&#x2F;1.1 版本的默认连接都是持久连接，但为了兼容老版本的 HTTP，需要指定 <code>Connection</code> 首部字段的值为 <code>Keep-Alive</code>。</p>
<p><em>Content-Type 字段</em></p>
<p><code>Content-Type</code> 字段用于服务器回应时，告诉客户端，本次数据是什么格式。如Content-Type: text&#x2F;html; charset&#x3D;utf-8。客户端请求的时候，可以使用 <code>Accept</code> 字段声明自己可以接受哪些数据格式。</p>
<p><em>Content-Encoding 字段</em></p>
<p><code>Content-Encoding</code> 字段说明数据的压缩方法。表示服务器返回的数据使用了什么压缩格式。客户端在请求时，用 <code>Accept-Encoding</code> 字段说明自己可以接受哪些压缩方法。</p>
<h3 id="GET和POST区别"><a href="#GET和POST区别" class="headerlink" title="GET和POST区别"></a>GET和POST区别</h3><p>根据 RFC 规范，<strong>GET 的语义是从服务器获取指定的资源</strong>，这个资源可以是静态的文本、页面、图片视频等。</p>
<p>根据 RFC 规范，<strong>POST 的语义是根据请求负荷（报文body）对指定的资源做出处理</strong>，具体的处理方式视资源类型而不同。</p>
<p><em>区别</em></p>
<p>1.一般而言，GET 请求的参数位置写在 URL 中，POST 请求携带数据的位置写在报文 body 中。</p>
<p>URL 规定只能支持 ASCII码，所以 GET 请求的参数只允许 ASCII 字符 ，而且浏览器会对 URL 的长度有限制(规范无限制)。</p>
<p> body 中的数据可以是任意格式的数据，只要客户端与服务端协商好即可，而且浏览器不会对 body 大小做限制。</p>
<p>2.GET请求会被浏览器主动cache，而POST不会，除非手动设置。</p>
<p>3.<code>POST </code>比<code> GET</code> 安全，因为数据在地址栏上不可见</p>
<p>然而，从传输的角度来说，他们都是不安全的，因为<code> HTTP</code> 在网络上是明文传输的，只要在网络节点上捉包，就能完整地获取数据报文</p>
<p>只有使用<code>HTTPS</code>才能加密安全、</p>
<p>4.GET产生一个TCP数据包，而POST产生两个TCP数据包。</p>
<p>对于<code>GET</code>方式的请求，浏览器会把<code>http header</code>和<code>data</code>一并发送出去，服务器响应200（返回数据）</p>
<p>对于<code>POST</code>，浏览器先发送<code>header</code>，服务器响应100 <code>continue</code>，浏览器再发送<code>data</code>，服务器响应200 ok</p>
<p>并不是所有浏览器都会在<code>POST</code>中发送两次包，<code>Firefox</code>就只发送一次</p>
<p>实际二者区别完全取决于客户端和服务器的协商。开发过程中，开发者不一定会按照 RFC 规范定义的语义来实现 GET 和 POST 方法。可以用 GET 方法实现新增或删除数据的请求，也可以用 POST 方法实现查询数据的请求。同理，GET可以使用body，POST也可以使用URL，一切取决于开发者。</p>
<h3 id="HTTP有哪几种缓存"><a href="#HTTP有哪几种缓存" class="headerlink" title="HTTP有哪几种缓存"></a>HTTP有哪几种缓存</h3><p>对于一些具有重复性的 HTTP 请求，比如每次请求得到的数据都一样的，我们可以把这对「请求-响应」的数据都缓存在本地，那么下次就直接读取本地的数据，不必在通过网络获取服务器的响应了。</p>
<p>HTTP 缓存有两种实现方式，分别是<strong>强制缓存和协商缓存</strong>。</p>
<p><em><strong>强制缓存</strong></em></p>
<p>强缓存指的是只要浏览器判断缓存没有过期，则直接使用浏览器的本地缓存，决定是否使用<strong>缓存的主动性在于浏览器</strong>这边。</p>
<p>如下，返回的是 200 状态码，但在 size 项中标识的是 from disk cache，就是使用了强制缓存。</p>
<p><em>Status Code:200(from disk cache)</em></p>
<p>强缓存是利用下面这两个 HTTP 响应头部（Response Header）字段实现的，它们都用来表示资源在客户端缓存的有效期：</p>
<ul>
<li><code>Cache-Control</code>， 是一个相对时间；</li>
<li><code>Expires</code>，是一个绝对时间；</li>
</ul>
<p>如果 HTTP 响应头部同时有 Cache-Control 和 Expires 字段的话，<strong>Cache-Control的优先级高于 Expires</strong> 。</p>
<p>Cache-control 选项更多一些，设置更加精细，所以建议使用 Cache-Control 来实现强缓存。具体的实现流程如下：</p>
<ul>
<li>当浏览器第一次请求访问服务器资源时，服务器会在返回这个资源的同时，在 Response 头部加上 Cache-Control，Cache-Control 中设置了过期时间大小；</li>
<li>浏览器再次请求访问服务器中的该资源时，会先<strong>通过请求资源的时间与 Cache-Control 中设置的过期时间大小，来计算出该资源是否过期</strong>，如果没有，则使用该缓存，否则重新请求服务器；</li>
<li>服务器再次收到请求后，会再次更新 Response 头部的 Cache-Control。</li>
</ul>
<p><em><strong>协商缓存</strong></em></p>
<p>你可能看到过某些请求的响应码是 <code>304</code>，这个是告诉浏览器可以使用本地缓存的资源。与服务端协商之后，通过协商结果来判断是否使用本地缓存，这种通过<strong>服务端告知客户端是否可以使用缓存</strong>的方式被称为协商缓存。只有在未能命中强制缓存的时候，才能发起带有协商缓存字段的请求。</p>
<p>协商缓存可以基于两种头部来实现。</p>
<p>第一种：请求头部中的 <code>If-Modified-Since</code> 字段与响应头部中的 <code>Last-Modified</code> 字段实现，这两个字段的意思是：</p>
<ul>
<li>响应头部中的 <code>Last-Modified</code>：标示这个响应资源的最后修改时间；</li>
<li>请求头部中的 <code>If-Modified-Since</code>：当资源过期了，发现响应头中具有 Last-Modified 声明，则再次发起请求的时候带上 Last-Modified 的时间，服务器收到请求后发现有 If-Modified-Since 则与被请求资源的最后修改时间进行对比（Last-Modified），如果最后修改时间较新（大），说明资源又被改过，则返回最新资源，HTTP 200 OK；如果最后修改时间较旧（小），说明资源无新修改，响应 HTTP 304 走缓存。</li>
</ul>
<p>第二种：请求头部中的 <code>If-None-Match</code> 字段与响应头部中的 <code>ETag</code> 字段，这两个字段的意思是：</p>
<ul>
<li>响应头部中 <code>Etag</code>：唯一标识响应资源；</li>
<li>请求头部中的 <code>If-None-Match</code>：当资源过期时，浏览器发现响应头里有 Etag，则再次向服务器发起请求时，会将请求头If-None-Match 值设置为 Etag 的值。服务器收到请求后进行比对，如果资源没有变化返回 304，如果资源变化了返回 200。</li>
</ul>
<p>第一种实现方式是基于时间实现的，第二种实现方式是基于一个唯一标识实现的，相对来说后者可以更加准确地判断文件内容是否被修改，避免由于时间篡改导致的不可靠问题。</p>
<p>如果 HTTP 响应头部同时有 Etag 和 Last-Modified 字段的时候， Etag 的优先级更高，也就是先会判断 Etag 是否变化了，如果 Etag 没有变化，然后再看 Last-Modified。</p>
<p>注意，<strong>协商缓存这两个字段都需要配合强制缓存中 Cache-control 字段来使用，只有在未能命中强制缓存的时候，才能发起带有协商缓存字段的请求</strong>。</p>
<p>使用 ETag 字段实现的协商缓存的过程如下；</p>
<ul>
<li><p>当浏览器第一次请求访问服务器资源时，服务器会在返回这个资源的同时，在 Response 头部加上 ETag 唯一标识，这个唯一标识的值是根据当前请求的资源生成的；</p>
</li>
<li><p>当浏览器再次请求访问服务器中的该资源时，首先会先检查强制缓存是否过期，如果没有过期，则直接使用本地缓存；如果缓存过期了，会在 Request 头部加上 If-None-Match 字段，该字段的值就是 ETag 唯一标识；</p>
</li>
<li><p>服务器再次收到请求后，</p>
<p>会根据请求中的 If-None-Match 值与当前请求的资源生成的唯一标识进行比较：</p>
<ul>
<li><strong>如果值相等，则返回 304 Not Modified，不会返回资源</strong>；</li>
<li>如果不相等，则返回 200 状态码和返回资源，并在 Response 头部加上新的 ETag 唯一标识；</li>
</ul>
</li>
<li><p>如果浏览器收到 304 的请求响应状态码，则会从本地缓存中加载资源，否则更新资源。</p>
</li>
</ul>
<h3 id="HTTP无状态"><a href="#HTTP无状态" class="headerlink" title="HTTP无状态"></a>HTTP无状态</h3><p>HTTP是无状态的协议，因为它的每个请求都是完全独立的，每个请求包含了处理这个请求所需的完整的数据，每一个请求发生时都不需要去回忆过去的请求产生了什么样的记忆，发送请求不涉及到状态变更。即使在HTTP&#x2F;1.1上，同一个连接允许传输多个HTTP请求的情况下，如果第一个请求出错了，后面的请求一般也能够继续处理。这里必须非常明确，无状态的仅仅是 HTTP 这一层，在此之上我们可以搭建有状态的应用层。</p>
<p>作为对比，SMTP协议，它的第一条消息必须是HELO，用来握手，在HELO发送之前其他任何命令都是不能发送的；接下来一般要进行AUTH阶段，用来验证用户名和密码；接下来可以发送邮件数据；最后，通过QUIT命令退出。可以看到，在整个传输层上，通信的双方是必须要时刻记住当前连接的状态的，因为不同的状态下能接受的命令是不同的；另外，之前的命令传输的某些数据也必须要记住，可能会对后面的命令产生影响。这种就叫做有状态的协议。</p>
<p><strong>无状态的优点</strong></p>
<p>1.协议的结构比有状态的协议更简单，一般来说实现起来也更简单，不需要使用状态机，一个循环就行了。</p>
<p>2.请求相互独立，前面请求出错不会影响后面的请求。</p>
<p><strong>无状态的缺点</strong></p>
<p>主要缺点在于，单个请求需要的所有信息都必须要包含在请求中一次发送到服务端，这导致单个消息的结构需要比较复杂，必须能够支持大量元数据，因此HTTP消息的解析要比其他许多协议都要复杂得多。同时，这也导致了相同的数据在多个请求上往往需要反复传输，例如同一个连接上的每个请求都需要传输Host、Authentication、Cookies、Server等往往是完全重复的元数据，一定程度上降低了协议的效率。</p>
<h3 id="cookie，session和token"><a href="#cookie，session和token" class="headerlink" title="cookie，session和token"></a>cookie，session和token</h3><p><em><strong>session</strong></em></p>
<p>由于HTTP协议是无状态的协议，所以服务端需要记录用户的状态时，就需要用某种机制来识具体的用户，session就是其中的一种。服务端为每个用户创建了特定的Session用于标识这个用户，Session里面存储着该用户的购物车等详细信息，服务器可以通过这些信息返回该用户的定制化网页，有效解决了追踪用户的问题。</p>
<p>第一次建立连接时会发一个会话标识给客户(session id)， 说白了就是一个随机的字串，每个人收到的都不一样， 每次客户端向服务器发起HTTP请求的时候，把这个字符串给一并捎过来， 这样服务器就能区分开谁是谁了。</p>
<p>总结：session 是一个数据结构，由网站的开发者设计，保存在服务器，有一个唯一标识session id，只要客户端传来一个唯一的 session ID，服务器就可以找到对应的 session，认出这个客户。</p>
<p><em><strong>cookie</strong></em></p>
<p>Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式。</p>
<p>cookie由服务器生成，发送给浏览器，浏览器把cookie以kv键值对形式保存到某个目录下的文本文件内，下一次请求同一网站时会把该cookie发送给服务器，这样服务器就通过cookie识别了用户的身份。</p>
<p>可以将session id存在cookie中，将 session ID 通过 cookie 的形式传送给客户端，这样客户端每次发送cookie的同时也发送了session id，所以说cookie可以用来实现session，但这只是session实现方式的一种。若客户端禁用了cookie，还可以通过使用url重写的方式使用session，即每次HTTP交互，URL后面都会被附加上一个诸如 sid&#x3D;xxxxx 这样的参数，服务端据此来识别用户。</p>
<p><em><strong>token</strong></em></p>
<p>session存在一定的缺陷：服务器要保存所有人的session id并在客户端请求时进行验证，这对服务器说是一个巨大的开销 ， 严重的限制了服务器扩展能力，比如说我用两个机器组成了一个集群， 小F通过机器A登录了系统， 那session id会保存在机器A上， 假设小F的下一次请求被转发到机器B怎么办？ 机器B没有小F的 session id。那只好做session 的复制了， 把session id 在两个机器复制一份，这样开销很大。</p>
<p>或者把session id 集中存储到一个地方， 所有的机器都来访问这个地方的数据， 这样一来，就不用复制了， 但是增加了单点失败的可能性， 要是那个负责session 的机器挂了， 所有人都得重新登录一遍。</p>
<p>于是有人就一直在思考， 让每个客户端去保存session不就可以解决这个问题了吗。</p>
<p>基于这个思想，token产生了。</p>
<p>token就是一个令牌，里面是加密的user id和user id的数字签名。客户端在连接时，服务器会生成一份token发给客户端，客户端每次连接时带上token，服务器只需验证token是否合法即可。</p>
<p>token 和 session 的验证机制最大的区别是用“签名验证机制”代替了“白名单验证机制”。</p>
<p>session 的问题在于前端传来的 session_id 可以伪造，所以必须在服务器维护一个 session_id 的白名单来验证 session_id 的合法性。token 的改进之处就在这里，token 通过签名机制，只要前端传来的 token 能通过签名认证就是合法的，不需要服务器维护任何东西，所有的需要东西都放在在 token 里面。</p>
<p><strong>安全问题</strong></p>
<p>浏览器的跨域+XSRF限制杜绝了钓鱼网站获取token；https杜绝了中间人劫持。</p>
<h3 id="cookie，localStorage，sessionStorage-的区别"><a href="#cookie，localStorage，sessionStorage-的区别" class="headerlink" title="cookie，localStorage，sessionStorage 的区别"></a>cookie，localStorage，sessionStorage 的区别</h3><p>cookie是在HTML4中使用的给客户端保存数据的，也可以和session配合实现跟踪浏览器用户身份；</p>
<p>而webstorage（包括：localStorage和sessionStorage）是在HTML5提出来的，纯粹为了保存数据，不会与服务器端通信。</p>
<p>WebStorage两个主要目标：（1）提供一种在cookie之外存储会话数据的路径。（2）提供一种存储大量可以跨会话存在的数据的机制。</p>
<p>相同点：</p>
<p> cookie，localStorage，sessionStorage都是在客户端保存数据的，存储数据的类型：都是字符串。</p>
<p>不同点：</p>
<p>1、生命周期：</p>
<p>localStorage 存储<strong>持久数据</strong>，浏览器关闭后数据<strong>不丢失</strong>除非主动删除数据；</p>
<p>sessionStorage 数据在当前浏览器窗口关闭后<strong>自动删除</strong>。</p>
<p>cookie 设置的cookie<strong>过期时间</strong>之前一直有效，即使窗口或浏览器关闭</p>
<p>2、网络流量：cookie的数据每次都会发给服务器端，而localstorage和sessionStorage不会与服务器端通信，纯粹为了保存数据，所以，webstorage更加节约网络流量。</p>
<p>3、大小限制：cookie大小限制在4KB，非常小；localstorage和sessionStorage在5M</p>
<p>4、安全性：WebStorage不会随着HTTP header发送到服务器端，所以安全性相对于cookie来说比较高一些，不会担心截获。</p>
<p>5、使用方便性上：WebStorage提供了一些方法，数据操作比cookie方便；</p>
<h3 id="HTTP-x2F-1-1、HTTP-x2F-2、HTTP-x2F-3-演变"><a href="#HTTP-x2F-1-1、HTTP-x2F-2、HTTP-x2F-3-演变" class="headerlink" title="HTTP&#x2F;1.1、HTTP&#x2F;2、HTTP&#x2F;3 演变"></a>HTTP&#x2F;1.1、HTTP&#x2F;2、HTTP&#x2F;3 演变</h3><h4 id="HTTP-x2F-1-1-相比-HTTP-x2F-1-0-提高了什么性能？"><a href="#HTTP-x2F-1-1-相比-HTTP-x2F-1-0-提高了什么性能？" class="headerlink" title="HTTP&#x2F;1.1 相比 HTTP&#x2F;1.0 提高了什么性能？"></a>HTTP&#x2F;1.1 相比 HTTP&#x2F;1.0 提高了什么性能？</h4><p>HTTP&#x2F;1.1 相比 HTTP&#x2F;1.0 性能上的改进：</p>
<ul>
<li>使用长连接的方式改善了 HTTP&#x2F;1.0 短连接造成的性能开销。</li>
<li>支持管道（pipeline）网络传输，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。</li>
</ul>
<p>但 HTTP&#x2F;1.1 还是有性能瓶颈：</p>
<ul>
<li>请求 &#x2F; 响应头部（Header）未经压缩就发送，首部信息越多延迟越大。只能压缩 <code>Body</code> 的部分；</li>
<li>发送冗长的首部。每次互相发送相同的首部造成的浪费较多；</li>
<li>服务器是按请求的顺序响应的，如果服务器响应慢，会招致客户端一直请求不到数据，也就是<strong>队头阻塞</strong>；</li>
<li>没有请求优先级控制；</li>
<li>请求只能从客户端开始，服务器只能被动响应。</li>
</ul>
<h4 id="HTTP-x2F-2-做了什么优化？"><a href="#HTTP-x2F-2-做了什么优化？" class="headerlink" title="HTTP&#x2F;2 做了什么优化？"></a>HTTP&#x2F;2 做了什么优化？</h4><p>HTTP&#x2F;2 相比 HTTP&#x2F;1.1 性能上的改进：</p>
<p><em>1. 头部压缩</em></p>
<p>HTTP&#x2F;2 会<strong>压缩头</strong>（Header）如果你同时发出多个请求，他们的头是一样的或是相似的，那么，协议会帮你<strong>消除重复的部分</strong>。</p>
<p>这就是所谓的 <code>HPACK</code> 算法：在客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就<strong>提高速度</strong>了。</p>
<p><em>2. 二进制格式</em></p>
<p>HTTP&#x2F;2 不再像 HTTP&#x2F;1.1 里的纯文本形式的报文，而是全面采用了<strong>二进制格式</strong>，头信息和数据体都是二进制，并且统称为帧（frame）：<strong>头信息帧（Headers Frame）和数据帧（Data Frame）</strong>。</p>
<p><em>3. 数据流</em></p>
<p>HTTP&#x2F;2 的数据包不是按顺序发送的，同一个连接里面连续的数据包，可能属于不同的回应。因此，必须要对数据包做标记，指出它属于哪个回应。</p>
<p>在 HTTP&#x2F;2 中每个请求或相应的所有数据包，称为一个数据流（<code>Stream</code>）。每个数据流都标记着一个独一无二的编号（Stream ID），<strong>不同 Stream 的帧是可以乱序发送的（因此可以并发不同的 Stream ）</strong>，因为每个帧的头部会携带 Stream ID 信息，所以接收端可以通过 Stream ID 有序组装成 HTTP 消息</p>
<p>客户端和服务器<strong>双方都可以建立 Stream</strong>， Stream ID 也是有区别的，客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号。</p>
<p>客户端还可以<strong>指定数据流的优先级</strong>。优先级高的请求，服务器就先响应该请求。</p>
<p><em>4. 多路复用</em></p>
<p>HTTP&#x2F;2 是可以在<strong>一个连接中并发多个请求或回应，而不用按照顺序一一对应</strong>。</p>
<p>移除了 HTTP&#x2F;1.1 中的串行请求，不需要排队等待，也就不会再出现「队头阻塞」问题，<strong>降低了延迟，大幅度提高了连接的利用率</strong>。</p>
<p><em>5. 服务器推送</em></p>
<p>HTTP&#x2F;2 还在一定程度上改善了传统的「请求 - 应答」工作模式，服务端不再是被动地响应，可以<strong>主动</strong>向客户端发送消息。</p>
<p>比如，客户端通过 HTTP&#x2F;1.1 请求从服务器那获取到了 HTML 文件，而 HTML 可能还需要依赖 CSS 来渲染页面，这时客户端还要再发起获取 CSS 文件的请求，需要两次消息往返，此时服务器可以直接主动推送 CSS 文件，减少了消息传递的次数。</p>
<p><em><strong>HTTP&#x2F;2 有什么缺陷？</strong></em></p>
<p>HTTP&#x2F;2 是基于 TCP 协议来传输数据的，一旦发生了丢包现象，就会触发 TCP 的重传机制，一个 TCP 连接中的所有的 HTTP 请求都必须等待这个丢了的包被重传回来。<strong>这就是 HTTP&#x2F;2 队头阻塞问题。</strong></p>
<h4 id="HTTP-x2F-3-做了哪些优化？"><a href="#HTTP-x2F-3-做了哪些优化？" class="headerlink" title="HTTP&#x2F;3 做了哪些优化？"></a>HTTP&#x2F;3 做了哪些优化？</h4><p>前面我们知道了 HTTP&#x2F;1.1 和 HTTP&#x2F;2 都有队头阻塞的问题：</p>
<ul>
<li>HTTP&#x2F;1.1 中的管道（ pipeline）虽然解决了请求的队头阻塞，但是<strong>没有解决响应的队头阻塞</strong>，因为服务端需要按顺序响应收到的请求，如果服务端处理某个请求消耗的时间比较长，那么只能等相应完这个请求后， 才能处理下一个请求，这属于 HTTP 层队头阻塞。</li>
<li>HTTP&#x2F;2 虽然通过多个请求复用一个 TCP 连接解决了 HTTP 的队头阻塞 ，但是<strong>一旦发生丢包，就会阻塞住所有的 HTTP 请求</strong>，这属于 TCP 层队头阻塞。</li>
</ul>
<p>HTTP&#x2F;2 队头阻塞的问题是因为 TCP，所以 <strong>HTTP&#x2F;3 把 HTTP 下层的 TCP 协议改成了 UDP！</strong></p>
<p>UDP 发生是不管顺序，也不管丢包的，所以不会出现像 HTTP&#x2F;2 队头阻塞的问题</p>
<p>大家都知道 UDP 是不可靠传输的，但基于 UDP 的 <strong>QUIC 协议</strong> 可以实现类似 TCP 的可靠性传输。</p>
<p>QUIC 有以下 3 个特点。</p>
<p><em>1、无队头阻塞</em></p>
<p>QUIC 协议也有类似 HTTP&#x2F;2 Stream 与多路复用的概念，也是可以在同一条连接上并发传输多个 Stream，Stream 可以认为就是一条 HTTP 请求。</p>
<p>QUIC 有自己的一套机制可以保证传输的可靠性的。<strong>当某个流发生丢包时，只会阻塞这个流，其他流不会受到影响，因此不存在队头阻塞问题</strong>。这与 HTTP&#x2F;2 不同，HTTP&#x2F;2 只要某个流中的数据包丢失了，其他流也会因此受影响。</p>
<p>所以，QUIC 连接上的多个 Stream 之间并没有依赖，都是独立的，某个流发生丢包了，只会影响该流，其他流不受影响。</p>
<p><em>2、更快的连接建立</em></p>
<p>对于 HTTP&#x2F;1 和 HTTP&#x2F;2 协议，TCP 和 TLS 是分层的，分别属于内核实现的传输层、openssl 库实现的表示层，因此它们难以合并在一起，需要分批次来握手，先 TCP 握手，再 TLS 握手。</p>
<p>HTTP&#x2F;3 在传输数据前虽然需要 QUIC 协议握手，这个握手过程只需要 1 RTT，握手的目的是为确认双方的「连接 ID」，连接迁移就是基于连接 ID 实现的。</p>
<p>但是 HTTP&#x2F;3 的 QUIC 协议并不是与 TLS 分层，而是QUIC 内部包含了 TLS，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 TLS&#x2F;1.3，因此仅需三次握手即 1 个 RTT 就可以「同时」完成建立连接与密钥协商</p>
<p><em>3、连接迁移</em></p>
<p>基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接，那么<strong>当移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立连接</strong>。而建立连接的过程包含 TCP 三次握手和 TLS 四次握手的时延，以及 TCP 慢启动的减速过程，给用户的感觉就是网络突然卡顿了一下，因此连接的迁移成本是很高的。</p>
<p>而 QUIC 协议没有用四元组的方式来“绑定”连接，而是通过<strong>连接 ID</strong>来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本，没有丝毫卡顿感，达到了<strong>连接迁移</strong>的功能。</p>
<h3 id="HTTPS"><a href="#HTTPS" class="headerlink" title="HTTPS"></a>HTTPS</h3><h4 id="HTTP-与-HTTPS-有哪些区别？"><a href="#HTTP-与-HTTPS-有哪些区别？" class="headerlink" title="HTTP 与 HTTPS 有哪些区别？"></a>HTTP 与 HTTPS 有哪些区别？</h4><ol>
<li>HTTP 是超文本传输协议，信息是明文传输，存在安全风险的问题。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL&#x2F;TLS 安全协议，使得报文能够加密传输。</li>
<li>HTTP 连接建立相对简单， TCP 三次握手之后便可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之后，还需进行 SSL&#x2F;TLS 的握手过程，才可进入加密报文传输。</li>
<li>HTTP 的端口号是 80，HTTPS 的端口号是 443。</li>
<li>HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。</li>
</ol>
<p>HTTP 由于是明文传输，所以安全上存在以下三个风险：</p>
<ul>
<li><strong>窃听风险</strong>，比如通信链路上可以获取通信内容</li>
<li><strong>篡改风险</strong>，比如强制植入垃圾广告，视觉污染</li>
<li><strong>冒充风险</strong>，比如冒充淘宝网站</li>
</ul>
<p>HTTPS 在 HTTP 与 TCP 层之间加入了 <code>SSL/TLS</code>（TLS是SSL的更新版本，SSL因为安全问题已被弃用） 协议，可以很好的解决了上述的风险</p>
<ul>
<li><strong>混合加密</strong>的方式实现信息的<strong>机密性</strong>，解决了窃听的风险。</li>
<li><strong>摘要算法</strong>的方式来实现<strong>完整性</strong>，它能够为数据生成独一无二的「指纹」，指纹用于校验数据的完整性，解决了篡改的风险。</li>
<li>将服务器公钥放入到<strong>数字证书</strong>中，解决了冒充的风险。</li>
</ul>
<p><em>1. 混合加密</em></p>
<p>通过<strong>混合加密</strong>的方式可以保证信息的<strong>机密性</strong>，解决了窃听的风险。</p>
<p>HTTPS 采用的是<strong>对称加密</strong>和<strong>非对称加密</strong>结合的「混合加密」方式：</p>
<ul>
<li>在通信建立前采用<strong>非对称加密</strong>的方式交换「会话秘钥」，后续就不再使用非对称加密。</li>
<li>在通信过程中全部使用<strong>对称加密</strong>的「会话秘钥」的方式加密明文数据。</li>
</ul>
<p>采用「混合加密」的方式的原因：</p>
<ul>
<li><strong>对称加密</strong>只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。</li>
<li><strong>非对称加密</strong>使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。</li>
</ul>
<p><em>2. 摘要算法 + 数字签名</em></p>
<p>为了保证传输的内容不被篡改，我们需要对内容计算出一个「哈希值」，然后同内容一起传输给对方。</p>
<p>对方收到后，先是对内容也计算出一个「哈希值」，然后跟发送方发送的「哈希值」做一个比较，如果「哈希值」相同，说明内容没有被篡改，否则就可以判断出内容被篡改了。</p>
<p>但这方法<strong>仍有缺陷</strong>，因为哈希值可以确保内容不被篡改，<strong>但是并不能保证「内容 + 哈希值」不会被中间人篡改，</strong>因为这里缺少对客户端收到的消息是否来源于服务端的证明。</p>
<p>那为了避免这种情况，计算机里会用<strong>非对称加密算法</strong>来解决，共有两个密钥：</p>
<ul>
<li>一个是公钥，这个是可以公开给所有人的；</li>
<li>一个是私钥，这个必须由本人管理，不可泄露。</li>
</ul>
<p>这两个密钥可以<strong>双向加解密</strong>的，比如可以用公钥加密内容，然后用私钥解密，也可以用私钥加密内容，公钥解密内容。</p>
<p>流程的不同，意味着目的也不相同：</p>
<ul>
<li><strong>公钥加密，私钥解密</strong>。这个目的是为了<strong>保证内容传输的安全</strong>，因为被公钥加密的内容，其他人是无法解密的，只有持有私钥的人，才能解密出实际的内容；</li>
<li><strong>私钥加密，公钥解密</strong>。这个目的是为了<strong>保证消息不会被冒充</strong>，因为私钥是不可泄露的，如果公钥能正常解密出私钥加密的内容，就能证明这个消息是来源于持有私钥身份的人发送的。</li>
</ul>
<p>所以非对称加密的用途主要在于<strong>通过「私钥加密，公钥解密」的方式，来确认消息的身份</strong>，我们常说的<strong>数字签名算法</strong>，就是用的是这种方式，不过私钥加密内容不是内容本身，而是<strong>对内容的哈希值加密</strong>。</p>
<p><em>3. 数字证书</em></p>
<p>前面可知</p>
<ul>
<li>可以通过哈希算法来保证消息的完整性；</li>
<li>可以通过数字签名来保证消息的来源可靠性（能确认消息是由持有私钥的一方发送的）；</li>
</ul>
<p>但<strong>仍有缺陷</strong>，<strong>缺少身份验证的环节</strong>，如中间人在客户端服务器交换公钥的时候可以伪造公钥，从而达到冒充服务器的目的。</p>
<p>解决办法是服务器提前将公钥注册到CA（数字证书认证机构），CA会颁发数字证书，只要证书是可信的，公钥就是可信的。</p>
<p>具体而言，服务器提前将公钥注册到CA，CA会把持有者的公钥、用途、颁发者、有效时间等信息打成一个包并算得一个哈希值，这个哈希值用CA的私钥加密得到一份数字签名，将签名和之前的信息打包形成数字证书颁发给服务器，客户端和服务器通信时，服务器就将这份数字证书发送给客户端，客户端通过CA的公钥（已事先放在浏览器或操作系统中）解密就得到了服务器的公钥。</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/%E6%95%B0%E5%AD%97%E8%AF%81%E4%B9%A6%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png"></p>
<h4 id="HTTPS建立连接过程"><a href="#HTTPS建立连接过程" class="headerlink" title="HTTPS建立连接过程"></a>HTTPS建立连接过程</h4><p>SSL&#x2F;TLS 协议基本流程：</p>
<ul>
<li>客户端向服务器索要并验证服务器的公钥。</li>
<li>双方协商生产「会话秘钥」。</li>
<li>双方采用「会话秘钥」进行加密通信。</li>
</ul>
<p>SSL&#x2F;TLS 的「握手阶段」涉及<strong>四次</strong>通信</p>
<p>SSL&#x2F;TLS 协议建立的详细流程：</p>
<p><em>1. ClientHello</em></p>
<p>首先，由客户端向服务器发起加密通信请求，也就是 <code>ClientHello</code> 请求。</p>
<p>在这一步，客户端主要向服务器发送以下信息：</p>
<p>（1）客户端支持的 SSL&#x2F;TLS 协议版本，如 TLS 1.2 版本。</p>
<p>（2）客户端生产的随机数（<code>Client Random</code>），后面用于生成「会话秘钥」条件之一。</p>
<p>（3）客户端支持的密码套件列表，如 RSA 加密算法。</p>
<p><em>2. SeverHello</em></p>
<p>服务器收到客户端请求后，向客户端发出响应，也就是 <code>SeverHello</code>。服务器回应的内容有如下内容：</p>
<p>（1）确认 SSL&#x2F; TLS 协议版本，如果浏览器不支持，则关闭加密通信。</p>
<p>（2）服务器生产的随机数（<code>Server Random</code>），也是后面用于生产「会话秘钥」条件之一。</p>
<p>（3）确认的密码套件列表，如 RSA 加密算法。</p>
<p>（4）服务器的数字证书。</p>
<p><em>3.客户端回应</em></p>
<p>客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。</p>
<p>如果证书没有问题，客户端会<strong>从数字证书中取出服务器的公钥</strong>，然后使用它加密报文，向服务器发送如下信息：</p>
<p>（1）一个随机数（<code>pre-master key</code>）。该随机数会被服务器公钥加密。</p>
<p>（2）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。</p>
<p>（3）客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供服务端校验。</p>
<p>上面第一项的随机数是整个握手阶段的第三个随机数，会发给服务端，所以这个随机数客户端和服务端都是一样的。</p>
<p><strong>服务器和客户端有了这三个随机数（Client Random、Server Random、pre-master key），接着就用双方协商的加密算法，各自生成本次通信的「会话秘钥」</strong>。</p>
<p><em>4. 服务器的最后回应</em></p>
<p>服务器收到客户端的第三个随机数（<code>pre-master key</code>）之后，通过协商的加密算法，计算出本次通信的「会话秘钥」。</p>
<p>然后，向客户端发送最后的信息：</p>
<p>（1）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。</p>
<p>（2）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供客户端校验。</p>
<p>至此，整个 SSL&#x2F;TLS 的握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的 HTTP 协议，只不过用「会话秘钥」加密内容。</p>
<h4 id="RSA密钥交换算法缺陷"><a href="#RSA密钥交换算法缺陷" class="headerlink" title="RSA密钥交换算法缺陷"></a>RSA密钥交换算法缺陷</h4><p><strong>使用 RSA 密钥协商算法的最大问题是不支持前向保密</strong>。</p>
<p>因为客户端传递随机数（用于生成对称加密密钥的条件之一）给服务端时使用的是公钥加密的，服务端收到到后，会用私钥解密得到随机数。所以一旦服务端的私钥泄漏了，<strong>过去被第三方截获的所有 TLS 通讯密文都会被破解</strong>。</p>
<p>为了解决这个问题，后面就出现了 ECDHE 密钥协商算法，我们现在大多数网站使用的正是 ECDHE 密钥协商算法</p>
<h4 id="ECDHE-握手解析"><a href="#ECDHE-握手解析" class="headerlink" title="ECDHE 握手解析"></a>ECDHE 握手解析</h4><p>ECDHE是基于离散对数的，并且它的私钥每次通信都是随机生成的，这就保证了前向安全，即使某次通信的私钥泄密也不会导致之前的报文都被解密。</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/%E7%A6%BB%E6%95%A3%E5%AF%B9%E6%95%B0.png" alt="离散对数"></p>
<p>客户端和服务器使用 ECDHE 密钥交换算法的过程：</p>
<ul>
<li>双方事先确定好使用哪种椭圆曲线，和曲线上的基点 G，这两个参数都是公开的；</li>
<li>双方各自随机生成一个随机数作为<strong>私钥d</strong>，并与基点 G相乘得到<strong>公钥Q</strong>（Q &#x3D; dG），此时客户端的公私钥为 Q1 和 d1，服务器的公私钥为 Q2 和 d2；</li>
<li>双方加密交换各自的公钥（通过CA证书中服务器的公钥加密），最后客户端计算点（x1，y1） &#x3D; d1Q2，服务器计算点（x2，y2） &#x3D; d2Q1，由于椭圆曲线上是可以满足乘法交换和结合律，所以 d1Q2 &#x3D; d1d2G &#x3D; d2d1G &#x3D; d2Q1 ，因此<strong>双方的 x 坐标是一样的，所以它是共享密钥，也就是会话密钥</strong>。</li>
</ul>
<p>对于ECDHE算法需要破解两次私钥（服务器私钥 + 椭圆曲线私钥），且椭圆曲线私钥每次握手都会随机生成，保证了向前安全性。</p>
<h3 id="解决http服务器端推送消息问题"><a href="#解决http服务器端推送消息问题" class="headerlink" title="解决http服务器端推送消息问题"></a>解决http服务器端推送消息问题</h3><h4 id="meta标签"><a href="#meta标签" class="headerlink" title="meta标签"></a>meta标签</h4><p>在 Web早期，通过配置meta标签让浏览器自动刷新，从而实现服务器端的推送</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">META</span> <span class="attr">HTTP-RQUIV</span>=<span class="string">&quot;Refresh&quot;</span> <span class="attr">CONTENT</span>=<span class="string">12</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>优点：使用方式简单，可以在JS禁用情况下使用<br>缺点：不是实时更新数据，对服务器造成的压力大，带宽浪费多。并且每次都会更新整个页面</p>
<h4 id="AJAX轮询"><a href="#AJAX轮询" class="headerlink" title="AJAX轮询"></a>AJAX轮询</h4><p>Ajax(Asynchronous JavaScript and XML)，即异步 JavaScript 和 XML。</p>
<p>它可以通过和服务器的交互实现网页中部分信息的更新，这就使得我们在只需要进行少数数据更新的情况下，避免整个网页数据的再次加载，提升了开发性能。</p>
<p>举个例子就是说；现在你要注册一个网站，成为正式用户，然后有一大堆的信息录入表等你填写，这个时候你已经填了好多内容了，有个地方需要你输入验证码，然后呢，你表示看不清楚想要换一张验证码，这个时候，如果用普通的数据刷新请求方式去处理获取新的验证码的话，随之而来的问题就是，验证码虽然刷新了，但是你之前好不容易填写的一堆个人信息也被刷新了，没了，是不是很无语。</p>
<p>这就是Web的运作原理：一次HTTP请求对应一个页面。</p>
<p>如果要让用户留在当前页面中，同时发出新的HTTP请求，就必须用JavaScript发送这个新请求，接收到数据后，再用JavaScript更新页面，这样一来，用户就感觉自己仍然停留在当前页面，但是数据却可以不断地更新。所以，这种情况下，我们就需要使用Ajax这种部分内容刷新的请求方式去进行数据请求，从而达到更加高效快捷的更新网页内容的效果。</p>
<p>AJAX轮询即定时的通过Ajax查询服务器端，客户端定时向服务器端发送ajax请求，服务器端接收到请求后马上响应信息并关闭连接。要求两次请求间隔时间必须尽可能的小，但若时间间隔减小，客户端浏览器在相同时间内就会发出更多的请求，这些请求中大部分都不会返回有用的数据，这会白白地浪费掉带宽和处理资源。。</p>
<p>当我们利用AJAX实现服务器推送时，其实质是客户端不停地向服务器询问”有没有给我的消息呀？”，然后服务器回答”有”或”没有”来达到的实现效果。</p>
<p><strong>优点:</strong><br>服务端逻辑简单，编码实现简单。<br><strong>缺点:</strong><br>这是通过模拟服务器发起的通信，<strong>不是实时通信</strong>，不顾及应用的状态改变而盲目检查更新，导致服务器资源的浪费，且会加重网络负载，拖累服务器。其中大多数请求可能是无效请求，在大量用户轮询很频繁的情况下对服务器的压力很大。</p>
<h4 id="comet"><a href="#comet" class="headerlink" title="comet"></a>comet</h4><p><strong>Comet</strong>是一种用于web的推送技术，能使服务器实时地将更新的信息传送到客户端，而无须客户端发出请求，目前有两种实现方式，长轮询和iframe流。</p>
<p> <strong>长轮询</strong></p>
<p>长轮询是在打开一条连接以后保持，等待服务器推送来数据再关闭的方式。</p>
<p><strong>iframe流</strong></p>
<p>iframe流方式是在页面中插入一个隐藏的iframe，利用其src属性在服务器和客户端之间建立一条长链接，服务器向iframe传输数据（通常是HTML，内有负责插入信息的javascript），来实时更新页面。 iframe流方式的优点是浏览器兼容好，Google公司在一些产品中使用了iframe流，如Google Talk。</p>
<h4 id="Server-Sent-Events"><a href="#Server-Sent-Events" class="headerlink" title="Server-Sent Events"></a>Server-Sent Events</h4><p>Server-Sent Events 是一种服务器推送技术, 使客户端可以通过 <strong>HTTP 连接</strong>从服务器自动接收更新. 每个通知以<strong>文本流(文本应该为 utf-8)\的形式发送, 并以一对换行符</strong>结尾. 与 WebSocket 相比:</p>
<ol>
<li>它不是全双工的, 只能服务器向浏览器发送, 因为流信息本质上就是下载, 一旦连接后不能再次发送请求(否则就变成了一次新的连接)</li>
<li>WebSocket 使用的 ws 协议, 而 SSE 使用的仍然是 HTTP 协议.</li>
</ol>
<h4 id="websocket"><a href="#websocket" class="headerlink" title="websocket"></a>websocket</h4><p>Comet和SSE仍然是单向通信（服务器向客户端），不能适应Web应用的飞速发展。于是，各种新技术不断涌现，其中WebSocket在Google的力推之下已经成为业界标准，并不断完善中。其基于TCP之上定义了帧协议，支持双向的通信。</p>
<p>WebSocket是一种全新的协议，不属于http无状态协议，协议名为ws协议或wss协议。ws不是http，所以传统的web服务器不一定支持。</p>
<h2 id="TCP"><a href="#TCP" class="headerlink" title="TCP"></a>TCP</h2><h3 id="tcp首部"><a href="#tcp首部" class="headerlink" title="tcp首部"></a>tcp首部</h3><p><img src="https://raw.githubusercontent.com/hufei96/Image/main/tcphead.png" alt="TCP 头格式"></p>
<p>源端口和目的端口：端口范围为0~65535，该端口是传输层与应用层的服务端口，传输层的复用和分用都要通过端口来实现。</p>
<p>序列号：TCP是面向字节流的，所以TCP传送的时候每一个字节都编上一个序号，发送报文段的时候，序列号的值为该报文段第一个字节的序号。</p>
<p>确认号：是期待接收方下一个报文段的序号（即下一个报文段的第一个字节的序号）。若确认号为N，则表示期待下一个报文段发送序号为N开始的字节流过来（因为TCP是有序的，所以实际上现在0~N-1都已经接收到，所以期待发送N开始的字节流过来）。</p>
<p>首部长度：记录的是当前TCP报文段首部长度。该字段以4字节为单位，实际值范围5-15，表示TCP首部长度为20字节~60字节。</p>
<p>保留字段：在设计TCP首部时，保留为以后所用，但直到现在仍未用到，置为0.</p>
<p>URG紧急位：URG&#x3D;1表示紧急指针有效，表示该报文段中有紧急数据，应优先传送该报文段。<strong>URG需要和紧急指针配套使用，URG紧急位只是指示是否紧急，紧急指针指示实际的紧急数据大小</strong>（实际紧急数据是从第一个字节到紧急指针所指字节）。</p>
<p>ACK确认位：ACK&#x3D;1表示确认号字段有效，ACK&#x3D;0表示确认号字段无效。</p>
<blockquote>
<p>TCP规定，连接建立后所有报文段ACK确认位置为1。所以，<strong>可以通过ACK&#x3D;0&#x2F;1来判断当前是否已建立起TCP连接。</strong></p>
</blockquote>
<p>PSH推送位：PSH&#x3D;1表示某端接收到TCP报文段之后，应尽快交付给应用层，而不是等到接收缓存满了后再向上交付</p>
<p>RST复位位：RST&#x3D;1表示TCP连接中出现严重差错，必须释放当前连接，然后重新建立起连接。<strong>用RST终止连接只需要2次挥手</strong></p>
<p>SYN同步位：SYN&#x3D;1表示这是一个连接请求或连接接收报文。</p>
<blockquote>
<p>两者关系：某端发送连接请求报文,SYN&#x3D;1,ACK&#x3D;0，为第一次握手，<br>若对方同意建立连接，则发送响应报文(连接接收报文),SYN&#x3D;1,ACK&#x3D;1，为第二次握手。</p>
</blockquote>
<p>FIN终止位：FIN&#x3D;1表示此报文段的发送方的数据已发送完毕，并要求释放传输连接。</p>
<p>窗口字段：根据自己接收缓存（即接收窗口）剩余空间指示另一端可以发送的数据量，单位为字节。如：确认号为301，窗口字段为1000，则表示，从301号开始，发送此报文段的一端告诉另一端自己还可以接收1000个字节数据，且期待从301开始，即期待字节序号为301-1300。</p>
<p>检验和：TCP的检验同时检验首部和数据部分，在报文段前面加12个字节伪首部，用于检验之用。</p>
<blockquote>
<p>关于IP、IMCP、TCP和UDP的检验：<br>IP校验和只校验20字节的IP报头；而ICMP校验和覆盖整个报文（ICMP报头+ICMP数据）；UDP和TCP校验和不仅覆盖整个报文，而且还有12字节的IP伪首部，包括源IP地址(4字节)、目的IP地址(4字节)、协议(2字节，第一字节补0)和TCP&#x2F;UDP包长(2字节，包含协议头和数据)。</p>
</blockquote>
<p>紧急指针：指示出本报文段的紧急数据的范围，即紧急数据有多少个字节。</p>
<blockquote>
<p>紧急数据放在TCP报文段数据部分的最前面，紧急指针为16位，范围为0-65535，可以表示前0-66535个字节是紧急数据。</p>
</blockquote>
<p>选项字段：长度可变。功能：TCP最初只规定了一种选项，即最大报文段长度MSS,MSS表示TCP报文段中数据字段的最大长度。</p>
<p>填充字段：长度可变。功能：无意义，纯粹填充使TCP首部为4N个字节。</p>
<h3 id="tcp基本描述"><a href="#tcp基本描述" class="headerlink" title="tcp基本描述"></a>tcp基本描述</h3><p><em><strong>为什么需要 TCP 协议</strong></em></p>
<p><code>IP</code> 层是「不可靠」的，它不保证网络包的交付、不保证网络包的按序交付、也不保证网络包中的数据的完整性。如果需要保障网络数据包的可靠性，那么就需要由上层（传输层）的 <code>TCP</code> 协议来负责。</p>
<p><em><strong>tcp定义</strong></em></p>
<p>TCP 是<strong>面向连接的、可靠的、基于字节流</strong>的传输层通信协议。</p>
<ul>
<li><strong>面向连接</strong>：一定是「一对一」才能连接，不能像 UDP 协议可以一个主机同时向多个主机发送消息，也就是一对多是无法做到的；</li>
<li><strong>可靠的</strong>：无论的网络链路中出现了怎样的链路变化，通过TCP 连接传输的数据，无差错、不丢失、不重复、并且按序到达接收端</li>
<li><strong>字节流</strong>：用户消息通过 TCP 协议传输时，消息可能会被操作系统「分组」成多个的 TCP 报文，如果接收方的程序如果不知道「消息的边界」，是无法读出一个有效的用户消息的，因为用户消息被拆分成多个 TCP 报文后，并不能像 UDP 那样，一个 UDP 报文就能代表一个完整的用户消息。假设发送端陆续调用 send 函数先后发送 「hi.」和「how are you」 报文，那么实际的发送很有可能是这几种情况。1：hi how are you一起发送 2：先发hi how，再发are you 3: 先发h，再发i how are you</li>
</ul>
<p><em><strong>什么是tcp连接</strong></em></p>
<p>用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括Socket、序列号和窗口大小称为连接。</p>
<p>所以我们可以知道，建立一个 TCP 连接是需要客户端与服务器端达成上述三个信息的共识。</p>
<ul>
<li><strong>Socket</strong>：由 IP 地址和端口号组成</li>
<li><strong>序列号</strong>：用来解决乱序问题等</li>
<li><strong>窗口大小</strong>：用来做流量控制</li>
</ul>
<p>TCP 四元组可以唯一的确定一个连接，四元组包括源地址，源端口，目的地址，目的端口</p>
<p><em><strong>有一个 IP 的服务器监听了一个端口，它的 TCP 的最大连接数是多少？</strong></em></p>
<p>连接由四元组确定，只要四个中有一个不同就是不同的tcp连接，固定ip固定端口的服务器目的地址和目的端口是确定的，所以连接数取决于客户端ip和端口。</p>
<p>最大TCP连接数&#x3D;客户端ip数x客户端端口数</p>
<p>对 IPv4，客户端的 IP 数最多为 <code>2</code> 的 <code>32</code> 次方，客户端的端口数最多为 <code>2</code> 的 <code>16</code> 次方，也就是服务端单机最大 TCP 连接数，约为 <code>2</code> 的 <code>48</code> 次方。</p>
<p>当然，服务端最大并发 TCP 连接数远不能达到理论上限，会受以下因素影响：</p>
<ul>
<li><p>文件描述符限制</p>
<p>每个 TCP 连接都是一个文件，如果文件描述符被占满了，会发生 too many open files。Linux 对可打开的文件描述符的数量分别作了三个方面的限制：</p>
<ul>
<li><strong>系统级</strong>：当前系统可打开的最大数量，通过 cat &#x2F;proc&#x2F;sys&#x2F;fs&#x2F;file-max 查看；</li>
<li><strong>用户级</strong>：指定用户可打开的最大数量，通过 cat &#x2F;etc&#x2F;security&#x2F;limits.conf 查看；</li>
<li><strong>进程级</strong>：单个进程可打开的最大数量，通过 cat &#x2F;proc&#x2F;sys&#x2F;fs&#x2F;nr_open 查看；</li>
</ul>
</li>
<li><p><strong>内存限制</strong>，每个 TCP 连接都要占用一定内存，操作系统的内存是有限的，如果内存资源被占满后，会发生 OOM。</p>
</li>
</ul>
<h3 id="tcp建立断开连接"><a href="#tcp建立断开连接" class="headerlink" title="tcp建立断开连接"></a>tcp建立断开连接</h3><p>一开始，客户端和服务端都处于 <code>CLOSED</code> 状态。先是服务端主动监听某个端口，处于 <code>LISTEN</code> 状态</p>
<p>客户端会随机初始化序号（<code>client_isn</code>），将此序号置于 TCP 首部的「序号」字段中，同时把 <code>SYN</code> 标志位置为 <code>1</code> ，表示 <code>SYN</code> 报文。接着把第一个 SYN 报文发送给服务端，表示向服务端发起连接，该报文不包含应用层数据，之后客户端处于 <code>SYN-SENT</code> 状态。</p>
<p>服务端收到客户端的 <code>SYN</code> 报文后，首先服务端也随机初始化自己的序号（<code>server_isn</code>），将此序号填入 TCP 首部的「序号」字段中，其次把 TCP 首部的「确认应答号」字段填入 <code>client_isn + 1</code>, 接着把 <code>SYN</code> 和 <code>ACK</code> 标志位置为 <code>1</code>。最后把该报文发给客户端，该报文也不包含应用层数据，之后服务端处于 <code>SYN-RCVD</code> 状态。</p>
<p>客户端收到服务端报文后，还要向服务端回应最后一个应答报文，首先该应答报文 TCP 首部 <code>ACK</code> 标志位置为 <code>1</code> ，其次「确认应答号」字段填入 <code>server_isn + 1</code> ，最后把报文发送给服务端，这次报文可以携带客户到服务器的数据，之后客户端处于 <code>ESTABLISHED</code> 状态。</p>
<p>服务器收到客户端的应答报文后，也进入 <code>ESTABLISHED</code> 状态。</p>
<p><strong>TCP 的连接状态查看</strong>，在 Linux 可以通过 <code>netstat -napt</code> 命令查看。</p>
<h4 id="为什么是三次握手？不是两次、四次？"><a href="#为什么是三次握手？不是两次、四次？" class="headerlink" title="为什么是三次握手？不是两次、四次？"></a><em><strong>为什么是三次握手？不是两次、四次？</strong></em></h4><p>三次握手的原因：</p>
<ul>
<li>三次握手才可以阻止历史连接的初始化（主要原因）</li>
<li>三次握手才可以同步双方的初始序列号</li>
</ul>
<p><em>原因一：避免历史连接</em></p>
<p>假设在<strong>网络拥堵</strong>情况下，客户端连续发送多次 SYN 建立连接的报文，此时会有</p>
<ul>
<li>一个「旧 SYN 报文」比「最新的 SYN 」 报文早到达了服务端；</li>
<li>那么此时服务端就会回一个 <code>SYN + ACK</code> 报文给客户端；</li>
<li>客户端收到后根据自身的上下文，判断这是一个历史连接（序列号过期或超时），那么客户端就会发送 <code>RST</code> 报文给服务端，表示中止这一次连接。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/tcpthreehand.png" alt="三次握手避免历史连接"></p>
<p><strong>如果是两次握手连接，就无法阻止历史连接</strong></p>
<p>两次握手的情况下，「被动方」在收到 SYN 报文后，即第一次握手之后，就进入 ESTABLISHED 状态，意味着这时可以给对方发送数据。被动方并不知道这个是历史连接，导致「被动方」建立了一个历史连接，并白白给发送方发送了数据，浪费了「被动方」的资源。</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/tcpthreehand2.png" alt="两次握手无法阻止历史连接"></p>
<p><em>原因二：同步双方初始序列号</em></p>
<p>TCP 协议的通信双方， 都必须维护一个「序列号」， 序列号是可靠传输的一个关键因素。</p>
<ul>
<li>接收方可以去除重复的数据；</li>
<li>接收方可以根据数据包的序列号按序接收；</li>
<li>可以标识发送出去的数据包中， 哪些是已经被对方收到的（通过 ACK 报文中的序列号知道）；</li>
</ul>
<p>并且每次连接序列号都是不同的，所以要先同步序列号(序列号不能每次都从0开始，具体看后续笔记)</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/threehand3.png" alt="四次握手与三次握手"></p>
<p>当客户端发送携带「初始序列号」的 <code>SYN</code> 报文的时候，需要服务端回一个 <code>ACK</code> 应答报文，表示客户端的 SYN 报文已被服务端成功接收，那当服务端发送「初始序列号」给客户端的时候，依然也要得到客户端的应答回应，<strong>这样一来一回，才能确保双方的初始序列号能被可靠的同步。</strong> <strong>如果不先同步序列号而直接传输报文，连接复用时无法分辨出序列号是延迟或者是旧连接的序列号，因此需要三次握手来约定确定双方的 ISN（初始 seq 序列号）。</strong></p>
<p><strong>而两次握手只保证了一方的初始序列号能被对方成功接收</strong>，没办法保证双方的初始序列号都能被确认接收。</p>
<p><strong>总结：</strong></p>
<p>TCP 建立连接时，通过三次握手<strong>能防止历史连接的建立，能减少双方不必要的资源开销，能帮助双方同步初始化序列号</strong>。序列号能够保证数据包不重复、不丢弃和按序传输。</p>
<p>不使用「两次握手」和「四次握手」的原因：</p>
<ul>
<li>「两次握手」：无法防止历史连接的建立，会造成双方资源的浪费，也无法可靠的同步双方序列号；</li>
<li>「四次握手」：三次握手就已经理论上最少可靠连接建立，所以不需要使用更多的通信次数。</li>
</ul>
<h4 id="tcp如何规避三次握手"><a href="#tcp如何规避三次握手" class="headerlink" title="tcp如何规避三次握手"></a><em><strong>tcp如何规避三次握手</strong></em></h4><p>为了减少 TCP 连接建立的时延，在 Linux 3.7 内核版本之后，提供了 TCP Fast Open 功能，它使得双方不需要握手就可以直接传输数据，但我认为这可能导致错误接收历史报文(有待查证)。<strong>要开启这个功能可以通过设置 tcp_fastopn 内核参数</strong></p>
<p>tcp_fastopn 各个值的意义:</p>
<ul>
<li>0 关闭</li>
<li>1 作为客户端使用 Fast Open 功能</li>
<li>2 作为服务端使用 Fast Open 功能</li>
<li>3 无论作为客户端还是服务器，都可以使用 Fast Open 功能</li>
</ul>
<p><strong>TCP Fast Open 功能需要客户端和服务端同时支持，才有效果。</strong></p>
<p>TCP Fast Open 功能的工作方式：</p>
<p>在客户端首次建立连接时的过程：</p>
<ol>
<li>客户端发送 SYN 报文，该报文<strong>tcp首部包含 Fast Open 选项</strong>，且该选项的 Cookie 为空，这表明客户端请求 Fast Open Cookie；</li>
<li>支持 TCP Fast Open 的服务器生成 Cookie(cookie包括客户端ip地址的加密)，并将其置于 SYN-ACK 数据包中的 Fast Open 选项以发回客户端；</li>
<li>客户端收到 SYN-ACK 后，本地缓存 Fast Open 选项中的 Cookie。</li>
</ol>
<p>所以，<strong>第一次发起 HTTP GET 请求的时候，还是需要正常的三次握手流程。</strong></p>
<p>之后，如果客户端再次向服务器建立连接时的过程：</p>
<ol>
<li>客户端发送 SYN 报文，该报文包含「数据」以及此前记录的 Cookie；</li>
<li>支持 TCP Fast Open 的服务器会对收到 Cookie 进行校验：如果 Cookie 有效，服务器将在 SYN-ACK 报文中对 SYN 和「数据」进行确认，服务器随后将「数据」递送至相应的应用程序；如果 Cookie 无效，服务器将丢弃 SYN 报文中包含的「数据」，且其随后发出的 SYN-ACK 报文将只确认 SYN 的对应序列号；</li>
<li>如果服务器接受了 SYN 报文中的「数据」，服务器可在握手完成之前发送「数据」，<strong>这就减少了握手带来的 1 个 RTT 的时间消耗</strong>；</li>
<li>客户端将发送 ACK 确认服务器发回的 SYN 以及「数据」；但如果客户端在初始的 SYN 报文中发送的「数据」没有被确认，则客户端将重新发送「数据」，这与正常的三次握手一样；</li>
<li>此后的 TCP 连接的数据传输过程和非 TFO 的正常情况一致。</li>
</ol>
<h4 id="为什么每次建立-TCP-连接时，初始化的序列号都要求不一样呢？"><a href="#为什么每次建立-TCP-连接时，初始化的序列号都要求不一样呢？" class="headerlink" title="为什么每次建立 TCP 连接时，初始化的序列号都要求不一样呢？"></a><em>为什么每次建立 TCP 连接时，初始化的序列号都要求不一样呢？</em></h4><p>主要原因有两个方面：</p>
<ul>
<li>为了防止历史报文被下一个相同四元组的连接接收（主要方面）；</li>
<li>为了安全性，防止黑客伪造的相同序列号的 TCP 报文被对方接收(这就是序列号预测攻击，但仍无法阻止黑客截获报文得到序列号，tcp首部由内核生成，是明文传输的，程序员只能在应用层加密，无法加密tcp首部)；</li>
</ul>
<p>假设每次建立连接，客户端和服务端的初始化序列号都是从 0 开始：</p>
<p>过程如下：</p>
<ul>
<li>客户端和服务端建立一个 TCP 连接，在客户端发送数据包被网络阻塞了，而此时服务端的进程重启了，于是就会发送 RST 报文来断开连接。</li>
<li>紧接着，客户端又与服务端建立了与上一个连接相同四元组的连接；</li>
<li>在新连接建立完成后，上一个连接中被网络阻塞的数据包正好抵达了服务端，刚好该数据包的序列号正好是在服务端的接收窗口内，所以该数据包会被服务端正常接收，就会造成数据错乱。</li>
</ul>
<p>可以看到，<strong>如果每次建立连接，客户端和服务端的初始化序列号都是一样的话，很容易出现历史报文被下一个相同四元组的连接接收的问题</strong>。</p>
<p>但是tcp序号是会循环的，所以即使初始化序号随机，仍有可能会发生接收历史报文的情况，</p>
<p>为了解决这个问题，就需要有 TCP 时间戳。tcp_timestamps 参数是默认开启的，开启了 tcp_timestamps 参数，TCP 头部就会使用时间戳选项，它有两个好处，<strong>一个是便于精确计算 RTT ，另一个是能防止序列号回绕（PAWS）</strong>。</p>
<p>防回绕序列号算法要求连接双方维护最近一次收到的数据包的时间戳（Recent TSval），每收到一个新数据包都会读取数据包中的时间戳值跟 Recent TSval 值做比较，<strong>如果发现收到的数据包中时间戳不是递增的，则表示该数据包是过期的，就会直接丢弃这个数据包</strong>。</p>
<h4 id="为什么ip层和tcp都需要分片"><a href="#为什么ip层和tcp都需要分片" class="headerlink" title="为什么ip层和tcp都需要分片"></a><em>为什么ip层和tcp都需要分片</em></h4><p>我们先来认识下 MTU 和 MSS</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/mtuandmss.png" alt="MTU 与 MSS"></p>
<ul>
<li><code>MTU</code>：<strong>Maximum Transmit Unit</strong>，最大传输单元，即物理接口（数据链路层）提供给其上层最大一次传输数据的大小，比如IP层、MPLS层等等，因为目前应用最多的接口是以太网，所以谈谈以太网口的MTU，假定其上层协议是IP，缺省MTU&#x3D;1500，意思是：整个IP包最大从这个接口发送出去的是1500个字节。可以通过配置修改成更大或更小的值，只要在系统的边界值以内即可，但是切记要在链路的两端都要修改，而且要大小一样，如果不一样，会造成大侧的数据被小侧丢弃！</li>
<li><code>MSS</code>：<strong>Maximum Segment Size</strong> ，最大TCP分段大小，不包含TCP头和 option，只包含TCP Payload ，TCP用来限制自己每次发送的最大分段尺寸。</li>
</ul>
<p>如果在 TCP 的整个报文（头部 + 数据）交给 IP 层进行分片，会有什么异常呢？</p>
<p>当 IP 层有一个超过 <code>MTU</code> 大小的数据（TCP 头部 + TCP 数据）要发送，那么 IP 层就要进行分片，把数据分片成若干片，保证每一个分片都小于 MTU。把一份 IP 数据报进行分片以后，由目标主机的 IP 层来进行重新组装后，再交给上一层 TCP 传输层。</p>
<p>这看起来井然有序，但这存在隐患的，<strong>那么当如果一个 IP 分片丢失，整个 IP 报文的所有分片都得重传</strong>。</p>
<p>因为 IP 层本身没有超时重传机制，它由传输层的 TCP 来负责超时和重传。</p>
<p>当接收方发现 TCP 报文（头部 + 数据）的某一片丢失后，则不会响应 ACK 给对方，那么发送方的 TCP 在超时后，就会重发「整个 TCP 报文（头部 + 数据）」。</p>
<p>因此，可以得知由 IP 层进行分片传输，是非常没有效率的。</p>
<p>所以，为了达到最佳的传输效能 TCP 协议在<strong>建立连接的时候通常要协商双方的 MSS 值</strong>，当 TCP 层发现数据超过 MSS 时，则就先会进行分片，当然由它形成的 IP 包的长度也就不会大于 MTU ，自然也就不用 IP 分片了。</p>
<h4 id="为什么要四次挥手"><a href="#为什么要四次挥手" class="headerlink" title="为什么要四次挥手"></a><em><strong>为什么要四次挥手</strong></em></h4><ul>
<li>客户端打算关闭连接，此时会发送一个 TCP 首部 <code>FIN</code> 标志位被置为 <code>1</code> 的报文，也即 <code>FIN</code> 报文，之后客户端进入 <code>FIN_WAIT_1</code> 状态。</li>
<li>服务端收到该报文后，就向客户端发送 <code>ACK</code> 应答报文，接着服务端进入 <code>CLOSED_WAIT</code> 状态。</li>
<li>客户端收到服务端的 <code>ACK</code> 应答报文后，之后进入 <code>FIN_WAIT_2</code> 状态。</li>
<li>等待服务端处理完数据后，也向客户端发送 <code>FIN</code> 报文，之后服务端进入 <code>LAST_ACK</code> 状态。</li>
<li>客户端收到服务端的 <code>FIN</code> 报文后，回一个 <code>ACK</code> 应答报文，之后进入 <code>TIME_WAIT</code> 状态</li>
<li>服务器收到了 <code>ACK</code> 应答报文后，就进入了 <code>CLOSED</code> 状态，至此服务端已经完成连接的关闭。</li>
<li>客户端在经过 <code>2MSL</code> 一段时间后，自动进入 <code>CLOSED</code> 状态，至此客户端也完成连接的关闭。</li>
</ul>
<p>你可以看到，每个方向都需要<strong>一个 FIN 和一个 ACK</strong>，因此通常被称为<strong>四次挥手</strong>。</p>
<p>这里一点需要注意是：<strong>主动关闭连接的，才有 TIME_WAIT 状态。</strong></p>
<p><em><strong>为什么挥手需要四次？</strong></em></p>
<ul>
<li>关闭连接时，客户端向服务端发送 <code>FIN</code> 时，仅仅表示客户端不再发送数据了但是还能接收数据。</li>
<li>服务器收到客户端的 <code>FIN</code> 报文时，先回一个 <code>ACK</code> 应答报文，而服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 <code>FIN</code> 报文给客户端来表示同意现在关闭连接。</li>
</ul>
<p>从上面过程可知，服务端通常需要等待完成数据的发送和处理，所以服务端的 <code>ACK</code> 和 <code>FIN</code> 一般都会分开发送，从而比三次握手导致多了一次，<strong>如果服务器没有要发送的数据时， <code>ACK</code> 和 <code>FIN</code> 会一起发送，此时是三次挥手</strong></p>
<h4 id="为什么需要-TIME-WAIT-状态？"><a href="#为什么需要-TIME-WAIT-状态？" class="headerlink" title="为什么需要 TIME_WAIT 状态？"></a><em>为什么需要 TIME_WAIT 状态？</em></h4><p>主动发起关闭连接的一方，才会有 <code>TIME-WAIT</code> 状态。</p>
<p>需要 TIME-WAIT 状态，主要是两个原因：</p>
<ul>
<li>防止历史连接中的数据，被后面相同四元组的连接错误的接收；</li>
<li>保证「被动关闭连接」的一方，能被正确的关闭；</li>
</ul>
<p><em>原因一：防止历史连接中的数据，被后面相同四元组的连接错误的接收</em></p>
<ul>
<li>服务端在关闭连接之前发送的报文，被网络延迟了。</li>
<li>接着，服务端以相同的四元组重新打开了新连接，前面被延迟的报文这时抵达了客户端，而且该数据报文的序列号刚好在客户端接收窗口内，因此客户端会正常接收这个数据报文，但是这个数据报文是上一个连接残留下来的，这样就产生数据错乱等严重的问题。</li>
</ul>
<p>为了防止历史连接中的数据，被后面相同四元组的连接错误的接收，因此 TCP 设计了 TIME_WAIT 状态，状态会持续 <code>2MSL</code> 时长，这个时间<strong>足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，再出现的数据包一定都是新建立连接所产生的。</strong></p>
<p><em>原因二：保证「被动关闭连接」的一方，能被正确的关闭</em></p>
<p>TIME-WAIT 作用是<strong>等待足够的时间以确保最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭。</strong></p>
<p>如果客户端（主动关闭方）最后一次 ACK 报文（第四次挥手）在网络中丢失了，那么按照 TCP 可靠性原则，服务端（被动关闭方）会重发 FIN 报文。</p>
<p>假设客户端没有 TIME_WAIT 状态，而是在发完最后一次回 ACK 报文就直接进入 CLOSED 状态，如果该 ACK 报文丢失了，服务端则重传的 FIN 报文，而这时客户端已经进入到关闭状态了，在收到服务端重传的 FIN 报文后，就会回 RST 报文。</p>
<p>服务端收到这个 RST 并将其解释为一个错误（Connection reset by peer），这对于一个可靠的协议来说不是一个优雅的终止方式。</p>
<p>为了防止这种情况出现，客户端必须等待足够长的时间确保对端收到 ACK，如果对端没有收到 ACK，那么就会触发 TCP 重传机制，服务端会重新发送一个 FIN，这样一去一来刚好两个 MSL 的时间。</p>
<h4 id="为什么-TIME-WAIT-等待的时间是-2MSL？"><a href="#为什么-TIME-WAIT-等待的时间是-2MSL？" class="headerlink" title="为什么 TIME_WAIT 等待的时间是 2MSL？"></a><em>为什么 TIME_WAIT 等待的时间是 2MSL？</em></h4><p><code>MSL</code> 是 Maximum Segment Lifetime，<strong>报文最大生存时间</strong>，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。</p>
<p><strong>TTL 的值一般是 64，Linux 将 MSL 设置为 30 秒，意味着 Linux 认为数据报文经过 64 个路由器的时间不会超过 30 秒，如果超过了，就认为报文已经消失在网络中了</strong>。</p>
<p><code>2MSL</code> 的时间是从<strong>客户端接收到 FIN 后发送 ACK 开始计时的</strong>。如果在 TIME-WAIT 时间内，因为客户端的 ACK 没有传输到服务端，客户端又接收到了服务端重发的 FIN 报文，那么 <strong>2MSL 时间将重新计时</strong>。</p>
<p>TIME_WAIT 等待 2 倍的 MSL，比较合理的解释是： 网络中可能存在来自发送方的数据包，当这些发送方的数据包被接收方处理后又会向对方发送响应，所以<strong>一来一回需要等待 2 倍的时间</strong>。</p>
<p>比如，如果被动关闭方没有收到断开连接的最后的 ACK 报文，就会触发超时重发 <code>FIN</code> 报文，另一方接收到 FIN 后，会重发 ACK 给被动关闭方， 一来一去正好 2 个 MSL。</p>
<p>可以看到 <strong>2MSL时长</strong> 这其实是相当于<strong>至少允许报文丢失一次</strong>。比如，若 ACK 在一个 MSL 内丢失，这样被动方重发的 FIN 会在第 2 个 MSL 内到达，TIME_WAIT 状态的连接可以应对。</p>
<p>为什么不是 4 或者 8 MSL 的时长呢？你可以想象一个丢包率达到百分之一的糟糕网络，连续两次丢包的概率只有万分之一，这个概率实在是太小了，忽略它比解决它更具性价比。</p>
<h4 id="如何优化-TIME-WAIT？"><a href="#如何优化-TIME-WAIT？" class="headerlink" title="如何优化 TIME_WAIT？"></a><em>如何优化 TIME_WAIT？</em></h4><p>过多的 TIME-WAIT 状态主要的危害有两种：</p>
<ul>
<li>第一是内存资源占用；</li>
<li>第二是对端口资源的占用，一个 TCP 连接至少消耗「发起连接方」的一个本地端口；</li>
</ul>
<p><strong>如果「发起连接方」的 TIME_WAIT 状态过多，占满了所有端口资源，则会导致无法创建新连接。</strong>因为端口就 65536 个，被占满就会导致无法创建新的连接。</p>
<p><strong>以上只针对连接同一服务器的情况</strong>，如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端 TIME_WAIT 状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。但是，<strong>只要客户端连接的服务器不同，端口资源可以重复使用的</strong>。</p>
<p>处于TIME-WAIT状态的TCP连接，在链接表槽中存活1分钟，意味着另一个相同四元组（源地址，源端口，目标地址，目标端口）的连接不能出现，也就是说新的TCP（相同四元组）连接无法建立。</p>
<p>解决办法是，增加四元组的范围，这有很多方法去实现。（以下建议的顺序，实施难度从小到大排列）</p>
<ul>
<li>修改net.ipv4.ip_local_port_range参数，增加客户端端口可用范围。</li>
<li>增加服务端端口，多监听一些端口，比如81、82、83这些，web服务器前有负载均衡，对用户友好。</li>
<li>增加客户端IP，尤其是作为负载均衡服务器时，使用更多IP去跟后端的web服务器通讯。</li>
<li>增加服务端IP。</li>
</ul>
<p>其他优化 TIME-WAIT 的几个方式，都是有利有弊：</p>
<ul>
<li>打开 net.ipv4.tcp_tw_reuse 和 net.ipv4.tcp_timestamps 选项；</li>
<li>net.ipv4.tcp_max_tw_buckets</li>
<li>程序中使用 SO_LINGER ，应用强制使用 RST 关闭。</li>
</ul>
<p><em>方式一：net.ipv4.tcp_tw_reuse 和 tcp_timestamps</em></p>
<p><code>net.ipv4.tcp_tw_reuse </code>Linux 内核参数开启后，则可以<strong>复用处于 TIME_WAIT 的 socket 为新的连接所用</strong>。</p>
<p>有一点需要注意的是，<strong>tcp_tw_reuse 功能只能用客户端（连接发起方），因为开启了该功能，在调用 connect() 函数时，内核会随机找一个 time_wait 状态超过 1 秒的连接给新的连接复用。</strong></p>
<p>使用这个选项，还有一个前提，需要打开对 TCP 时间戳的支持，即<code>tcp_timestamps</code></p>
<p>这个时间戳的字段是在 TCP 头部的「选项」里，它由一共 8 个字节表示时间戳，其中第一个 4 字节字段用来保存发送该数据包的时间，第二个 4 字节字段用来保存最近一次接收对方发送到达数据的时间。</p>
<p>由于引入了时间戳，我们在前面提到的 <code>2MSL</code> 问题就不复存在了，因为重复的数据包会因为时间戳过期被自然丢弃。</p>
<p><em>方式二：net.ipv4.tcp_max_tw_buckets</em></p>
<p>这个值默认为 18000，<strong>当系统中处于 TIME_WAIT 的连接一旦超过这个值时，系统就会将后面的 TIME_WAIT 连接状态重置</strong>，这个方法比较暴力。</p>
<p><em>方式三：程序中使用 SO_LINGER</em></p>
<p>我们可以通过设置 socket 选项，来设置调用 close 关闭连接行为。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">linger</span> <span class="title">so_linger</span>;</span></span><br><span class="line">so_linger.l_onoff = <span class="number">1</span>;</span><br><span class="line">so_linger.l_linger = <span class="number">0</span>;</span><br><span class="line">setsockopt(s, SOL_SOCKET, SO_LINGER, &amp;so_linger,<span class="keyword">sizeof</span>(so_linger));</span><br></pre></td></tr></table></figure>

<p>如果<code>l_onoff</code>为非 0， 且<code>l_linger</code>值为 0，那么调用<code>close</code>后，会立该发送一个<code>RST</code>标志给对端，该 TCP 连接将跳过四次挥手，也就跳过了<code>TIME_WAIT</code>状态，直接关闭。</p>
<p>但这为跨越<code>TIME_WAIT</code>状态提供了一个可能，不过是一个非常危险的行为，不值得提倡。</p>
<h4 id="为什么tcp-tw-reuse默认关闭"><a href="#为什么tcp-tw-reuse默认关闭" class="headerlink" title="为什么tcp_tw_reuse默认关闭"></a><em>为什么tcp_tw_reuse默认关闭</em></h4><p>tcp_tw_reuse加上时间戳选项，看起来已经解决了可能会接收历史报文的问题，不再需要TIME_WAIT了，那么为什么不默认开启tcp_tw_reuse呢？</p>
<p>tcp_tw_reuse 的作用是让客户端快速复用处于 TIME_WAIT 状态的端口，相当于跳过了 TIME_WAIT 状态，这可能会出现这样的两个问题：</p>
<ul>
<li>历史 RST 报文可能会终止后面相同四元组的连接，因为 PAWS检查RST 报文时， <strong>RST 报文的时间戳即使过期了，只要 RST 报文的序列号在对方的接收窗口内，也是能被接受的</strong>。</li>
<li>如果第四次挥手的 ACK 报文丢失了，有可能被动关闭连接的一方不能被正常的关闭;</li>
</ul>
<h4 id="tcp保活机制"><a href="#tcp保活机制" class="headerlink" title="tcp保活机制"></a><em>tcp保活机制</em></h4><p>TCP 有一个机制是<strong>保活机制</strong>。这个机制的原理是这样的：</p>
<p>定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一个时间间隔，发送一个探测报文，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序。</p>
<p>在 Linux 内核可以有对应的参数可以设置保活时间、保活探测的次数、保活探测的时间间隔，以下都为默认值：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.tcp_keepalive_time=7200</span><br><span class="line">net.ipv4.tcp_keepalive_intvl=75  </span><br><span class="line">net.ipv4.tcp_keepalive_probes=9</span><br></pre></td></tr></table></figure>

<ul>
<li>tcp_keepalive_time&#x3D;7200：表示保活时间是 7200 秒（2小时），也就 2 小时内如果没有任何连接相关的活动，则会启动保活机制</li>
<li>tcp_keepalive_intvl&#x3D;75：表示每次检测间隔 75 秒；</li>
<li>tcp_keepalive_probes&#x3D;9：表示检测 9 次无响应，认为对方是不可达的，从而中断本次的连接。</li>
</ul>
<p>也就是说在 Linux 系统中，<strong>最少需要经过 2 小时 11 分 15 秒</strong>才可以发现一个「死亡」连接。</p>
<p>注意，应用程序若想使用 TCP 保活机制需要通过 socket 接口设置 <code>SO_KEEPALIVE</code> 选项才能够生效，如果没有设置，那么就无法使用 TCP 保活机制。</p>
<p>如果开启了 TCP 保活，需要考虑以下几种情况：</p>
<ul>
<li>第一种，对端程序是正常工作的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 <strong>TCP 保活时间会被重置</strong>，等待下一个 TCP 保活时间的到来。</li>
<li>第二种，对端程序崩溃并重启。当 TCP 保活的探测报文发送给对端后，对端是可以响应的，但由于没有该连接的有效信息，<strong>会产生一个 RST 报文</strong>，这样很快就会发现 TCP 连接已经被重置。</li>
<li>第三种，是对端程序崩溃，或对端由于其他原因导致报文不可达。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，<strong>TCP 会报告该 TCP 连接已经死亡</strong>。</li>
</ul>
<p>TCP 保活的这个机制检测的时间是有点长，我们可以自己在应用层实现一个心跳机制。</p>
<p>比如，web 服务软件一般都会提供 <code>keepalive_timeout</code> 参数，用来指定 HTTP 长连接的超时时间。如果设置了 HTTP 长连接的超时时间是 60 秒，web 服务软件就会<strong>启动一个定时器</strong>，如果客户端在完后一个 HTTP 请求后，在 60 秒内都没有再发起新的请求，<strong>定时器的时间一到，就会触发回调函数来释放该连接。</strong></p>
<h3 id="socket编程"><a href="#socket编程" class="headerlink" title="socket编程"></a>socket编程</h3><p><img src="https://raw.githubusercontent.com/hufei96/Image/main/socket.png" alt="基于 TCP 协议的客户端和服务器工作"></p>
<ul>
<li>服务端和客户端初始化 <code>socket</code>，得到文件描述符；</li>
<li>服务端调用 <code>bind</code>，将绑定在 IP 地址和端口;</li>
<li>服务端调用 <code>listen</code>，进行监听；</li>
<li>服务端调用 <code>accept</code>，等待客户端连接；</li>
<li>客户端调用 <code>connect</code>，向服务器端的地址和端口发起连接请求；</li>
<li>服务端 <code>accept</code> 返回用于传输的 <code>socket</code> 的文件描述符；</li>
<li>客户端调用 <code>write</code> 写入数据；服务端调用 <code>read</code> 读取数据；</li>
<li>客户端断开连接时，会调用 <code>close</code>，向对端发送FIN包，服务端接收到了 FIN 报文，TCP 协议栈会为 FIN 包插入一个文件结束符 <code>EOF</code> 到接收缓冲区中，那么服务端 <code>read</code> 读取数据的时候，就会读取到了 <code>EOF</code>，待处理完数据后，服务端调用 <code>close</code>，表示连接关闭。</li>
</ul>
<p>这里需要注意的是，服务端调用 <code>accept</code> 时，连接成功了会返回一个已完成连接的 socket，后续用来传输数据。</p>
<p>所以，监听的 socket 和真正用来传送数据的 socket，是「两个」 socket，一个叫作<strong>监听 socket</strong>，一个叫作<strong>已完成连接 socket</strong>。</p>
<p>成功连接建立之后，双方开始通过 read 和 write 函数来读写数据，就像往一个文件流里面写东西一样。</p>
<h4 id="listen-时候参数-backlog-的意义？"><a href="#listen-时候参数-backlog-的意义？" class="headerlink" title="listen 时候参数 backlog 的意义？"></a><em>listen 时候参数 backlog 的意义？</em></h4><p>Linux内核中会维护两个队列：</p>
<ul>
<li>半连接队列（SYN 队列）：接收到一个 SYN 建立连接请求，处于 SYN_RCVD 状态；</li>
<li>全连接队列（Accpet 队列）：已完成 TCP 三次握手过程，处于 ESTABLISHED 状态；</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">listen</span> <span class="params">(<span class="type">int</span> socketfd, <span class="type">int</span> backlog)</span></span><br></pre></td></tr></table></figure>

<p><strong>全连接队列（icsk_accept_queue）是个链表</strong>，而<strong>半连接队列（syn_table）是个哈希表</strong>。</p>
<p>先对比下<strong>全连接里队列</strong>，他本质是个链表，因为也是线性结构，说它是个队列也没毛病。它里面放的都是已经建立完成的连接，这些连接正等待被取走。而服务端取走连接的过程中，并不关心具体是哪个连接，只要是个连接就行，所以直接从队列头取就行了。这个过程算法复杂度为<code>O(1)</code>。</p>
<p>而<strong>半连接队列</strong>却不太一样，因为队列里的都是不完整的连接，嗷嗷等待着第三次握手的到来。那么现在有一个第三次握手来了，则需要从队列里把相应IP端口的连接取出，<strong>如果半连接队列还是个链表，那我们就需要依次遍历，才能拿到我们想要的那个连接，算法复杂度就是O(n)。</strong>而如果将半连接队列设计成哈希表，那么查找半连接的算法复杂度就回到<code>O(1)</code>了。</p>
<p>因此出于效率考虑，全连接队列被设计成链表，而半连接队列被设计为哈希表。</p>
<p>在早期 Linux 内核 backlog 是 SYN 队列大小，也就是未完成的队列大小。</p>
<p>在 Linux 内核 2.2 之后，backlog 变成 accept 队列，也就是已完成连接建立的队列长度，<strong>所以现在通常认为 backlog 是 accept 队列。</strong></p>
<p><strong>但是上限值是内核参数 somaxconn 的大小，也就说 accpet 队列长度 &#x3D; min(backlog, somaxconn)。</strong></p>
<h4 id="connect和accept-发生在三次握手的哪一步？"><a href="#connect和accept-发生在三次握手的哪一步？" class="headerlink" title="connect和accept 发生在三次握手的哪一步？"></a><em>connect和accept 发生在三次握手的哪一步？</em></h4><p>客户端协议栈收到 服务器的ACK 之后，使得应用程序从 <code>connect</code> 调用返回，表示客户端到服务器端的单向连接建立成功，客户端的状态为 ESTABLISHED</p>
<p>客户端的ACK 应答包到达服务器端后，服务器端的 TCP 连接进入 ESTABLISHED 状态，同时服务器端协议栈使得 <code>accept</code> 阻塞调用返回，这个时候服务器端到客户端的单向连接也建立成功。至此，客户端与服务端两个方向的连接都建立成功。</p>
<p>从上面的描述过程，我们可以得知<strong>客户端 connect 成功返回是在第二次握手之后，服务端 accept 成功返回是在三次握手成功之后。</strong></p>
<h4 id="SYN-报文什么时候情况下会被丢弃？"><a href="#SYN-报文什么时候情况下会被丢弃？" class="headerlink" title="SYN 报文什么时候情况下会被丢弃？"></a><em>SYN 报文什么时候情况下会被丢弃？</em></h4><ul>
<li>开启 tcp_tw_recycle 参数，并且在 NAT 环境下，造成 SYN 报文被丢弃</li>
<li>TCP 两个队列满了（半连接队列和全连接队列），造成 SYN 报文被丢弃</li>
</ul>
<p>1.<strong>开启 tcp_tw_recycle 参数</strong></p>
<p>net.ipv4.tcp_tw_recycle，如果开启该选项的话，允许处于 TIME_WAIT 状态的连接被快速回收；但是tcp_tw_recycle 在使用了 NAT 的网络下是不安全的，具体如下：</p>
<p>开启了 recycle 和 timestamps 选项，就会开启一种叫 per-host 的 PAWS 机制。<strong>per-host 是对「对端 IP 做 PAWS 检查」</strong>，而非对「IP + 端口」四元组做 PAWS 检查。</p>
<p>但是如果客户端网络环境是用了 NAT 网关，那么客户端环境的每一台机器通过 NAT 网关后，都会是相同的 IP 地址，在服务端看来，就好像只是在跟一个客户端打交道一样，无法区分出来。</p>
<p>当客户端 A 通过 NAT 网关和服务器建立 TCP 连接，然后服务器主动关闭并且快速回收 TIME-WAIT 状态的连接后，客户端 B 也通过 NAT 网关和服务器建立 TCP 连接，<strong>注意客户端 A 和 客户端 B 因为经过相同的 NAT 网关，所以是用相同的 IP 地址与服务端建立 TCP 连接</strong>，如果客户端 B 的 timestamp 比 客户端 A 的 timestamp 小，那么由于服务端的 per-host 的 PAWS 机制的作用，服务端就会丢弃客户端主机 B 发来的 SYN 包。</p>
<p>tcp_tw_recycle 在 <strong>Linux 4.12 版本后，直接取消了这一参数</strong>。</p>
<p>2.<strong>TCP 两个队列满了</strong></p>
<p>服务端收到客户端发起的 SYN 请求后，<strong>内核会把该连接存储到半连接队列</strong>，并向客户端响应 SYN+ACK，接着客户端会返回 ACK，服务端收到第三次握手的 ACK 后，<strong>内核会把连接从半连接队列移除，然后创建新的完全的连接，并将其添加到 accept 队列，等待进程调用 accept 函数时把连接取出来。</strong></p>
<p>若半连接队列满了，这时后面来的 syn 包都会被丢弃。 </p>
<p>若全连接队列满了，且没有重传 SYN+ACK 包的连接请求多于 1 个，则会丢弃；</p>
<p><em>解决办法</em></p>
<p><em>方式一：增大半连接队列</em></p>
<p>半连接队列的值跟 tcp_max_syn_backlog ， somaxconn，backlog都有关系</p>
<p><strong>要想增大半连接队列，我们得知不能只单纯增大 tcp_max_syn_backlog 的值，还需一同增大 somaxconn 和 backlog，也就是增大全连接队列</strong>。否则，只单纯增大 tcp_max_syn_backlog 是无效的。</p>
<p>增大 tcp_max_syn_backlog 和 somaxconn 的方法是修改 Linux 内核参数</p>
<p><em>方式二：开启 tcp_syncookies 功能</em></p>
<p>开启 tcp_syncookies 功能的方式也很简单，修改 Linux 内核参数。</p>
<p><strong>如果开启了syncookies 功能，半连接队列满后，后续的请求就不会存放到半连接队列了，因此也不会丢弃syn 包</strong>。</p>
<p>syncookies 是这么做的：服务器收到TCP SYN包时，服务器会返回适当的SYN+ACK 响应，但会丢弃 SYN 队列条目。这个响应包中的序号是按照某种算法将时间戳，MSS， IP 地址和端口号等信息编码得到的，这个序号就是所谓的 <em>SYN cookie</em>。服务器接收到客户端随后的ACK响应，就能够解码在 TCP 序号内的信息重构 SYN 队列条目。</p>
<p><em>方式三：减少 SYN+ACK 重传次数</em></p>
<p>当服务端受到 SYN 攻击时，就会有大量处于 SYN_REVC 状态的 TCP 连接，处于这个状态的 TCP 会重传 SYN+ACK ，当重传超过次数达到上限后，就会断开连接。</p>
<p>那么针对 SYN 攻击的场景，我们可以减少 SYN+ACK 的重传次数，以加快处于 SYN_REVC 状态的 TCP 连接断开。</p>
<h4 id="shutdown和close有什么区别"><a href="#shutdown和close有什么区别" class="headerlink" title="shutdown和close有什么区别"></a><em>shutdown和close有什么区别</em></h4><p>总结：</p>
<ol>
<li>close 会关闭连接，<strong>并释放所有连接对应的资源，而 shutdown 并不会释放掉套接字和所有的资源。</strong></li>
<li><strong>close 存在引用计数的概念，在多进程共享socket时并不一定会关闭连接；shutdown 则不管引用计数，直接关闭连接，如果有别的进程企图使用该socket，将会受到影响。</strong></li>
<li>close是双向关闭，而shutdown可以控制单向关闭</li>
</ol>
<p><em>shutdown函数</em> </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">shutdown</span><span class="params">(<span class="type">int</span> sockfd, <span class="type">int</span> howto)</span></span><br></pre></td></tr></table></figure>

<p>howto 是这个函数的设置选项，它的设置有三个主要选项：</p>
<ul>
<li>SHUT_RD(0)：关闭连接的“读”这个方向，对该套接字进行读操作直接返回 EOF。从数据角度来看，套接字上<strong>接收缓冲区已有的数据将被丢弃</strong>，如果再有新的数据流到达，会对数据进行 ACK，然后悄悄地丢弃。也就是说，对端还是会接收到 ACK，在这种情况下根本不知道数据已经被丢弃了。</li>
<li>SHUT_WR(1)：关闭连接的“写”这个方向，这就是常被称为”半关闭“的连接。此时，不管套接字引用计数的值是多少，都会直接关闭连接的写方向。套接字上<strong>发送缓冲区已有的数据将被立即发送出去</strong>，并发送一个 FIN 报文给对端。应用程序如果对该套接字进行写操作会报错。</li>
<li>SHUT_RDWR(2)：相当于 SHUT_RD 和 SHUT_WR 操作各一次，关闭套接字的读和写两个方向。</li>
</ul>
<p><em>close函数</em></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">close</span><span class="params">(<span class="type">int</span> sockfd)</span></span><br></pre></td></tr></table></figure>

<p>这个函数会对套接字引用计数减一，一旦发现套接字引用计数到 0，就会对套接字进行彻底释放，<strong>并且会关闭 TCP 两个方向的数据流</strong>。</p>
<p>在输入方向，系统内核会将该套接字设置为不可读，任何读操作都会返回异常。</p>
<p>在输出方向，系统内核尝试将发送缓冲区的数据发送给对端，并最后向对端发送一个 FIN 报文，接下来如果再对该套接字进行写操作会返回异常。</p>
<p>如果对端没有检测到套接字已关闭，还继续发送报文，就会收到一个 RST 报文，告诉对端：“Hi, 我已经关闭了，别再给我发数据了。”</p>
<h4 id="TCP-和-UDP-可以同时绑定相同的端口吗？"><a href="#TCP-和-UDP-可以同时绑定相同的端口吗？" class="headerlink" title="TCP 和 UDP 可以同时绑定相同的端口吗？"></a><strong>TCP 和 UDP 可以同时绑定相同的端口吗？</strong></h4><p>可以。</p>
<p>当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP&#x2F;UDP，所以可以根据这个信息确定送给哪个模块（TCP&#x2F;UDP）处理，送给 TCP&#x2F;UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。</p>
<p>因此， TCP&#x2F;UDP 各自的端口号也相互独立，如 TCP 有一个 80 号端口，UDP 也可以有一个 80 号端口，二者并不冲突。</p>
<h4 id="多个-TCP-服务进程可以绑定同一个端口吗？"><a href="#多个-TCP-服务进程可以绑定同一个端口吗？" class="headerlink" title="多个 TCP 服务进程可以绑定同一个端口吗？"></a><strong>多个 TCP 服务进程可以绑定同一个端口吗？</strong></h4><p>如果两个 TCP 服务进程<strong>同时绑定的 IP 地址和端口都相同</strong>，那么执行 bind() 时候就会出错，错误是“Address already in use”。</p>
<p>如果两个 TCP 服务进程绑定的 IP 地址不同，而端口相同的话，也是可以绑定成功的。</p>
<p>注意，如果 TCP 服务进程 A 绑定的地址是 0.0.0.0 和端口 8888，而如果 TCP 服务进程 B 绑定的地址是 192.168.1.100 地址（或者其他地址）和端口 8888，那么执行 bind() 时候也会出错。</p>
<p>这是因为 <strong>0.0.0.0 地址比较特殊，代表任意地址</strong>，意味着绑定了 0.0.0.0 地址，相当于把主机上的所有 IP 地址都绑定了。(这个问题可以由 SO_REUSEADDR 解决)</p>
<p>如果想多个进程绑定相同的 IP 地址和端口，也是有办法的，就是对 socket 设置 SO_REUSEPORT 属性</p>
<h4 id="客户端的端口可以重复使用吗？"><a href="#客户端的端口可以重复使用吗？" class="headerlink" title="客户端的端口可以重复使用吗？"></a><strong>客户端的端口可以重复使用吗？</strong></h4><p>可以。</p>
<p>客户端在执行 connect 函数的时候，会在内核里随机选择一个端口，然后向服务端发起 SYN 报文，然后与服务端进行三次握手。</p>
<p>所以，客户端的端口选择的发生在 connect 函数，内核在选择端口的时候，会从 <code>net.ipv4.ip_local_port_range</code> 这个内核参数指定的范围来选取一个端口作为客户端端口。该参数的默认值是 32768 61000，意味着端口总可用的数量是 61000 - 32768 &#x3D; 28232 个。</p>
<p>TCP 连接是由四元组（源IP地址，源端口，目的IP地址，目的端口）唯一确认的，只要四元组中其中一个元素发生了变化，那么就表示不同的 TCP 连接。所以<strong>如果客户端已使用端口 64992 与服务端 A 建立了连接，那么客户端要与服务端 B 建立连接，还是可以使用端口 64992 的</strong>，因为内核是通过四元组信息来定位一个 TCP 连接的，并不会因为客户端的端口号相同，而导致连接冲突的问题。</p>
<h4 id="客户端可以bind吗"><a href="#客户端可以bind吗" class="headerlink" title="客户端可以bind吗"></a>客户端可以bind吗</h4><p>bind 函数虽然常用于服务端网络编程中，但是它也是可以用于客户端的。</p>
<p>前面我们知道，客户端是在调用 connect 函数的时候，由内核随机选取一个端口作为连接的端口。</p>
<p>而如果我们想自己指定连接的端口，就可以用 bind 函数来实现：客户端先通过 bind 函数绑定一个端口，然后调用 connect 函数就会跳过端口选择的过程了，转而使用 bind 时确定的端口。</p>
<h4 id="多个客户端可以-bind-同一个端口吗？"><a href="#多个客户端可以-bind-同一个端口吗？" class="headerlink" title="多个客户端可以 bind 同一个端口吗？"></a>多个客户端可以 bind 同一个端口吗？</h4><p>跟服务端一样，要看多个客户端绑定的 IP + PORT 是否都相同，如果都是相同的，那么在执行 bind() 时候就会出错，错误是“Address already in use”。</p>
<h4 id="没有-listen，能建立-TCP-连接吗？"><a href="#没有-listen，能建立-TCP-连接吗？" class="headerlink" title="没有 listen，能建立 TCP 连接吗？"></a>没有 listen，能建立 TCP 连接吗？</h4><p><strong>服务端如果只 bind 了 IP 地址和端口，而没有调用 listen 的话，然后客户端对服务端发起了连接建立，服务端会回 RST 报文。</strong></p>
<p>但没有listen，客户端可以建立tcp自连接，也就是自己连自己。</p>
<p>我们知道执行 listen 方法时，会创建半连接队列和全连接队列。三次握手的过程中会在这两个队列中暂存连接信息。所以形成连接，前提是你得有个地方存放着，方便握手的时候能根据 IP + 端口等信息找到对应的 socket。</p>
<p>因为客户端没有执行listen，所以没有半连接队列和全连接队列。但内核还有个全局 hash 表，可以用于存放 sock 连接的信息。</p>
<p><strong>在 TCP 自连接的情况中，客户端在 connect 方法时，最后会将自己的连接信息放入到这个全局 hash 表中，然后将信息发出，消息在经过回环地址重新回到 TCP 传输层的时候，就会根据 IP + 端口信息，再一次从这个全局 hash 中取出信息。于是握手包一来一回，最后成功建立连接</strong>。</p>
<h4 id="没有-accept，能建立-TCP-连接吗？"><a href="#没有-accept，能建立-TCP-连接吗？" class="headerlink" title="没有 accept，能建立 TCP 连接吗？"></a>没有 accept，能建立 TCP 连接吗？</h4><p>accept方法只是为了从全连接队列中拿出一条连接，本身跟三次握手几乎<strong>毫无关系</strong>。</p>
<p><strong>就算不执行accept()方法，三次握手照常进行，并顺利建立连接。</strong> <strong>并且在服务端执行accept()前，如果客户端发送消息给服务端，服务端是能够正常回复ack确认包的。</strong></p>
<h3 id="tcp是怎么保证可靠性传输的"><a href="#tcp是怎么保证可靠性传输的" class="headerlink" title="tcp是怎么保证可靠性传输的"></a>tcp是怎么保证可靠性传输的</h3><p>通过TCP 连接传输的数据，无差错、不丢失、不重复、并且按序到达接收端</p>
<h4 id="差错检测"><a href="#差错检测" class="headerlink" title="差错检测"></a><em>差错检测</em></h4><p>每个TCP报文段都包括检验和字段，校验和用来检查报文段是否出现传输错误，如果报文段出现传输错误，TCP检查出错就丢弃该报文段。</p>
<h4 id="序列号，确认应答和重传"><a href="#序列号，确认应答和重传" class="headerlink" title="序列号，确认应答和重传"></a><em>序列号，确认应答和重传</em></h4><p>TCP 连接中，为传送的字节流（数据）中的每一个字节按顺序编号。当发送端的数据到达接收主机时，接收端主机会返回一个确认应答消息，表示已收到消息。</p>
<p>但在错综复杂的网络，并不一定能如上图那么顺利能正常的数据传输，万一数据在传输过程中丢失了呢？</p>
<p>所以 TCP 针对数据包丢失的情况，会用<strong>重传机制</strong>解决。</p>
<p>接下来说说常见的重传机制：</p>
<ul>
<li>超时重传</li>
<li>快速重传</li>
<li>SACK</li>
<li>D-SACK</li>
</ul>
<p><em>超时重传</em></p>
<p>重传机制的其中一个方式，就是在发送数据时，设定一个定时器，当超过指定的时间后，没有收到对方的 <code>ACK</code> 确认应答报文，就会重发该数据，也就是我们常说的<strong>超时重传</strong>。</p>
<p>数据包会触发超时重传，而单独的ACK报文丢失则不会，因为收到ACK报文后是不需要确认的。</p>
<p>超时重传时间是以 <code>RTO</code> （Retransmission Timeout 超时重传时间）表示。因为我们的网络也是时常变化的。也就因为「报文往返 RTT 的值」 是经常波动变化的，所以「超时重传时间 RTO 的值」应该是一个<strong>动态变化的值</strong>。实际中，RTO是通过RTT的加权平均值和方差计算而得。</p>
<p><strong>每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送。</strong></p>
<p>超时触发重传存在的问题是，超时周期可能相对较长。</p>
<p><em>快速重传</em></p>
<p>TCP 还有另外一种<strong>快速重传（Fast Retransmit）机制</strong>，它<strong>不以时间为驱动，而是以数据驱动重传</strong>。</p>
<p>快速重传的工作方式是当收到三个重复冗余ACK（其实是收到4个同样的ACK，第一个是正常的，后三个才是冗余的），会在<strong>定时器过期之前</strong>，重传丢失的报文段。</p>
<p>快速重传机制只解决了一个问题，就是超时时间的问题，但是它依然面临着另外一个问题。就是<strong>重传的时候，是重传丢失的这个报文，还是重传这个时间段发送的所有报文的问题。</strong></p>
<p>根据 TCP 不同的实现，以上两种情况都是有可能的。可见，这是一把双刃剑。</p>
<p>为了解决不知道该重传哪些 TCP 报文，于是就有 <code>SACK</code> 方法。SACK是选择性重传，解决快速重传不知道重传哪些报文的缺点</p>
<p><em>SACK 方法</em></p>
<p>还有一种实现重传机制的方式叫：<code>SACK</code>（ Selective Acknowledgment 选择性确认）。如果要支持 <code>SACK</code>，必须双方都要支持。在 Linux 下，可以通过 <code>net.ipv4.tcp_sack</code> 参数打开这个功能（Linux 2.4 后默认打开）。</p>
<p>这种方式会在 TCP 头部「选项」字段里加一个 <code>SACK</code> 的东西，接收方<strong>可以将已经接收的数据信息发送给发送方</strong>，这样发送方就可以知道哪些数据收到了，哪些数据没收到，知道了这些信息，就可以<strong>只重传丢失的数据</strong>。</p>
<p>如下图，发送方收到了三次同样的 ACK 确认报文，于是就会触发快速重发机制，通过 <code>SACK</code> 信息发现只有 <code>200~299</code> 这段数据丢失，则重发时，就只选择了这个 TCP 段进行重复。</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/sack.jpg" alt="选择性确认"></p>
<p><em>Duplicate SACK</em></p>
<p>RFC2883对SACK进行了扩展，称为D-SACK：使得扩展后的SACK具有通知发送端哪些数据被重复接收了。</p>
<p>下面举例两个例子，来说明 <code>D-SACK</code> 的作用。</p>
<p><em>1：ACK 丢包</em></p>
<ul>
<li>「接收方」发给「发送方」的两个 ACK 确认应答都丢失了，所以发送方超时后，重传第一个数据包（3000 ~ 3499）</li>
<li><strong>于是「接收方」发现数据是重复收到的，于是回了一个 SACK &#x3D; 3000~3500</strong>，告诉「发送方」 3000~3500 的数据早已被接收了，因为 ACK 都到了 4000 了，已经意味着 4000 之前的所有数据都已收到，所以这个 SACK 就代表着 <code>D-SACK</code>。</li>
<li>这样「发送方」就知道了，数据没有丢，是「接收方」的 ACK 确认报文丢了。</li>
</ul>
<p><em>2：网络延时</em></p>
<ul>
<li>数据包（1000~1499） 被网络延迟了，导致「接收方」没有第一时间收到数据包。</li>
<li>而后「接收方」给「发送方」发了三个冗余ACK，就触发了快速重传机制，但是在重传后，被延迟的数据包（1000~1499）又到了「接收方」；</li>
<li><strong>所以「接收方」回了一个 SACK&#x3D;1000~1500，因为 ACK 已经到了 3000，所以这个 SACK 是 D-SACK，表示收到了重复的包。</strong></li>
<li>这样发送方就知道快速重传触发的原因不是发出去的包丢了，也不是因为回应的 ACK 包丢了，而是因为网络延迟了。</li>
</ul>
<p>可见，<code>D-SACK</code> 可以让发送方更好的了解网络状态</p>
<ol>
<li>可以让「发送方」知道，是发出去的包丢了，还是接收方回应的 ACK 包丢了;</li>
<li>可以知道是不是「发送方」的数据包被网络延迟了;</li>
<li>可以知道网络中是不是把「发送方」的数据包给复制了;</li>
</ol>
<p>在 Linux 下可以通过 <code>net.ipv4.tcp_dsack</code> 参数开启&#x2F;关闭这个功能（Linux 2.4 后默认打开）。</p>
<h4 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a><em><strong>滑动窗口</strong></em></h4><p>为什么要引入窗口：如果没有窗口，发送方每发送一个数据包后必须收到接收方的确认应答才能发送下一个包，效率十分低下。为解决这个问题，TCP 引入了<strong>窗口</strong>这个概念。即使在往返时间较长的情况下，它也不会降低网络通信的效率。有了窗口，就可以指定窗口大小，窗口大小就是指<strong>无需等待确认应答，而可以继续发送数据的最大值</strong>。</p>
<p>窗口的实现实际上是操作系统开辟的一个缓存空间，发送方主机在等到确认应答返回之前，必须在缓冲区中保留已发送的数据。如果按期收到确认应答，此时数据就可以从缓存区清除。</p>
<blockquote>
<p>窗口大小由哪一方决定？</p>
</blockquote>
<p>TCP 头里有一个字段叫 <code>Window</code>，也就是窗口大小。</p>
<p><strong>这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来。</strong></p>
<p>所以，通常窗口的大小是由接收方的窗口大小来决定的。</p>
<p>发送方发送的数据大小不能超过接收方的窗口大小，否则接收方就无法正常接收到数据。</p>
<blockquote>
<p>接收窗口和发送窗口的大小是相等的吗？</p>
</blockquote>
<p>并不是完全相等，接收窗口的大小是<strong>约等于</strong>发送窗口的大小的。</p>
<p>因为滑动窗口并不是一成不变的。比如，当接收方的应用进程读取数据的速度非常快的话，这样的话接收窗口可以很快的就空缺出来。那么新的接收窗口大小，是通过 TCP 报文中的 Windows 字段来告诉发送方。那么这个传输过程是存在时延的，所以接收窗口和发送窗口是约等于的关系。</p>
<blockquote>
<p>窗口大小和序号空间大小的关系</p>
</blockquote>
<p>窗口大小和序号空间大小一样时会出现新的问题，即接收方无法确定收到的分组是新的分组还是老的分组（老的分组在网络中传输了很久才到达接收方），因为它们的序号一样，要解决这个问题只能使用不重复的序号，但序号空间是有限的，通常我们强制窗口小于等于序号空间大小的二分之一，这可以避免绝大部分的情况，即假定绕一圈之后超过了网络的TTL，但仍无法避免那种窝藏在中途延缓很久发送出来的古老循环段造成的影响。</p>
<h4 id="流量控制"><a href="#流量控制" class="headerlink" title="流量控制"></a><em>流量控制</em></h4><p>发送方不能无脑的发数据给接收方，要考虑接收方处理能力。</p>
<p>如果一直无脑的发数据给对方，但对方处理不过来，那么就会导致触发重发机制，从而导致网络流量的无端的浪费。</p>
<p>为了解决这种现象发生，<strong>TCP 提供一种机制可以让「发送方」根据「接收方」的实际接收能力控制发送的数据量，这就是所谓的流量控制。</strong>TCP 通过让接收方指明希望从发送方接收的数据大小（窗口大小）来进行流量控制。</p>
<p>发送窗口和接收窗口中所存放的字节数，都是放在操作系统内存缓冲区中的，而操作系统的缓冲区，会<strong>被操作系统调整</strong>。</p>
<p>当接收方应用进程没办法及时读取缓冲区的内容时，就会减少窗口大小并通知发送方。</p>
<p>若接收方操作系统先减少缓冲区大小，再通知对方就会造成丢包问题。</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/flowcontrol.jpg" alt="img"></p>
<p>所以，如果发生了先减少缓存，再收缩窗口，就会出现丢包的现象。</p>
<p><strong>为了防止这种情况发生，TCP 规定是不允许同时减少缓存又收缩窗口的，而是采用先收缩窗口，过段时间再减少缓存，这样就可以避免了丢包情况。</strong></p>
<p><em>窗口关闭</em></p>
<p>如果窗口大小为 0 时，就会阻止发送方给接收方传递数据，直到窗口变为非 0 为止，这就是窗口关闭。</p>
<blockquote>
<p>窗口关闭潜在的危险</p>
</blockquote>
<p>接收方向发送方通告窗口大小时，是通过 <code>ACK</code> 报文来通告的。</p>
<p>那么，当发生窗口关闭时，接收方处理完数据后，会向发送方通告一个窗口非 0 的 ACK 报文，如果这个通告窗口的 ACK 报文在网络中丢失了，那麻烦就大了。</p>
<p>这会导致发送方一直等待接收方的非 0 窗口通知，接收方也一直等待发送方的数据，如不采取措施，这种相互等待的过程，会造成了死锁的现象。</p>
<blockquote>
<p>TCP 是如何解决窗口关闭时，潜在的死锁现象呢？</p>
</blockquote>
<p>为了解决这个问题，TCP 为每个连接设有一个持续定时器，<strong>只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器。</strong></p>
<p>如果持续计时器超时，就会发送<strong>窗口探测 ( Window probe ) 报文</strong>，而对方在确认这个探测报文时，给出自己现在的接收窗口大小。</p>
<ul>
<li>如果接收窗口仍然为 0，那么收到这个报文的一方就会重新启动持续计时器；</li>
<li>如果接收窗口不是 0，那么死锁的局面就可以被打破了。</li>
</ul>
<p>窗口探测的次数一般为 3 次，每次大约 30-60 秒（不同的实现可能会不一样）。如果 3 次过后接收窗口还是 0 的话，有的 TCP 实现就会发 <code>RST</code> 报文来中断连接。</p>
<p><em>糊涂窗口综合症</em></p>
<p>如果接收方太忙了，来不及取走接收窗口里的数据，那么就会导致发送方的发送窗口越来越小。</p>
<p>到最后，<strong>如果接收方腾出几个字节并告诉发送方现在有几个字节的窗口，而发送方会义无反顾地发送这几个字节，这就是糊涂窗口综合症</strong>。</p>
<p>要知道，我们的 <code>TCP + IP</code> 头有 <code>40</code> 个字节，为了传输那几个字节的数据，要达上这么大的开销，这太不经济了。</p>
<p>所以，糊涂窗口综合症的现象是可以发生在发送方和接收方：</p>
<ul>
<li>接收方可以通告一个小的窗口</li>
<li>而发送方可以发送小数据</li>
</ul>
<p>于是，要解决糊涂窗口综合症，就解决上面两个问题就可以了</p>
<ul>
<li>让接收方不通告小窗口给发送方</li>
<li>让发送方避免发送小数据</li>
</ul>
<blockquote>
<p>怎么让接收方不通告小窗口呢？</p>
</blockquote>
<p>接收方通常的策略如下:</p>
<p>当「窗口大小」小于 min( MSS，缓存空间&#x2F;2 ) ，也就是小于 MSS 与 1&#x2F;2 缓存大小中的最小值时，就会向发送方通告窗口为 <code>0</code>，也就阻止了发送方再发数据过来。</p>
<p>等到接收方处理了一些数据后，窗口大小 &gt;&#x3D; MSS，或者接收方缓存空间有一半可以使用，就可以把窗口打开让发送方发送数据过来。</p>
<blockquote>
<p>怎么让发送方避免发送小数据呢？</p>
</blockquote>
<p>发送方通常的策略:</p>
<p>使用 Nagle 算法，该算法的思路是延时处理，它满足以下两个条件中的一条才可以发送数据：</p>
<ul>
<li>要等到窗口大小 &gt;&#x3D; <code>MSS</code> 或是 数据大小 &gt;&#x3D; <code>MSS</code></li>
<li>收到之前发送数据的 <code>ack</code> 回包</li>
</ul>
<p>只要没满足上面条件中的一条，发送方一直在囤积数据，直到满足上面的发送条件。</p>
<p>另外，Nagle 算法默认是打开的，如果对于一些需要小数据包交互的场景的程序，比如，telnet 或 ssh 这样的交互性比较强的程序，则需要关闭 Nagle 算法。</p>
<p>可以在 Socket 设置 <code>TCP_NODELAY</code> 选项来关闭这个算法（关闭 Nagle 算法没有全局参数，需要根据每个应用自己的特点来关闭）</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">setsockopt(sock_fd, IPPROTO_TCP, TCP_NODELAY, (<span class="type">char</span> *)&amp;value, <span class="keyword">sizeof</span>(<span class="type">int</span>));</span><br></pre></td></tr></table></figure>

<h4 id="拥塞控制"><a href="#拥塞控制" class="headerlink" title="拥塞控制"></a><em>拥塞控制</em></h4><p>前面的流量控制是避免「发送方」的数据填满「接收方」的缓存，但是并不知道网络状态。</p>
<p><strong>在网络出现拥堵时，如果继续发送大量数据包，可能会导致数据包时延、丢失等，这时 TCP 就会重传数据，但是一重传就会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，这个情况就会进入恶性循环被不断地放大….</strong></p>
<p>所以，TCP 不能忽略网络上发生的事，它被设计成一个无私的协议，当网络发送拥塞时，TCP 会自我牺牲，降低发送的数据量。</p>
<p>于是，就有了<strong>拥塞控制</strong>，控制的目的就是<strong>避免「发送方」的数据填满整个网络。</strong></p>
<p>为了在「发送方」调节所要发送数据的量，定义了一个叫做「<strong>拥塞窗口</strong>」的概念。</p>
<blockquote>
<p>什么是拥塞窗口？和发送窗口有什么关系呢？</p>
</blockquote>
<p><strong>拥塞窗口 cwnd</strong>是发送方维护的一个的状态变量，它会根据<strong>网络的拥塞程度动态变化的</strong>。</p>
<p>我们在前面提到过发送窗口 <code>swnd</code> 和接收窗口 <code>rwnd</code> 是约等于的关系，那么由于加入了拥塞窗口的概念后，此时发送窗口的值是swnd &#x3D; min(cwnd, rwnd)，也就是拥塞窗口和接收窗口中的最小值。</p>
<p>拥塞窗口 <code>cwnd</code> 变化的规则：</p>
<ul>
<li>只要网络中没有出现拥塞，<code>cwnd</code> 就会增大；</li>
<li>但网络中出现了拥塞，<code>cwnd</code> 就减少；</li>
</ul>
<blockquote>
<p>那么怎么知道当前网络是否出现了拥塞呢？</p>
</blockquote>
<p>其实只要「发送方」没有在规定时间内接收到 ACK 应答报文，也就是<strong>发生了超时重传，就会认为网络出现了拥塞。</strong></p>
<blockquote>
<p>拥塞控制有哪些控制算法？</p>
</blockquote>
<p>拥塞控制主要是四个算法：</p>
<ul>
<li>慢启动</li>
<li>拥塞避免</li>
<li>拥塞发生</li>
<li>快速恢复</li>
</ul>
<p><em>慢启动</em></p>
<p>TCP 在刚建立连接完成后，首先是有个慢启动的过程，这个慢启动的意思就是一点一点的提高发送数据包的数量，如果一上来就发大量的数据，这不是给网络添堵吗？</p>
<p>慢启动的算法记住一个规则就行：<strong>当发送方每收到一个MSS段的ACK，拥塞窗口 cwnd 的大小就会加 1。</strong></p>
<p>这里假定拥塞窗口 <code>cwnd</code> 和发送窗口 <code>swnd</code> 相等，下面举个栗子：</p>
<ul>
<li>连接建立完成后，一开始初始化 <code>cwnd = 1</code>，表示可以传一个 <code>MSS</code> 大小的数据。</li>
<li>当收到一个 ACK 确认应答后，cwnd 增加 1，于是一次能够发送 2 个</li>
<li>当收到 2 个的 ACK 确认应答后， cwnd 增加 2，于是就可以比之前多发2 个，所以这一次能够发送 4 个</li>
<li>当这 4 个的 ACK 确认到来的时候，每个确认 cwnd 增加 1， 4 个确认 cwnd 增加 4，于是就可以比之前多发 4 个，所以这一次能够发送 8 个。</li>
</ul>
<p>可以看出慢启动算法，发包的个数是随时间<strong>指数性的增长</strong>。</p>
<blockquote>
<p>那慢启动涨到什么时候是个头呢？</p>
</blockquote>
<p>有一个叫慢启动门限 <code>ssthresh</code> （slow start threshold）状态变量。</p>
<ul>
<li>当 <code>cwnd</code> &lt; <code>ssthresh</code> 时，使用慢启动算法。</li>
<li>当 <code>cwnd</code> &gt;&#x3D; <code>ssthresh</code> 时，就会使用「拥塞避免算法」。</li>
</ul>
<p><em>拥塞避免算法</em></p>
<p>前面说道，当拥塞窗口 <code>cwnd</code> 「超过」慢启动门限 <code>ssthresh</code> 就会进入拥塞避免算法。</p>
<p>一般来说 <code>ssthresh</code> 的大小是 <code>65535</code> 字节。</p>
<p>那么进入拥塞避免算法后，它的规则是：<strong>每当收到一个 ACK 时，cwnd 增加 1&#x2F;cwnd。</strong></p>
<p>接上前面的慢启动的栗子，现假定 <code>ssthresh</code> 为 <code>8</code>：</p>
<ul>
<li>当 8 个 ACK 应答确认到来时，每个确认增加 1&#x2F;8，8 个 ACK 确认 cwnd 一共增加 1，于是这一次能够发送 9 个 <code>MSS</code> 大小的数据，变成了<strong>线性增长。</strong></li>
</ul>
<p>所以，我们可以发现，拥塞避免算法就是将原本慢启动算法的指数增长变成了线性增长，还是增长阶段，但是增长速度缓慢了一些。</p>
<p>就这么一直增长着后，网络就会慢慢进入了拥塞的状况了，于是就会出现丢包现象，这时就需要对丢失的数据包进行重传。</p>
<p>当触发了重传机制，也就进入了「拥塞发生算法」。</p>
<p><em>拥塞发生</em></p>
<p>当网络出现拥塞，也就是会发生数据包重传，重传机制主要有两种：</p>
<ul>
<li>超时重传</li>
<li>快速重传</li>
</ul>
<p>这两种使用的拥塞发送算法是不同的，接下来分别来说说。</p>
<blockquote>
<p>发生超时重传的拥塞发生算法</p>
</blockquote>
<p>当发生了「超时重传」，则就会使用拥塞发生算法。</p>
<p>这个时候，ssthresh 和 cwnd 的值会发生变化：</p>
<ul>
<li><code>ssthresh</code> 设为 <code>cwnd/2</code>，</li>
<li><code>cwnd</code> 重置为 初始值（我们可以用 ss -nli 命令查看每一个 TCP 连接的 cwnd 初始化值，默认10）</li>
</ul>
<p>接着，就重新开始慢启动，慢启动是会突然减少数据流的。这真是一旦「超时重传」，马上回到解放前。但是这种方式太激进了，反应也很强烈，会造成网络卡顿。</p>
<blockquote>
<p>发生快速重传的拥塞发生算法</p>
</blockquote>
<p>还有更好的方式，前面我们讲过「快速重传算法」。当接收方发现丢了一个中间包的时候，发送三次冗余 ACK，于是发送端就会快速地重传，不必等待超时再重传。</p>
<p>TCP 认为这种情况不严重，因为大部分没丢，只丢了一小部分，则 <code>ssthresh</code> 和 <code>cwnd</code> 变化如下：</p>
<ul>
<li><code>cwnd = 原cwnd/2</code> ，也就是设置为原来的一半;</li>
<li><code>ssthresh = 原cwnd/2</code>，跟超时重传一致，变为原拥塞窗口大小的一半</li>
<li>进入快速恢复算法</li>
</ul>
<p><em>快速恢复</em></p>
<p>快速重传和快速恢复算法一般同时使用，快速恢复算法是认为，你还能收到 3 个重复 ACK 说明网络也不那么糟糕，所以没有必要像 <code>RTO</code> 超时那么强烈。</p>
<p>正如前面所说，进入快速恢复之前，<code>cwnd</code> 和 <code>ssthresh</code> 已被更新了：</p>
<ul>
<li><code>cwnd = cwnd/2</code> ，也就是设置为原来的一半;</li>
<li><code>ssthresh = 原cwnd/2</code>;</li>
</ul>
<p>然后，进入快速恢复算法如下：</p>
<ul>
<li>拥塞窗口 <code>cwnd = ssthresh + 3</code> （ 3 的意思是确认有 3 个数据包被收到了）；</li>
<li>重传丢失的数据包；</li>
<li>如果再收到重复的 ACK，那么 cwnd 增加 1；</li>
<li>如果收到新数据的 ACK 后，把 cwnd 设置为第一步中的 ssthresh 的值，原因是该 ACK 确认了新的数据，说明从 duplicated ACK 时的数据都已收到，该恢复过程已经结束，可以回到恢复之前的状态了，也即再次进入拥塞避免状态；</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/fastrecover.png" alt="快速重传和快速恢复"></p>
<p><strong>总结：</strong>发生超时重传时先慢启动再拥塞避免，发生快速重传时先快速恢复再拥塞避免。</p>
<h3 id="tcp为什么是面向字节流的"><a href="#tcp为什么是面向字节流的" class="headerlink" title="tcp为什么是面向字节流的"></a>tcp为什么是面向字节流的</h3><blockquote>
<p>先来说说为什么 UDP 是面向报文的协议？</p>
</blockquote>
<p>当用户消息通过 UDP 协议传输时，<strong>操作系统不会对消息进行拆分</strong>，在组装好 UDP 头部后就交给网络层来处理，所以发出去的 UDP 报文中的数据部分就是完整的用户消息，也就是<strong>每个 UDP 报文就是一个用户消息的边界</strong>，这样接收方在接收到 UDP 报文后，读一个 UDP 报文就能读取到完整的用户消息。</p>
<blockquote>
<p>再来说说为什么 TCP 是面向字节流的协议？</p>
</blockquote>
<p>当用户消息通过 TCP 协议传输时，<strong>发送的报文最小单位是字节而不是以消息为单位</strong>。<strong>消息可能会被操作系统分组成多个的 TCP 报文</strong>，也就是一个完整的用户消息被拆分成多个 TCP 报文进行传输。</p>
<p>这时，接收方的程序如果不知道发送方发送的消息的长度，也就是不知道消息的边界时，是无法读出一个有效的用户消息的，因为用户消息被拆分成多个 TCP 报文后，并不能像 UDP 那样，一个 UDP 报文就能代表一个完整的用户消息。假设发送端陆续调用 send 函数先后发送 「hi.」和「how are you」 报文，那么实际的发送很有可能是这几种情况。1：hi how are you一起发送 2：先发hi how，再发are you 3: 先发h，再发i how are you</p>
<p>当两个消息的某个部分内容被分到同一个 TCP 报文时，就是我们常说的 TCP 粘包问题，这时接收方不知道消息的边界的话，是无法读出有效的消息。</p>
<p>要解决这个问题，要交给<strong>应用程序</strong>。</p>
<h4 id="如何解决粘包"><a href="#如何解决粘包" class="headerlink" title="如何解决粘包"></a><em>如何解决粘包</em></h4><p>粘包的问题出现是因为不知道一个用户消息的边界在哪，如果知道了边界在哪，接收方就可以通过边界来划分出有效的用户消息。</p>
<p>一般有三种方式分包的方式：</p>
<ul>
<li>固定长度的消息；</li>
<li>特殊字符作为边界；</li>
<li>自定义消息结构。</li>
</ul>
<p><em>固定长度的消息</em></p>
<p>这种是最简单方法，即每个用户消息都是固定长度的，比如规定一个消息的长度是 64 个字节，当接收方接满 64 个字节，就认为这个内容是一个完整且有效的消息。</p>
<p>但是这种方式灵活性不高，实际中很少用。</p>
<p><em>特殊字符作为边界</em></p>
<p>我们可以在两个用户消息之间插入一个特殊的字符串，这样接收方在接收数据时，读到了这个特殊字符，就把认为已经读完一个完整的消息。</p>
<p>HTTP 是一个非常好的例子。</p>
<p>HTTP 通过设置空格、回车符、换行符作为 HTTP 报文协议的边界。</p>
<p>有一点要注意，这个作为边界点的特殊字符，如果刚好消息内容里有这个特殊字符，我们要对这个字符转义，避免被接收方当作消息的边界点而解析到无效的数据。</p>
<p><em>自定义消息结构</em></p>
<p>我们可以自定义一个消息结构，由包头和数据组成，其中包头包是固定大小的，而且包头里有一个字段来说明紧随其后的数据有多大。</p>
<p>比如这个消息结构体，首先 4 个字节大小的变量来表示数据长度，真正的数据则在后面。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> &#123;</span> </span><br><span class="line">    <span class="type">u_int32_t</span> message_length; </span><br><span class="line">    <span class="type">char</span> message_data[]; </span><br><span class="line">&#125; message;</span><br></pre></td></tr></table></figure>

<p>当接收方接收到包头的大小（比如 4 个字节）后，就解析包头的内容，于是就可以知道数据的长度，然后接下来就继续读取数据，直到读满数据的长度，就可以组装成一个完整到用户消息来处理了。</p>
<h3 id="tcp中的各种异常情况"><a href="#tcp中的各种异常情况" class="headerlink" title="tcp中的各种异常情况"></a>tcp中的各种异常情况</h3><h4 id="已建立连接的TCP，收到SYN会发生什么？"><a href="#已建立连接的TCP，收到SYN会发生什么？" class="headerlink" title="已建立连接的TCP，收到SYN会发生什么？"></a><em>已建立连接的TCP，收到SYN会发生什么？</em></h4><p>TCP 连接是由「四元组」唯一确认的。这个场景中，客户端的IP、服务端IP、目的端口并没有变化，所以这个问题关键要看客户端发送的 <strong>SYN 报文中的源端口是否和上一次连接的源端口相同</strong>。</p>
<p><strong>1. 客户端的 SYN 报文里的端口号与历史连接不相同</strong></p>
<p>如果客户端恢复后发送的 SYN 报文中的源端口号跟上一次连接的源端口号不一样，此时服务端会认为是新的连接要建立，于是就<strong>会通过三次握手来建立新的连接。</strong></p>
<p>那旧连接里处于 establish 状态的服务端最后会怎么样呢？</p>
<p>如果服务端发送了数据包给客户端，由于客户端的连接已经被关闭了，此时客户的内核就会回 RST 报文，服务端收到后就会释放连接。</p>
<p>如果服务端一直没有发送数据包给客户端，在超过一段时间后， TCP 保活机制就会启动，检测到客户端没有存活后，接着服务端就会释放掉该连接。</p>
<p><strong>2. 客户端的 SYN 报文里的端口号与历史连接相同</strong></p>
<p>如果客户端恢复后，发送的 SYN 报文中的源端口号跟上一次连接的源端口号一样，也就是处于 establish 状态的服务端收到了这个 SYN 报文。</p>
<p>处于 establish 状态的服务端如果收到了客户端的 SYN 报文（注意此时的 SYN 报文其实是乱序的，因为 SYN 报文的初始化序列号其实是一个随机数），**会回复一个携带了正确序列号和确认号的 ACK 报文(也就是之前连接中最后发送的ACK报文)**，这个 ACK 被称之为 Challenge ACK。</p>
<p>接着，客户端收到这个 Challenge ACK，发现序列号并不是自己期望收到的，于是就会回 RST 报文，服务端收到后，就会释放掉该连接。</p>
<p><strong>这个有漏洞，可能会导致黑客伪造RST报文进行攻击</strong>，因为黑客可以伪造成跟客户端一样的端口和ip地址发送一个SYN报文，按照上述所说服务端会回复一个携带了正确序列号和确认号的 ACK 报文，<strong>说白了，就是可以通过这一步拿到服务端下一次预期接收的序列号。</strong></p>
<p>然后黑客用这个确认号作为 RST 报文的序列号，发送给服务端，此时服务端会认为这个 RST 报文里的序列号是合法的，于是就会释放跟客户端的连接</p>
<h4 id="在-TCP-正常挥手过程中，处于-TIME-WAIT-状态的连接，收到相同四元组的-SYN-后会发生什么？"><a href="#在-TCP-正常挥手过程中，处于-TIME-WAIT-状态的连接，收到相同四元组的-SYN-后会发生什么？" class="headerlink" title="在 TCP 正常挥手过程中，处于 TIME_WAIT 状态的连接，收到相同四元组的 SYN 后会发生什么？"></a><em>在 TCP 正常挥手过程中，处于 TIME_WAIT 状态的连接，收到相同四元组的 SYN 后会发生什么？</em></h4><p>如果双方开启了时间戳机制：</p>
<ul>
<li>如果客户端的 SYN 的「序列号」比服务端「期望下一个收到的序列号」要<strong>大</strong>，<strong>并且</strong>SYN 的「时间戳」比服务端「最后收到的报文的时间戳」要<strong>大</strong>。那么就会重用该四元组连接，跳过 2MSL 而转变为 SYN_RECV 状态，接着就能进行建立连接过程。</li>
<li>如果客户端的 SYN 的「序列号」比服务端「期望下一个收到的序列号」要<strong>小</strong>，<strong>或者</strong>SYN 的「时间戳」比服务端「最后收到的报文的时间戳」要<strong>小</strong>。那么就会<strong>再回复一个第四次挥手的 ACK 报文，客户端收到后，发现并不是自己期望收到确认号，就回 RST 报文给服务端</strong>。</li>
</ul>
<p>在 TIME_WAIT 状态，收到 RST 会断开连接吗？</p>
<ul>
<li>如果 <code>net.ipv4.tcp_rfc1337</code> 参数为 0，则提前结束 TIME_WAIT 状态，释放连接。</li>
<li>如果 <code>net.ipv4.tcp_rfc1337</code> 参数为 1，则会丢掉该 RST 报文。</li>
</ul>
<h4 id="拔网线，断电，进程崩溃时会发生什么"><a href="#拔网线，断电，进程崩溃时会发生什么" class="headerlink" title="拔网线，断电，进程崩溃时会发生什么"></a><em>拔网线，断电，进程崩溃时会发生什么</em></h4><p><em>拔网线，断电等主机宕机情况</em></p>
<p>若服务端没有要向客户端发送的报文，过一段时间后会触发保活机制，当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，<strong>TCP 会报告该 TCP 连接已经死亡并断开连接</strong>。应用程序若想使用 TCP 保活机制需要通过 socket 接口设置 <code>SO_KEEPALIVE</code> 选项才能够生效，如果没有设置，那么就无法使用 TCP 保活机制。</p>
<p>若服务端有要向客户端发送的报文并且客户端没有重启，在客户端主机宕机后，服务端向客户端发送的报文会得不到任何的响应，在一定时长后，服务端就会触发<strong>超时重传</strong>机制，重传未得到响应的报文。<strong>在重传报文且一直没有收到对方响应的情况时，先达到「最大重传次数」或者「最大超时时间」这两个的其中一个条件后，就会停止重传并断开连接</strong>。</p>
<p>若客户端宕机后重启，客户端的内核就会接收重传的报文，然后根据报文的信息传递给对应的进程：</p>
<ul>
<li>如果客户端主机上<strong>没有</strong>进程监听该 TCP 报文的目标端口号，那么客户端内核就会<strong>回复 RST 报文，重置该 TCP 连接</strong>；</li>
<li>如果客户端主机上<strong>有</strong>进程监听该 TCP 报文的目标端口号，由于客户端主机重启后，之前的 TCP 连接的数据结构已经丢失了，客户端内核里协议栈会发现找不到该 TCP 连接的 socket 结构体，于是就会<strong>回复 RST 报文，重置该 TCP 连接</strong>。</li>
</ul>
<p>所以，只要有一方重启完成后，收到之前 TCP 连接的报文，都会<strong>回复 RST 报文，以断开连接</strong></p>
<p><em>进程崩溃情况</em></p>
<p><strong>在 kill 掉进程后，服务端会发送 FIN 报文，与客户端进行四次挥手</strong>。即使没有开启 TCP keepalive，且双方也没有数据交互的情况下，如果其中一方的进程发生了崩溃，这个过程操作系统是可以感知的到的，于是就会发送 FIN 报文给对方，然后与对方进行 TCP 四次挥手。</p>
<h4 id="重启-TCP-服务进程时，为什么会有“Address-in-use”的报错信息？"><a href="#重启-TCP-服务进程时，为什么会有“Address-in-use”的报错信息？" class="headerlink" title="重启 TCP 服务进程时，为什么会有“Address in use”的报错信息？"></a>重启 TCP 服务进程时，为什么会有“Address in use”的报错信息？</h4><p>当我们重启 TCP 服务进程的时候，意味着通过服务器端发起了关闭连接操作，于是就会经过四次挥手，而对于主动关闭方，会在 TIME_WAIT 这个状态里停留一段时间，这个时间大约为 2MSL。</p>
<p><strong>当 TCP 服务进程重启时，服务端会出现 TIME_WAIT 状态的连接，TIME_WAIT 状态的连接使用的 IP+PORT 仍然被认为是一个有效的 IP+PORT 组合，相同机器上不能够在该 IP+PORT 组合上进行绑定，那么执行 bind() 函数的时候，就会返回了 Address already in use 的错误</strong>。</p>
<p>而等 TIME_WAIT 状态的连接结束后，重启 TCP 服务进程就能成功。</p>
<p>解决办法：</p>
<p>我们可以在调用 bind 前，对 socket 设置 SO_REUSEADDR 属性，可以解决这个问题。(<strong>跟net.ipv4.tcp_tw_reuse不同</strong>，那个用于客户端connect，这个用于服务端bind)</p>
<p>SO_REUSEADDR 作用是<strong>：如果当前启动进程绑定的 IP+PORT 与处于TIME_WAIT 状态的连接占用的 IP+PORT 存在冲突，但是新启动的进程使用了 SO_REUSEADDR 选项，那么该进程就可以绑定成功。</strong></p>
<h3 id="tcp的缺陷"><a href="#tcp的缺陷" class="headerlink" title="tcp的缺陷"></a>tcp的缺陷</h3><h4 id="升级-TCP-的工作很困难"><a href="#升级-TCP-的工作很困难" class="headerlink" title="升级 TCP 的工作很困难"></a>升级 TCP 的工作很困难</h4><p> TCP 协议是在内核中实现的，应用程序只能使用不能修改，如果要想升级 TCP 协议，那么只能升级内核。</p>
<p>而升级内核这个工作是很麻烦的事情，麻烦的事情不是说升级内核这个操作很麻烦，而是由于内核升级涉及到底层软件和运行库的更新，我们的服务程序就需要回归测试是否兼容新的内核版本，所以服务器的内核升级也比较保守和缓慢。</p>
<p>很多 TCP 协议的新特性，都是需要客户端和服务端同时支持才能生效的，比如 TCP Fast Open 这个特性，虽然在2013 年就被提出了，但是 Windows 很多系统版本依然不支持它，这是因为 PC 端的系统升级滞后很严重，Windows Xp 现在还有大量用户在使用，尽管它已经存在快 20 年。</p>
<p>所以，即使 TCP 有比较好的特性更新，也很难快速推广，用户往往要几年或者十年才能体验到。</p>
<h4 id="TCP-建立连接的延迟"><a href="#TCP-建立连接的延迟" class="headerlink" title="TCP 建立连接的延迟"></a>TCP 建立连接的延迟</h4><p>基于 TCP 实现的应用协议，都是需要先建立三次握手才能进行数据传输，比如 HTTP 1.0&#x2F;1.1、HTTP&#x2F;2、HTTPS。</p>
<p>现在大多数网站都是使用 HTTPS 的，这意味着在 TCP 三次握手之后，还需要经过 TLS 四次握手后，才能进行 HTTP 数据的传输，这在一定程序上增加了数据传输的延迟。</p>
<p>解决方法：开启TCP FAST OPEN，使用TLS1.3</p>
<h4 id="TCP-存在队头阻塞问题"><a href="#TCP-存在队头阻塞问题" class="headerlink" title="TCP 存在队头阻塞问题"></a>TCP 存在队头阻塞问题</h4><p>TCP 是字节流协议，<strong>TCP 层必须保证收到的字节数据是完整且有序的</strong>，如果序列号较低的 TCP 段在网络传输中丢失了，即使序列号较高的 TCP 段已经被接收了，应用层也无法从内核中读取到这部分数据，而必须等到丢失的TCP段重传并接收后才可以。</p>
<p>这就是 TCP 队头阻塞问题，但这也不能怪 TCP ，因为只有这样做才能保证数据的有序性。</p>
<p>HTTP&#x2F;2 多个请求是跑在一个 TCP 连接中的，那么当 TCP 丢包时，整个 TCP 都要等待重传，那么就会阻塞该 TCP 连接中的所有请求，所以 HTTP&#x2F;2 队头阻塞问题就是因为 TCP 协议导致的。</p>
<h4 id="网络迁移需要重新建立-TCP-连接"><a href="#网络迁移需要重新建立-TCP-连接" class="headerlink" title="网络迁移需要重新建立 TCP 连接"></a>网络迁移需要重新建立 TCP 连接</h4><p>基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接。</p>
<p>那么<strong>当移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立 TCP 连接</strong>。</p>
<p>而建立连接的过程包含 TCP 三次握手和 TLS 四次握手的时延，以及 TCP 慢启动的减速过程，给用户的感觉就是网络突然卡顿了一下，因此连接的迁移成本是很高的。</p>
<h2 id="UDP"><a href="#UDP" class="headerlink" title="UDP"></a>UDP</h2><p><img src="C:\Users\hufei\Desktop\udp.png" alt="UDP 头部格式"></p>
<ul>
<li>目标和源端口：主要是告诉 UDP 协议应该把报文发给哪个进程。</li>
<li>包长度：该字段保存了 UDP 首部的长度跟数据的长度之和。</li>
<li>校验和：校验和是为了提供可靠的 UDP 首部和数据而设计，防止收到在网络传输中受损的 UDP包。</li>
</ul>
<h3 id="udp和tcp区别"><a href="#udp和tcp区别" class="headerlink" title="udp和tcp区别"></a>udp和tcp区别</h3><p><em>1. 连接</em></p>
<ul>
<li>TCP 是面向连接的传输层协议，传输数据前先要建立连接。</li>
<li>UDP 是不需要连接，即刻传输数据。</li>
</ul>
<p><em>2. 服务对象</em></p>
<ul>
<li>TCP 是一对一的两点服务，即一条连接只有两个端点。</li>
<li>UDP 支持一对一、一对多、多对多的交互通信</li>
</ul>
<p><em>3. 可靠性</em></p>
<ul>
<li>TCP 是可靠交付数据的，数据可以无差错、不丢失、不重复、按需到达。</li>
<li>UDP 是尽最大努力交付，不保证可靠交付数据。</li>
</ul>
<p><em>4. 拥塞控制、流量控制</em></p>
<ul>
<li>TCP 有拥塞控制和流量控制机制，保证数据传输的安全性。</li>
<li>UDP 则没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率。</li>
</ul>
<p><em>5. 首部开销</em></p>
<ul>
<li>TCP 首部长度较长，会有一定的开销，首部在没有使用「选项」字段时是 <code>20</code> 个字节，如果使用了「选项」字段则会变长的。</li>
<li>UDP 首部只有 8 个字节，并且是固定不变的，开销较小。</li>
</ul>
<p><em>6. 传输方式</em></p>
<ul>
<li>TCP 是流式传输，没有边界，但保证顺序和可靠。</li>
<li>UDP 是一个包一个包的发送，是有边界的，但可能会丢包和乱序。</li>
</ul>
<p><em>7. 分片不同</em></p>
<ul>
<li>TCP 的数据大小如果大于 MSS 大小，则会在传输层进行分片，目标主机收到后，也同样在传输层组装 TCP 数据包，如果中途丢失了一个分片，只需要传输丢失的这个分片。</li>
<li>UDP 的数据大小如果大于 MTU 大小，则会在 IP 层进行分片，目标主机收到后，在 IP 层组装完数据，接着再传给传输层。</li>
</ul>
<p><em>应用场景差别：</em></p>
<p>由于 TCP 是面向连接，能保证数据的可靠性交付，因此经常用于：</p>
<ul>
<li><code>FTP</code> 文件传输；</li>
<li>HTTP &#x2F; HTTPS；</li>
</ul>
<p>由于 UDP 面向无连接，它可以随时发送数据，再加上UDP本身的处理既简单又高效，因此经常用于：</p>
<ul>
<li>包总量较少的通信，如 <code>DNS</code> 、<code>SNMP</code> 等(udp包长度16位，加上首部最多只能传65535字节数据)；</li>
<li>视频、音频等多媒体通信；</li>
<li>广播通信；</li>
</ul>
<h3 id="如何基于udp实现可靠传输"><a href="#如何基于udp实现可靠传输" class="headerlink" title="如何基于udp实现可靠传输"></a>如何基于udp实现可靠传输</h3><p>把 TCP 可靠传输的特性（序列号、确认应答、超时重传、流量控制、拥塞控制）在应用层实现一遍。</p>
<p>实现的思路确实这样没错，<strong>但既然 TCP 天然支持可靠传输，为什么还需要基于 UDP 实现可靠传输呢？这不是重复造轮子吗？</strong></p>
<p>现在市面上已经有基于 UDP 协议实现的可靠传输协议的成熟方案了，那就是 QUIC 协议，已经应用在了 HTTP&#x2F;3。QUIC解决了TCP的几个缺陷</p>
<h4 id="QUIC协议"><a href="#QUIC协议" class="headerlink" title="QUIC协议"></a>QUIC协议</h4><p><em>QUIC首部</em></p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/http3-over-quic-protocol-works.png" alt="img"></p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/packet%20header.png" alt="Packet Header"></p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/QUICPACKET.png" alt="img"></p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/frame%20header.png" alt="img"></p>
<p><em>QUIC实现可靠传输</em></p>
<p>QUIC也有序列号，确认应答和重传机制，但相比TCP做了一些改进。</p>
<p>QUIC新增连接ID字段用于区分不同的连接，QUIC 也是需要三次握手来建立连接的，主要目的是为了协商连接 ID。协商出连接 ID 后，后续传输时，双方只需要固定住连接 ID，从而实现连接迁移功能。</p>
<p>QUIC新增 <code>Packet Number</code>用于区别不同报文，这 是每个报文独一无二的编号，它是<strong>严格递增</strong>的，也就是说就算 Packet N 丢失了，重传的 Packet N 的 Packet Number 已经不是 N，而是一个比 N 大的值。</p>
<p>这样设计有两个好处：</p>
<ul>
<li>可以更加精确计算 RTT，没有 TCP 重传的歧义性问题；(tcp「原始报文的响应」和「重传报文的响应」确认号一致，客户端无法判断，这样在计算 RTT 时应该选择从发送原始报文开始计算，还是重传原始报文开始计算呢)</li>
<li>可以支持乱序确认，因为丢包重传将当前窗口阻塞在原地，而 TCP 必须是顺序确认的，丢包时会导致窗口不滑动；</li>
</ul>
<p>QUIC新增Stream ID，Offset用于给数据排序，每个数据包首部包括Stream ID，Offset，Length</p>
<ul>
<li>Stream ID 作用：多个并发传输的 HTTP 消息，通过不同的 Stream ID 加以区别，类似于 HTTP2 的 Stream ID；</li>
<li>Offset 作用：类似于 TCP 协议中的 Seq 序号，<strong>保证数据的顺序性和可靠性</strong>；</li>
<li>Length 作用：指明了 Frame 数据的长度。</li>
</ul>
<p><strong>通过 Stream ID + Offset 字段信息实现数据的有序性</strong>，通过比较两个数据包的 Stream ID 与 Stream Offset ，如果都是一致，就说明这两个数据包的内容一致。</p>
<p><em>QUIC如何解决队头阻塞</em></p>
<p>TCP 发送出去的数据，都是需要按序确认的，只有在数据都被按顺序确认完后，发送窗口才会往前滑动。</p>
<p>同理TCP接收窗口收到有序数据时，接收窗口才能往前滑动，然后那些已经接收并且被确认的「有序」数据就可以被应用层读取。</p>
<ul>
<li>停留「发送窗口」会使得发送方无法继续发送数据。</li>
<li>停留「接收窗口」会使得应用层无法读取新的数据。</li>
</ul>
<p>这就是TCP的队头阻塞问题</p>
<p>QUIC 借鉴 HTTP&#x2F;2 里的 Stream 的概念，一个 Stream 就代表 HTTP&#x2F;1.1 里的请求和响应，在一条 QUIC 连接上可以并发发送多个 HTTP 请求 (Stream)。</p>
<p>但是 <strong>QUIC 给每一个 Stream 都分配了一个独立的滑动窗口，这样使得一个连接上的多个 Stream 之间没有依赖关系，都是相互独立的，各自控制的滑动窗口</strong>。</p>
<p>假如 Stream2 丢了一个 UDP 包，也只会影响 Stream2 的处理，不会影响其他 Stream，与 HTTP&#x2F;2 不同，HTTP&#x2F;2 只要某个流中的数据包丢失了，其他流也会因此受影响。</p>
<p><em>QUIC建立连接更快</em></p>
<p>对于 HTTP&#x2F;1 和 HTTP&#x2F;2 协议，TCP 和 TLS 是分层的，分别属于内核实现的传输层、openssl 库实现的表示层，因此它们难以合并在一起，需要分批次来握手，先 TCP 握手（1RTT），再 TLS 握手（2RTT），所以需要 3RTT 的延迟才能传输数据，就算 Session 会话服用，也需要至少 2 个 RTT。</p>
<p>HTTP&#x2F;3 在传输数据前虽然需要 QUIC 协议握手，这个握手过程只需要 1 RTT，握手的目的是为确认双方的「连接 ID」，连接迁移就是基于连接 ID 实现的。</p>
<p>但是 HTTP&#x2F;3 的 QUIC 协议并不是与 TLS 分层，而是<strong>QUIC 内部包含了 TLS，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 TLS1.3，因此仅需 1 个 RTT 就可以「同时」完成建立连接与密钥协商，甚至在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果</strong>。</p>
<p><em>QUIC迁移连接</em></p>
<p>基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接。那么<strong>当移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立 TCP 连接</strong>。</p>
<p>QUIC 协议没有用四元组的方式来“绑定”连接，而是通过<strong>连接 ID</strong>来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本，没有丝毫卡顿感，达到了<strong>连接迁移</strong>的功能。</p>
<h2 id="IP"><a href="#IP" class="headerlink" title="IP"></a>IP</h2><h3 id="ip首部"><a href="#ip首部" class="headerlink" title="ip首部"></a>ip首部</h3><p><img src="https://raw.githubusercontent.com/hufei96/Image/main/ipheader.png" alt="img"></p>
<p>第一行：<br>（1）版本号（Version），4位；用于标识IP协议版本，IPv4是0100，IPv6是0110，也就是二进制的4和6。<br>（2）首部长度（Internet Header Length），4位；用于标识首部的长度，单位为4字节，所以首部长度最大值为：(2^4 - 1) * 4 &#x3D; 60字节，但一般只推荐使用20字节的固定长度。<br>（3）服务类型（Type Of Service），8位；用于标识IP包的优先级，但现在并未使用。<br>（4）总长度（Total Length），16位；标识IP数据报的总长度，最大为：2^16 -1 &#x3D; 65535字节。<br>第二行：<br>（1）标识（Identification），16位；用于标识IP数据报，如果因为数据链路层帧数据段长度限制（也就是MTU，支持的最大传输单元），IP数据报需要进行分片发送，则每个分片的IP数据报标识都是一致的。<br>（2）标志（Flag），3位，但目前只有2位有意义；最低位为MF，MF&#x3D;1代表后面还有分片的数据报，MF&#x3D;0代表当前数据报已是最后的数据报。次低位为DF，DF&#x3D;1代表不能分片，DF&#x3D;0代表可以分片。<br>（3）片偏移（Fragment Offset），13位；代表某个分片在原始数据中的相对位置。<br>第三行：<br>（1）生存时间（TTL），8位；以前代表IP数据报最大的生存时间，现在标识IP数据报可以经过的路由器数。<br>（2）协议（Protocol），8位；代表上层传输层协议的类型，1代表ICMP，2代表IGMP，6代表TCP，17代表UDP。<br>（3）校验和（Header Checksum），16位；用于**验证首部(不包括数据)**完整性，计算方法为，首先将校验和位置零，然后将每16位二进制反码求和即为校验和，最后写入校验和位置。<br>第四行：源IP地址<br>第五行：目的IP地址</p>
<h4 id="为什么数据链路层做了差错检测，ip还要做"><a href="#为什么数据链路层做了差错检测，ip还要做" class="headerlink" title="为什么数据链路层做了差错检测，ip还要做"></a>为什么数据链路层做了差错检测，ip还要做</h4><p>从现在看来，这确实是个多余的设计，ipv6已经取消了差错检测</p>
<p>但设计之初数据链路层的协议并未统一用以太网，ip不知道数据链路层是否会做差错检测，所以ip进行了差错检测。</p>
<h3 id="ip地址"><a href="#ip地址" class="headerlink" title="ip地址"></a>ip地址</h3><h4 id="ipv4"><a href="#ipv4" class="headerlink" title="ipv4"></a>ipv4</h4><p>IP 地址（IPv4 地址）由 <code>32</code> 位正整数来表示，IP 地址在计算机是以二进制的方式处理的。</p>
<p>而人类为了方便记忆采用了<strong>点分十进制</strong>的标记方式，也就是将 32 位 IP 地址以每 8 位为组，共分为 <code>4</code> 组，每组以「<code>.</code>」隔开，再将每组转换成十进制。</p>
<h5 id="IP-地址的分类"><a href="#IP-地址的分类" class="headerlink" title="IP 地址的分类"></a>IP 地址的分类</h5><p><img src="https://raw.githubusercontent.com/hufei96/Image/main/ipaddress.jpg" alt="IP 地址分类"></p>
<p>其中对于 A、B、C 类主要分为两个部分，分别是<strong>网络号和主机号</strong>。</p>
<p>而 D 类和 E 类地址是没有主机号的，所以不可用于主机 IP，D 类常被用于<strong>多播</strong>，E 类是预留的分类，暂时未使用。</p>
<h5 id="特殊的地址"><a href="#特殊的地址" class="headerlink" title="特殊的地址"></a>特殊的地址</h5><p>主机号全为 1 指定某个网络下的所有主机，用于广播</p>
<p>主机号全为 0 指定某个网络</p>
<p>0.0.0.0。它表示的是这样一个集合：所有不清楚的主机和目的网络。这里的“不清楚”是指在本机的路由表里没有特定条目指明如何到达。对本机来说，它就是一个“收容所”，所有不认识的“三无”人员，一律送进去。如果你在网络设置中设置了缺省网关，那么Windows系统会自动产生一个目的地址为0.0.0.0的缺省路由。</p>
<p>255.255.255.255。限制广播地址，对本机来说，这个地址指本网段内(同一广播域)的所有主机。这个地址不能被路由器转发</p>
<p>127.0.0.1。本地回环地址，主要用于测试。在Windows系统中，这个地址有一个别名“Localhost”。这个地址只用于本机测试，不会发送到网络上。</p>
<h5 id="广播和多播"><a href="#广播和多播" class="headerlink" title="广播和多播"></a>广播和多播</h5><blockquote>
<p>广播地址用于什么？</p>
</blockquote>
<p>广播地址用于在<strong>同一个链路中相互连接的主机之间发送数据包</strong>。</p>
<p>学校班级中就有广播的例子，在准备上课的时候，通常班长会喊：“上课， 全体起立！”，班里的同学听到这句话是不是全部都站起来了？这个句话就有广播的含义。</p>
<p>当主机号全为 1 时，就表示该网络的广播地址。</p>
<p>广播地址可以分为本地广播和直接广播两种。</p>
<ul>
<li><strong>在本网络内广播的叫做本地广播</strong>。例如网络地址为 192.168.0.0&#x2F;24 的情况下，广播地址是 192.168.0.255 。因为这个广播地址的 IP 包会被路由器屏蔽，所以不会到达 192.168.0.0&#x2F;24 以外的其他链路上。</li>
<li><strong>在不同网络之间的广播叫做直接广播</strong>。例如网络地址为 192.168.0.0&#x2F;24 的主机向 192.168.1.255&#x2F;24 的目标地址发送 IP 包。收到这个包的路由器，将数据转发给 192.168.1.0&#x2F;24，从而使得所有 192.168.1.1~192.168.1.254 的主机都能收到这个包（由于直接广播有一定的安全问题，多数情况下会在路由器上设置为不转发。） 。</li>
</ul>
<blockquote>
<p>多播地址用于什么？</p>
</blockquote>
<p>多播用于<strong>将包发送给特定组内的所有主机。</strong></p>
<p>还是举班级的栗子，老师说：“最后一排的同学，上来做这道数学题。”，老师指定的是最后一排的同学，也就是多播的含义了。</p>
<p>由于<strong>广播无法穿透路由，若想给其他网段发送同样的包，就可以使用可以穿透路由的多播</strong>。</p>
<p>多播使用的 D 类地址，其前四位是 <code>1110</code> 就表示是多播地址，而剩下的 28 位是多播的组编号。</p>
<p>从 224.0.0.0 ~ 239.255.255.255 都是多播的可用范围，其划分为以下三类：</p>
<ul>
<li>224.0.0.0 ~ 224.0.0.255 为预留的组播地址，只能在局域网中，路由器是不会进行转发的。</li>
<li>224.0.1.0 ~ 238.255.255.255 为用户可用的组播地址，可以用于 Internet 上。</li>
<li>239.0.0.0 ~ 239.255.255.255 为本地管理组播地址，可供内部网在内部使用，仅在特定的本地范围内有效。</li>
</ul>
<p><strong>组播地址不是用于机器ip地址</strong>的，因为组播地址没有网络号和主机号，所以跟dhcp没关系。组播地址一般是用于udp协议，机器发送UDP组播数据时，目标地址填的是组播地址，那么在组播组内的机器都能收到数据包。</p>
<p>是否加入组播组和离开组播组，是由socket一个接口实现的，<strong>主机ip是不用改变的</strong>。</p>
<h5 id="无分类地址-CIDR和子网"><a href="#无分类地址-CIDR和子网" class="headerlink" title="无分类地址 CIDR和子网"></a>无分类地址 CIDR和子网</h5><p>IP 分类存在许多缺点，如主机分配不够灵活，C 类地址能包含的最大主机数量实在太少了，而 B 类地址能包含的最大主机数量又太多了。同时，同一网络下没有地址层次，比如一个公司里用了 B 类地址，但是可能需要根据生产环境、测试环境、开发环境来划分地址层次，而这种 IP 分类是没有地址层次划分的功能</p>
<p>所以后面提出了无分类地址的方案，即 <code>CIDR</code>，以及子网。</p>
<p>这种方式不再有分类地址的概念，32 比特的 IP 地址被划分为两部分，前面是<strong>网络号</strong>，后面是<strong>主机号</strong>。表示形式 <code>a.b.c.d/x</code>，其中 <code>/x</code> 表示前 x 位属于<strong>网络号</strong>， x 的范围是 <code>0 ~ 32</code>，这就使得 IP 地址更加具有灵活性。</p>
<p>还有另一种划分网络号与主机号形式，那就是<strong>子网掩码</strong>，掩码的意思就是掩盖掉主机号，剩余的就是网络号。<strong>将子网掩码和 IP 地址按位计算 AND，就可得到网络号。</strong></p>
<p>子网掩码还有一个作用，那就是<strong>划分子网</strong>。将主机地址分为两个部分：子网网络地址和子网主机地址</p>
<h5 id="NAT"><a href="#NAT" class="headerlink" title="NAT"></a>NAT</h5><p>IPv4 的地址是非常紧缺的，在前面我们也提到可以通过无分类地址来减缓 IPv4 地址耗尽的速度，但是互联网的用户增速是非常惊人的，所以 IPv4 地址依然有被耗尽的危险。</p>
<p>于是，提出了一种<strong>网络地址转换 NAT</strong> 的方法，再次缓解了 IPv4 地址耗尽的问题。</p>
<p>简单的来说 NAT 就是同个公司、家庭、教室内的主机对外部通信时，把私有 IP 地址转换成公有 IP 地址。多个私有地址统一用同一个公有地址来传输数据。</p>
<p>由于绝大多数的网络应用都是使用传输层协议 TCP 或 UDP 来传输数据的，因此可用端口号来对不同的私有地址进行区分，在通信时给不同的私有地址分配不同的端口号。</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/nat.jpg" alt="NAPT"></p>
<p>由于 NAT&#x2F;NAPT 都依赖于自己的转换表，因此会有以下的问题：</p>
<ul>
<li>外部无法主动与 NAT 内部服务器建立连接，因为 NAPT 转换表没有转换记录。</li>
<li>转换表的生成与转换操作都会产生性能开销。</li>
<li>通信过程中，如果 NAT 路由器重启了，所有的 TCP 连接都将被重置。</li>
</ul>
<p>解决的方法主要有两种方法。</p>
<p><em>第一种就是改用 IPv6</em></p>
<p>IPv6 可用范围非常大，以至于每台设备都可以配置一个公有 IP 地址，就不搞那么多花里胡哨的地址转换了，但是 IPv6 普及速度还需要一些时间。</p>
<p><em>第二种 NAT 内网穿透技术</em></p>
<p>借助一个有公网ip的云服务器。</p>
<p>内网机器向云服务器建立一个长连接，然后云服务器就可以主动向内网机器传数据。云服务器将自己某端口的数据转发到内网机器上，然后客户端访问云服务器的那个端口就可以访问内网机器了。</p>
<h5 id="DHCP"><a href="#DHCP" class="headerlink" title="DHCP"></a>DHCP</h5><p>DHCP 在生活中我们是很常见的了，我们的电脑通常都是通过 DHCP 动态获取 IP 地址，大大省去了配 IP 信息繁琐的过程。</p>
<ul>
<li>客户端首先发起 <strong>DHCP 发现报文（DHCP DISCOVER）</strong> 的 IP 数据报，由于客户端没有 IP 地址，也不知道 DHCP 服务器的地址，所以使用的是 UDP <strong>广播</strong>通信，其使用的广播目的地址是 255.255.255.255（端口 67） 并且使用 0.0.0.0（端口 68） 作为源 IP 地址。DHCP 客户端将该 IP 数据报传递给链路层，链路层然后将帧广播到所有的网络中设备。</li>
<li>DHCP 服务器收到 DHCP 发现报文时，用 <strong>DHCP 提供报文（DHCP OFFER）</strong> 向客户端做出响应。该报文仍然使用 IP 广播地址 255.255.255.255，该报文信息携带服务器提供可租约的 IP 地址、子网掩码、默认网关、DNS 服务器以及 <strong>IP 地址租用期</strong>。</li>
<li>客户端收到一个或多个服务器的 DHCP 提供报文后，从中选择一个服务器，并向选中的服务器发送 <strong>DHCP 请求报文（DHCP REQUEST</strong>进行响应，回显配置的参数。</li>
<li>最后，服务端用 <strong>DHCP ACK 报文</strong>对 DHCP 请求报文进行响应，应答所要求的参数。</li>
</ul>
<p>DHCP 交互中，<strong>全程都是使用 UDP 广播通信</strong>，而路由器不会转发广播包，为了解决这个问题就出现了 <strong>DHCP 中继代理</strong>。有了 DHCP 中继代理以后，<strong>对不同网段的 IP 地址分配也可以由一个 DHCP 服务器统一进行管理。</strong></p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/dhcprelay.jpg" alt=" DHCP 中继代理"></p>
<ul>
<li>DHCP 客户端会向 DHCP 中继代理发送 DHCP 请求包，而 DHCP 中继代理在收到这个广播包以后，再以<strong>单播</strong>的形式发给 DHCP 服务器。</li>
<li>服务器端收到该包以后再向 DHCP 中继代理返回应答，并由 DHCP 中继代理将此包广播给 DHCP 客户端 。</li>
</ul>
<p>因此，DHCP 服务器即使不在同一个链路上也可以实现统一分配和管理IP地址。</p>
<p><em>DHCP和RARP的区别</em></p>
<p>1.RARP可以满足主机IP地址配置的部分要求，但是不能完全满足</p>
<p>包括但不限于以下配置：网络掩码，网关地址，静态路由，DNS服务器，以及私有的，公有的option功能。</p>
<p>2.RARP是二层协议，无法穿透子网，DHCP可以穿透子网。如果用RARP来分配地址的话，需要在每个网段(子网)内部署一台RARP Server,管理难度太大。</p>
<p>3.RARP只能对地址进行识别，无法进行地址的统一规划和分配。</p>
<h5 id="ICMP"><a href="#ICMP" class="headerlink" title="ICMP"></a>ICMP</h5><p>ICMP 全称是 <strong>Internet Control Message Protocol</strong>，也就是<strong>互联网控制报文协议</strong>。</p>
<p>网络包在复杂的网络传输环境里，常常会遇到各种问题。</p>
<p>当遇到问题的时候，总不能死个不明不白，没头没脑的作风不是计算机网络的风格。所以<strong>需要传出消息，报告遇到了什么问题</strong>，这样才可以调整传输策略，以此来控制整个局面。</p>
<p><code>ICMP</code> 主要的功能包括：<strong>确认 IP 包是否成功送达目标地址、报告发送过程中 IP 包被废弃的原因和改善网络设置等。</strong></p>
<p>在 <code>IP</code> 通信中如果某个 <code>IP</code> 包因为某种原因未能达到目标地址，那么这个具体的原因将<strong>由 ICMP 负责通知</strong>。</p>
<blockquote>
<p>ICMP 类型</p>
</blockquote>
<p>ICMP 大致可以分为两大类：</p>
<ul>
<li>一类是用于诊断的查询消息，也就是「<strong>查询报文类型</strong>」</li>
<li>另一类是通知出错原因的错误消息，也就是「<strong>差错报文类型</strong>」</li>
</ul>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/icmp.jpg" alt="常见的 ICMP 类型"></p>
<h5 id="IGMP"><a href="#IGMP" class="headerlink" title="IGMP"></a>IGMP</h5><p>在前面我们知道了组播地址，也就是 D 类地址，既然是组播，那就说明是只有一组的主机能收到数据包，不在一组的主机不能收到数组包，怎么管理是否是在一组呢？那么，就需要 <code>IGMP</code> 协议了。</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/igmp.jpg" alt="组播模型"></p>
<p><strong>IGMP 是因特网组管理协议，工作在主机（组播成员）和最后一跳路由之间</strong>，如上图中的蓝色部分。同网络的路由器负责管理主机加入和退出组播组</p>
<ul>
<li>IGMP 报文向路由器申请加入和退出组播组，默认情况下路由器是不会转发组播包到连接中的主机，除非主机通过 IGMP 加入到组播组，主机申请加入到组播组时，路由器就会记录 IGMP 路由器表，路由器后续就会转发组播包到对应的主机了。</li>
<li>IGMP 报文采用 IP 封装，IP 头部的协议号为 2，而且 TTL 字段值通常为 1，因为 IGMP 是工作在主机与连接的路由器之间。</li>
</ul>
<h4 id="ipv6"><a href="#ipv6" class="headerlink" title="ipv6"></a>ipv6</h4><p>IPv4 的地址是 32 位的，大约可以提供 42 亿个地址，但是早在 2011 年 IPv4 地址就已经被分配完了。</p>
<p>但是 IPv6 的地址是 <code>128</code> 位的，这可分配的地址数量是大的惊人，说个段子 <strong>IPv6 可以保证地球上的每粒沙子都能被分配到一个 IP 地址。</strong></p>
<p>但 IPv6 除了有更多的地址之外，还有更好的安全性和扩展性，说简单点就是 IPv6 相比于 IPv4 能带来更好的网络体验。</p>
<p>但是因为 <strong>IPv4 和 IPv6 不能相互兼容</strong>，所以不但要我们电脑、手机之类的设备支持，还需要网络运营商对现有的设备进行升级，所以这可能是 IPv6 普及率比较慢的一个原因。</p>
<h5 id="ipv6和ipv4的差别"><a href="#ipv6和ipv4的差别" class="headerlink" title="ipv6和ipv4的差别"></a>ipv6和ipv4的差别</h5><p><img src="https://raw.githubusercontent.com/hufei96/Image/main/ipv4andipv6.jpg" alt="IPv4 首部与 IPv6 首部的差异"></p>
<p>IPv4 地址长度共 32 位，是以每 8 位作为一组，并用点分十进制的表示方式。IPv6 地址长度是 128 位，是以每 16 位作为一组，每组用冒号 「:」 隔开。</p>
<p>IPv6 的地址主要有以下类型地址：</p>
<ul>
<li>单播地址，用于一对一的通信</li>
<li>组播地址，用于一对多的通信</li>
<li>任播地址，用于通信最近的节点，最近的节点是由路由协议决定</li>
<li><strong>没有广播地址</strong></li>
</ul>
<p>对于一对一通信的 IPv6 地址，主要划分了三类单播地址，每类地址的有效范围都不同。</p>
<ul>
<li>在同一链路单播通信，不经过路由器，可以使用<strong>链路本地单播地址</strong>，IPv4 没有此类型</li>
<li>在内网里单播通信，可以使用<strong>唯一本地地址</strong>，相当于 IPv4 的私有 IP</li>
<li>在互联网通信，可以使用<strong>全局单播地址</strong>，相当于 IPv4 的公有 IP</li>
</ul>
<p>IPv6 相比 IPv4 的首部改进：</p>
<ul>
<li><strong>取消了首部校验和字段。</strong> 因为在数据链路层和传输层都会校验，因此 IPv6 直接取消了 IP 的校验。</li>
<li><strong>取消了分片&#x2F;重新组装相关字段。</strong> 分片与重组是耗时的过程，IPv6 不允许在中间路由器进行分片与重组，这种操作只能在源与目标主机，这将大大提高了路由器转发的速度。</li>
<li><strong>取消选项字段。</strong> 选项字段不再是标准 IP 首部的一部分了，但它并没有消失，而是可能出现在 IPv6 首部中的「下一个首部」指出的位置上。删除该选项字段使的 IPv6 的首部成为固定长度的 <code>40</code> 字节。</li>
</ul>
<h5 id="ipv6优点"><a href="#ipv6优点" class="headerlink" title="ipv6优点"></a>ipv6优点</h5><ul>
<li><p>可分配地址变多，可以保证地球上的每粒沙子都能被分配到一个 IP 地址</p>
</li>
<li><p>IPv6 可自动配置，即使没有 DHCP 服务器也可以实现自动分配IP地址，<strong>便捷到即插即用</strong>。</p>
</li>
<li><p>IPv6 包头包首部长度采用固定的值 <code>40</code> 字节，去掉了差错检测，简化了首部结构，禁止分片，减轻了路由器负荷，大大<strong>提高了传输的性能</strong>。</p>
</li>
<li><p>IPv6 有应对伪造 IP 地址的网络安全功能以及防止线路窃听的功能，大大<strong>提升了安全性</strong>。</p>
</li>
</ul>
<h2 id="RPC"><a href="#RPC" class="headerlink" title="RPC"></a>RPC</h2><p>RPC（Remote Procedure Call），又叫做远程过程调用。它本身并不是一个具体的协议，而是一种调用方式，一种思想。</p>
<p>举个例子，我们平时调用一个本地方法就像下面这样。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res = <span class="built_in">localFunc</span>(req)<span class="number">1.</span></span><br></pre></td></tr></table></figure>

<p>如果现在这不是个本地方法，而是个远端服务器暴露出来的一个方法remoteFunc，如果我们还能像调用本地方法那样去调用它，这样就可以屏蔽掉一些网络细节，用起来更方便，岂不美哉？</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res = <span class="built_in">remoteFunc</span>(req)</span><br></pre></td></tr></table></figure>

<p>这样跨机器调用必须用到网络编程才能实现，RPC 帮助我们屏蔽网络编程细节，实现调用远程方法就跟调用本地一样的体验，我们不需要因为这个方法是远程调用就需要编写很多与业务无关的代码。</p>
<p>RPC 是一个远程调用，需要通过网络来传输数据，并且 RPC 常用于业务系统之间的数据交互，需要保证其可靠性，所以 RPC 一般默认采用 TCP 来传输。我们常用的 HTTP 协议也是建立在 TCP 之上的。</p>
<p>调用方持续地把请求参数序列化成二进制后，经过 TCP 传输给了服务提供方。服务提供方从 TCP 通道里面收到二进制数据，那如何知道一个请求的数据到哪里结束，是一个什么类型的请求呢？这里就需要用到相关协议，而协议的作用就是做相关规定和约束。</p>
<p>将对象保存到文件，或者通过网络传输给对方，都是需要将对象转换二进制数据才能完成，那么<strong>对象转二进制数据的过程就是序列化</strong>，相反的，<strong>二进制数据转对象的过程就是反序列化的过程</strong>。</p>
<p>JSON 序列化大多数语言都支持，JSON 优势是使用起来简单，容易阅读，应用广泛，缺点就是不适合大数据量的场景；</p>
<p>ProtoBuf 使用需要定义 IDL 文件，序列化后体积相比 JSON 小很多，现在很多大公司都在用，gRPC 框架使用 protobuf 序列化。</p>
<p>大多数的协议会分成两部分，分别是数据头和消息体。数据头一般用于身份识别，包括协议标识、数据大小、请求类型、序列化类型等信息；消息体主要是请求的业务参数信息和扩展属性等。</p>
<p>根据协议格式，服务提供方就可以正确地从二进制数据中分割出不同的请求来，同时根据请求类型和序列化类型，把二进制的消息体逆向还原成请求对象。这个过程叫作“反序列化”。</p>
<p>服务提供方再根据反序列化出来的请求对象找到对应的实现类，完成真正的方法调用，然后把执行结果序列化后，回写到对应的 TCP 通道里面。调用方获取到应答的数据包后，再反序列化成应答对象，这样调用方就完成了一次 RPC 调用。</p>
<h2 id="网络攻击"><a href="#网络攻击" class="headerlink" title="网络攻击"></a>网络攻击</h2><p>浏览器安全可以分为三大块：<strong>Web页面安全、浏览器网络安全、浏览器系统安全</strong></p>
<p>Web页面安全主要就是同源策略限制</p>
<p><strong>什么是同源策略</strong></p>
<p>最初，它的含义是指，A网页设置的 Cookie，B网页不能打开，除非这两个网页”同源”。所谓”同源”指的是”三个相同”，也就是我们访问站点的：<code>协议</code>、<code>域名</code>、<code>端口号</code>必须一至，才叫<code>同源</code>。</p>
<p>同源策略的目的，是为了保证用户信息的安全，防止恶意的网站窃取数据。设想这样一种情况：A网站是一家银行，用户登录以后，又去浏览其他网站。如果其他网站可以读取A网站的 Cookie，会发生什么？很显然，如果 Cookie 包含隐私（比如存款总额），这些信息就会泄漏。更可怕的是，Cookie 往往用来保存用户的登录状态，如果用户没有退出登录，其他网站就可以冒充用户，为所欲为。</p>
<p>随着互联网的发展，”同源政策”越来越严格。目前，如果非同源，共有三种行为受到限制。</p>
<ol>
<li><strong>DOM层面</strong>：不同源站点之间不能相互访问和操作DOM</li>
<li><strong>数据层面</strong>：不能获取不同源站点的Cookie、LocalStorage、indexDB等数据</li>
<li><strong>网络层面</strong>：不能向不同源站点发送ajax请求</li>
</ol>
<p>浏览器收到响应数据之后，会判断响应回数据的源和当前页面的源是否是属于同源。针对不同源，如果后端没有对响应字段进行处理，则响应回的数据会被浏览器直接过滤掉。</p>
<p>当然<strong>同源策略限制也不是绝对隔离不同源的站点</strong>，比如link、img、script标签都没有跨域限制，这让我们开发更灵活了，但是也同样带来了一些安全问题，也就是<strong>浏览器网络安全</strong>问题，最典型的就是XSS攻击和CSRF攻击</p>
<h3 id="XSS攻击"><a href="#XSS攻击" class="headerlink" title="XSS攻击"></a>XSS攻击</h3><h4 id="什么是-XSS"><a href="#什么是-XSS" class="headerlink" title="什么是 XSS"></a><strong>什么是 XSS</strong></h4><p>Cross-Site Scripting（跨站脚本攻击）简称 XSS，是一种代码注入攻击。攻击者通过在目标网站上注入恶意脚本，使之在用户的浏览器上运行。利用这些恶意脚本，攻击者可获取用户的敏感信息如 Cookie、SessionID 等，进而危害数据安全。为了和 CSS 区分，这里把攻击的第一个字母改成了 X，于是叫做 XSS。</p>
<p>XSS 的本质是：恶意代码未经过滤，与网站正常的代码混在一起；浏览器无法分辨哪些脚本是可信的，导致恶意脚本被执行。而由于直接在用户的终端执行，恶意代码能够直接获取用户的信息，或者利用这些信息冒充用户向网站发起攻击者定义的请求。在部分情况下，由于输入的限制，注入的恶意脚本比较短。但可以通过引入外部的脚本，并由浏览器执行，来完成比较复杂的攻击策略。</p>
<p>这里有一个问题：用户是通过哪种方法“注入”恶意脚本的呢？</p>
<p>不仅仅是业务上的“用户的 UGC 内容”可以进行注入，包括 URL 上的参数等都可以是攻击的来源。在处理输入时，以下内容都不可信：</p>
<ul>
<li>来自用户的 UGC 信息</li>
<li>来自第三方的链接</li>
<li>URL 参数</li>
<li>POST 参数</li>
<li>Referer （可能来自不可信的来源）</li>
<li>Cookie （可能来自其他子域注入）</li>
</ul>
<h4 id="XSS-分类"><a href="#XSS-分类" class="headerlink" title="XSS 分类"></a><strong>XSS 分类</strong></h4><p>根据攻击的来源，XSS 攻击可分为存储型、反射型和 DOM 型三种</p>
<p>XSS攻击有三种类型：<strong>存储型</strong>、<strong>反射型</strong>、<strong>DOM型</strong></p>
<p><strong>存储型 XSS</strong></p>
<p>存储型 XSS 的攻击步骤：</p>
<ol>
<li>攻击者将恶意代码提交到目标网站的数据库中。如<code>&lt;script src=&quot;http://恶意网站&quot;&gt;&lt;/script&gt;</code></li>
<li>用户打开目标网站时，网站服务端将恶意代码从数据库取出，拼接在 HTML 中返回给浏览器。</li>
<li>用户浏览器接收到响应后解析执行，混在其中的恶意代码也被执行。</li>
<li>恶意代码窃取用户数据并发送到攻击者的网站，或者冒充用户的行为，调用目标网站接口执行攻击者指定的操作。</li>
</ol>
<p>这种攻击常见于带有用户保存数据的网站功能，如论坛发帖、商品评论、用户私信等。</p>
<p><strong>反射型XSS</strong></p>
<p>反射型 XSS 的攻击步骤：</p>
<ol>
<li>攻击者构造出特殊的 URL，其中包含恶意代码。</li>
<li>用户打开带有恶意代码的 URL 时，网站服务端将恶意代码从 URL 中取出，拼接在 HTML 中返回给浏览器。</li>
<li>用户浏览器接收到响应后解析执行，混在其中的恶意代码也被执行。</li>
<li>恶意代码窃取用户数据并发送到攻击者的网站，或者冒充用户的行为，调用目标网站接口执行攻击者指定的操作。</li>
</ol>
<p>反射型 XSS 跟存储型 XSS 的区别是：存储型 XSS 的恶意代码存在数据库里，反射型 XSS 的恶意代码存在 URL 里。</p>
<p>反射型 XSS 漏洞常见于通过 URL 传递参数的功能，如网站搜索、跳转等。由于需要用户主动打开恶意的 URL 才能生效，攻击者往往会结合多种手段诱导用户点击。</p>
<p><strong>DOM 型 XSS</strong></p>
<p>DOM 型 XSS 的攻击步骤：</p>
<ol>
<li>攻击者构造出特殊的 URL，其中包含恶意代码。</li>
<li>用户打开带有恶意代码的 URL。</li>
<li>用户浏览器接收到响应后解析执行，前端 JavaScript 取出 URL 中的恶意代码并执行。</li>
<li>恶意代码窃取用户数据并发送到攻击者的网站，或者冒充用户的行为，调用目标网站接口执行攻击者指定的操作。</li>
</ol>
<p>DOM 型 XSS 跟前两种 XSS 的区别：DOM 型 XSS 攻击中，取出和执行恶意代码由浏览器端完成，属于前端 JavaScript 自身的安全漏洞，而其他两种 XSS 都属于服务端的安全漏洞。</p>
<h4 id="XSS-攻击的预防"><a href="#XSS-攻击的预防" class="headerlink" title="XSS 攻击的预防"></a><strong>XSS 攻击的预防</strong></h4><p>通过前面的介绍可以得知，XSS 攻击有两大要素：</p>
<ol>
<li>攻击者提交恶意代码。</li>
<li>浏览器执行恶意代码。</li>
</ol>
<p>针对第一个要素：我们是否能够在用户输入的过程，过滤掉用户输入的恶意代码呢？</p>
<h5 id="输入过滤"><a href="#输入过滤" class="headerlink" title="输入过滤"></a><strong>输入过滤</strong></h5><p>在用户提交时，由前端过滤输入，然后提交到后端。这样做是否可行呢？答案是不可行。一旦攻击者绕过前端过滤，直接构造请求，就可以提交恶意代码了。</p>
<p>那么，换一个过滤时机：后端在写入数据库前，对输入进行过滤，然后把“安全的”内容，返回给前端。这样是否可行呢？</p>
<p>我们举一个例子，一个正常的用户输入了 <code>5 &lt; 7</code> 这个内容，在写入数据库前，被转义，变成了 &#96;&#96;。问题是：在提交阶段，我们并不确定内容要输出到哪里。</p>
<p>这里的“并不确定内容要输出到哪里”有两层含义：</p>
<ol>
<li><p>用户的输入内容可能同时提供给前端和客户端，而一旦经过了 <code>escapeHTML()</code>，客户端显示的内容就变成了乱码( <code>5 &lt; 7</code> )。</p>
</li>
<li><p>在前端中，不同的位置所需的编码也不同。</p>
<ul>
<li>当 <code>5 &amp;lt; 7</code> 作为 HTML 拼接页面时，可以正常显示：</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">title</span>=<span class="string">&quot;comment&quot;</span>&gt;</span>5 <span class="symbol">&amp;lt;</span> 7<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>当 <code>5 &amp;lt; 7</code> 通过 Ajax 返回，然后赋值给 JavaScript 的变量时，前端得到的字符串就是转义后的字符。这个内容不能直接用于 Vue 等模板的展示，也不能直接用于内容长度计算。不能用于标题、alert 等。</li>
</ul>
</li>
</ol>
<p>所以，输入侧过滤能够在某些情况下解决特定的 XSS 问题，但会引入很大的不确定性和乱码问题。<strong>在防范 XSS 攻击时应避免此类方法</strong>。</p>
<p>当然，对于明确的输入类型，例如数字、URL、电话号码、邮件地址等等内容，进行输入过滤还是必要的。</p>
<h5 id="防止浏览器执行恶意代码"><a href="#防止浏览器执行恶意代码" class="headerlink" title="防止浏览器执行恶意代码"></a><strong>防止浏览器执行恶意代码</strong></h5><p>既然输入过滤并非完全可靠，我们就要通过“防止浏览器执行恶意代码”来防范 XSS。这部分分为两类：</p>
<ul>
<li>防止 HTML 中出现注入。</li>
<li>防止 JavaScript 执行时，执行恶意代码。</li>
</ul>
<p><strong>预防存储型和反射型 XSS 攻击</strong></p>
<p>存储型和反射型 XSS 都是在服务端取出恶意代码后，插入到响应 HTML 里的，攻击者刻意编写的“数据”被内嵌到“代码”中，被浏览器所执行。</p>
<p>预防这两种漏洞，有两种常见做法：</p>
<ul>
<li>改成纯前端渲染，把代码和数据分隔开。</li>
<li>对 HTML 做充分转义。</li>
</ul>
<p><strong>纯前端渲染</strong></p>
<p>纯前端渲染的过程：</p>
<ol>
<li>浏览器先加载一个静态 HTML，此 HTML 中不包含任何跟业务相关的数据。</li>
<li>然后浏览器执行 HTML 中的 JavaScript。</li>
<li>JavaScript 通过 Ajax 加载业务数据，调用 DOM API 更新到页面上。</li>
</ol>
<p>在纯前端渲染中，我们会明确的告诉浏览器：下面要设置的内容是文本（<code>.innerText</code>），还是属性（<code>.setAttribute</code>），还是样式（<code>.style</code>）等等。浏览器不会被轻易的被欺骗，执行预期外的代码了。</p>
<p>但纯前端渲染还需注意避免 DOM 型 XSS 漏洞（例如 <code>onload</code> 事件和 <code>href</code> 中的 <code>javascript:xxx</code> 等，请参考下文”预防 DOM 型 XSS 攻击“部分）。</p>
<p>在很多内部、管理系统中，采用纯前端渲染是非常合适的。但对于性能要求高，或有 SEO 需求的页面，我们仍然要面对拼接 HTML 的问题。</p>
<p><strong>转义 HTML</strong></p>
<p>如果拼接 HTML 是必要的，就需要采用合适的转义库，对 HTML 模板各处插入点进行充分的转义。</p>
<p>常用的模板引擎，如 doT.js、ejs、FreeMarker 等，对于 HTML 转义通常只有一个规则，就是把 <code>&amp; &lt; &gt; &quot; &#39; /</code> 这几个字符转义掉，确实能起到一定的 XSS 防护作用，但并不完善，我们要使用更完善更细致的转义策略。例如 Java 工程里，常用的转义库为 <code>org.owasp.encoder</code>。</p>
<p><strong>预防 DOM 型 XSS 攻击</strong></p>
<p>DOM 型 XSS 攻击，实际上就是网站前端 JavaScript 代码本身不够严谨，把不可信的数据当作代码执行了。</p>
<p>在使用 <code>.innerHTML</code>、<code>.outerHTML</code>、<code>document.write()</code> 时要特别小心，不要把不可信的数据作为 HTML 插到页面上，而应尽量使用 <code>.textContent</code>、<code>.setAttribute()</code> 等。</p>
<p>如果用 Vue&#x2F;React 技术栈，并且不使用 <code>v-html</code>&#x2F;<code>dangerouslySetInnerHTML</code> 功能，就在前端 render 阶段避免 <code>innerHTML</code>、<code>outerHTML</code> 的 XSS 隐患。</p>
<p>DOM 中的内联事件监听器，如 <code>location</code>、<code>onclick</code>、<code>onerror</code>、<code>onload</code>、<code>onmouseover</code> 等，<code>&lt;a&gt;</code> 标签的 <code>href</code> 属性，JavaScript 的 <code>eval()</code>、<code>setTimeout()</code>、<code>setInterval()</code> 等，都能把字符串作为代码运行。如果不可信的数据拼接到字符串中传递给这些 API，很容易产生安全隐患，请务必避免。</p>
<p><strong>其他安全措施</strong></p>
<ul>
<li>HTTP-only Cookie: 禁止 JavaScript 读取某些敏感 Cookie，攻击者完成 XSS 注入后也无法窃取此 Cookie。</li>
<li>验证码：防止脚本冒充用户提交危险操作。</li>
<li>输入内容长度控制：对于不受信任的输入，都应该限定一个合理的长度。虽然无法完全防止 XSS 发生，但可以增加 XSS 攻击的难度。</li>
</ul>
<h3 id="CSRF攻击"><a href="#CSRF攻击" class="headerlink" title="CSRF攻击"></a>CSRF攻击</h3><p>CSRF（Cross-site request forgery）跨站请求伪造：攻击者诱导受害者进入第三方网站，在第三方网站中，向被攻击网站发送跨站请求。利用受害者在被攻击网站已经获取的注册凭证，绕过后台的用户验证，达到冒充用户对被攻击的网站执行某项操作的目的。</p>
<p>一个典型的CSRF攻击有着如下的流程：</p>
<ul>
<li>受害者登录a.com，并保留了登录凭证（Cookie）。</li>
<li>攻击者引诱受害者访问了b.com。</li>
<li>b.com 向 a.com 发送了一个请求：a.com&#x2F;act&#x3D;xx。浏览器会默认携带a.com的Cookie。</li>
<li>a.com接收到请求后，对请求进行验证，并确认是受害者的凭证，误以为是受害者自己发送的请求。</li>
<li>a.com以受害者的名义执行了act&#x3D;xx。</li>
<li>攻击完成，攻击者在受害者不知情的情况下，冒充受害者，让a.com执行了自己定义的操作。</li>
</ul>
<h4 id="常见的攻击类型"><a href="#常见的攻击类型" class="headerlink" title="常见的攻击类型"></a><strong>常见的攻击类型</strong></h4><p><strong>GET类型的CSRF</strong></p>
<p>GET类型的CSRF利用非常简单，只需要一个HTTP请求，一般会这样利用：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2018b/ff0cdbee.example/withdraw?amount=10000&amp;for=hacker)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在受害者访问含有这个img的页面后，浏览器会自动向<code>http://bank.example/withdraw?account=xiaoming&amp;amount=10000&amp;for=hacker</code>发出一次HTTP请求。bank.example就会收到包含受害者登录信息的一次跨域请求。</p>
<p><strong>POST类型的CSRF</strong></p>
<p>这种类型的CSRF利用起来通常使用的是一个自动提交的表单，如：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">&quot;http://bank.example/withdraw&quot;</span> <span class="attr">method</span>=<span class="string">POST</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">&quot;hidden&quot;</span> <span class="attr">name</span>=<span class="string">&quot;account&quot;</span> <span class="attr">value</span>=<span class="string">&quot;xiaoming&quot;</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">&quot;hidden&quot;</span> <span class="attr">name</span>=<span class="string">&quot;amount&quot;</span> <span class="attr">value</span>=<span class="string">&quot;10000&quot;</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">&quot;hidden&quot;</span> <span class="attr">name</span>=<span class="string">&quot;for&quot;</span> <span class="attr">value</span>=<span class="string">&quot;hacker&quot;</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"> <span class="variable language_">document</span>.<span class="property">forms</span>[<span class="number">0</span>].<span class="title function_">submit</span>(); </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span> </span><br></pre></td></tr></table></figure>

<p>访问该页面后，表单会自动提交，相当于模拟用户完成了一次POST操作。</p>
<p>POST类型的攻击通常比GET要求更加严格一点，但仍并不复杂。任何个人网站、博客，被黑客上传页面的网站都有可能是发起攻击的来源，后端接口不能将安全寄托在仅允许POST上面。</p>
<p><strong>链接类型的CSRF</strong></p>
<p>链接类型的CSRF并不常见，比起其他两种用户打开页面就中招的情况，这种需要用户点击链接才会触发。这种类型通常是在论坛中发布的图片中嵌入恶意链接，或者以广告的形式诱导用户中招，攻击者通常会以比较夸张的词语诱骗用户点击，例如：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;http://test.com/csrf/withdraw.php?amount=1000&amp;for=hacker&quot;</span> <span class="attr">taget</span>=<span class="string">&quot;_blank&quot;</span>&gt;</span></span><br><span class="line">重磅消息！！</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span>/&gt;</span></span><br></pre></td></tr></table></figure>

<p>由于之前用户登录了信任的网站A，并且保存登录状态，只要用户主动访问上面的这个PHP页面，则表示攻击成功。</p>
<h4 id="CSRF的特点"><a href="#CSRF的特点" class="headerlink" title="CSRF的特点"></a>CSRF的特点</h4><ul>
<li>攻击一般发起在第三方网站，而不是被攻击的网站。被攻击的网站无法防止攻击发生。</li>
<li>攻击利用受害者在被攻击网站的登录凭证，冒充受害者提交操作；而不是直接窃取数据。</li>
<li>整个过程攻击者并不能获取到受害者的登录凭证，仅仅是“冒用”。</li>
<li>跨站请求可以用各种方式：图片URL、超链接、CORS、Form提交等等。部分请求方式可以直接嵌入在第三方论坛、文章中，难以进行追踪。</li>
</ul>
<p>CSRF通常是跨域的，因为外域通常更容易被攻击者掌控。但是如果本域下有容易被利用的功能，比如可以发图和链接的论坛和评论区，攻击可以直接在本域下进行，而且这种攻击更加危险。</p>
<h4 id="防护策略"><a href="#防护策略" class="headerlink" title="防护策略"></a>防护策略</h4><p>CSRF通常从第三方网站发起，被攻击的网站无法防止攻击发生，只能通过增强自己网站针对CSRF的防护能力来提升安全性。</p>
<p>上文中讲了CSRF的两个特点：</p>
<ul>
<li>CSRF（通常）发生在第三方域名。</li>
<li>CSRF攻击者不能获取到Cookie等信息，只是使用。</li>
</ul>
<p>针对这两点，我们可以专门制定防护策略，如下：</p>
<ul>
<li>阻止不明外域的访问<ul>
<li>同源检测</li>
<li>Samesite Cookie</li>
</ul>
</li>
<li>提交时要求附加本域才能获取的信息<ul>
<li>CSRF Token</li>
<li>双重Cookie验证</li>
</ul>
</li>
</ul>
<p>以下我们对各种防护方法做详细说明。</p>
<p><strong>同源检测</strong></p>
<p>既然CSRF大多来自第三方网站，那么我们就直接禁止外域（或者不受信任的域名）对我们发起请求。</p>
<p>那么问题来了，我们如何判断请求是否来自外域呢？</p>
<p>在HTTP协议中，每一个异步请求都会携带两个Header，用于标记来源域名：</p>
<ul>
<li>Origin Header</li>
<li>Referer Header</li>
</ul>
<p>这两个Header在浏览器发起请求时，大多数情况会自动带上，并且不能由前端自定义内容。 服务器可以通过解析这两个Header中的域名，确定请求的来源域。</p>
<p>当Origin和Referer头文件不存在时该怎么办？如果Origin和Referer都不存在，建议直接进行阻止，特别是如果您没有使用随机CSRF Token（参考下方）作为第二次检查。</p>
<p><em>如何阻止外域请求</em></p>
<p>通过Header的验证，我们可以知道发起请求的来源域名，这些来源域名可能是网站本域，或者子域名，或者有授权的第三方域名，又或者来自不可信的未知域名。</p>
<p>我们已经知道了请求域名是否是来自不可信的域名，我们直接阻止掉这些的请求，就能防御CSRF攻击了吗？</p>
<p>且慢！当一个请求是页面请求（比如网站的主页），而来源是搜索引擎的链接（例如百度的搜索结果），也会被当成疑似CSRF攻击。所以在判断的时候需要过滤掉页面请求情况。</p>
<p>但相应的，页面请求就暴露在了CSRF的攻击范围之中。如果你的网站中，在页面的GET请求中对当前用户做了什么操作的话，防范就失效了。</p>
<p>例如，下面的页面请求：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET https://example.com/addComment?comment=XXX&amp;dest=orderId</span><br></pre></td></tr></table></figure>

<p>注：这种严格来说并不一定存在CSRF攻击的风险，但仍然有很多网站经常把主文档GET请求挂上参数来实现产品功能，但是这样做对于自身来说是存在安全风险的。</p>
<p>另外，前面说过，CSRF大多数情况下来自第三方域名，但并不能排除本域发起。如果攻击者有权限在本域发布评论（含链接、图片等，统称UGC），那么它可以直接在本域发起攻击，这种情况下同源策略无法达到防护的作用。</p>
<p>综上所述：同源验证是一个相对简单的防范方法，能够防范绝大多数的CSRF攻击。但这并不是万无一失的，对于安全性要求较高，或者有较多用户输入内容的网站，我们就要对关键的接口做额外的防护措施。</p>
<p><strong>CSRF Token</strong></p>
<p>前面讲到CSRF的另一个特征是，攻击者无法直接窃取到用户的信息（Cookie，Header，网站内容等），仅仅是冒用Cookie中的信息。</p>
<p>而CSRF攻击之所以能够成功，是因为服务器误把攻击者发送的请求当成了用户自己的请求。那么我们可以要求所有的用户请求都携带一个CSRF攻击者无法获取到的Token。服务器通过校验请求是否携带正确的Token，来把正常的请求和攻击的请求区分开，也可以防范CSRF的攻击。</p>
<p>用户打开页面的时候，服务器需要给这个用户生成一个Token，该Token通过加密算法对数据进行加密，一般Token都包括随机字符串和时间戳的组合，显然在提交时Token不能再放在Cookie中了，否则又会被攻击者冒用。因此，为了安全起见Token最好还是存在服务器的Session中，之后在每次页面加载时，使用JS遍历整个DOM树，对于DOM中所有的a和form标签后加入Token。这样可以解决大部分的请求，但是对于在页面加载之后动态生成的HTML代码，这种方法就没有作用，还需要程序员在编码时手动添加Token。</p>
<p>Token是一个比较有效的CSRF防护方法，只要页面没有XSS漏洞泄露Token，那么接口的CSRF攻击就无法成功。</p>
<p>但是此方法的实现比较复杂，需要给每一个页面都写入Token（前端无法使用纯静态页面），每一个Form及Ajax请求都携带这个Token，后端对每一个接口都进行校验，并保证页面Token及请求Token一致。这就使得这个防护策略不能在通用的拦截上统一拦截处理，而需要每一个页面和接口都添加对应的输出和校验。这种方法工作量巨大，且有可能遗漏。</p>
<!--验证码和密码其实也可以起到CSRF Token的作用哦，而且更安全。为什么很多银行等网站会要求已经登录的用户在转账时再次输入密码，现在是不是有一定道理了？-->

<p><strong>双重Cookie验证</strong></p>
<p>在会话中存储CSRF Token比较繁琐，而且不能在通用的拦截上统一处理所有的接口。</p>
<p>那么另一种防御措施是使用双重提交Cookie。利用CSRF攻击不能获取到用户Cookie的特点，我们可以要求Ajax和表单请求携带一个Cookie中的值。</p>
<p>双重Cookie采用以下流程：</p>
<ul>
<li>在用户访问网站页面时，向请求域名注入一个Cookie字段，内容为随机字符串（例如<code>csrfcookie=v8g9e4ksfhw</code>）。</li>
<li>在前端向后端发起请求时，取出Cookie字段，并添加到URL的参数中（接上例<code>POST https://www.a.com/comment?csrfcookie=v8g9e4ksfhw</code>）。</li>
<li>后端接口验证Cookie中的字段与URL参数中的字段是否一致，不一致则拒绝。</li>
</ul>
<p>此方法相对于CSRF Token就简单了许多。可以直接通过前后端拦截的的方法自动化实现。后端校验也更加方便，只需进行请求中字段的对比，而不需要再进行查询和存储Token。</p>
<p>当然，此方法并没有大规模应用，其在大型网站上的安全性还是没有CSRF Token高，原因我们举例进行说明。</p>
<p>由于任何跨域都会导致前端无法获取Cookie中的字段（包括子域名之间），于是发生了如下情况：</p>
<ul>
<li>如果用户访问的网站为<code>www.a.com</code>，而后端的api域名为<code>api.a.com</code>。那么在<code>www.a.com</code>下，前端拿不到<code>api.a.com</code>的Cookie，也就无法完成双重Cookie认证。</li>
<li>于是这个认证Cookie必须被种在<code>a.com</code>下，这样每个子域都可以访问。</li>
<li>任何一个子域都可以修改<code>a.com</code>下的Cookie。</li>
<li>某个子域名存在漏洞被XSS攻击（例如<code>upload.a.com</code>）。虽然这个子域下并没有什么值得窃取的信息。但攻击者修改了<code>a.com</code>下的Cookie。</li>
<li>攻击者可以直接使用自己配置的Cookie，对XSS中招的用户再向<code>www.a.com</code>下，发起CSRF攻击。</li>
</ul>
<p><strong>总结：</strong></p>
<p><strong>用双重Cookie防御CSRF的优点：</strong></p>
<ul>
<li>无需使用Session，适用面更广，易于实施。</li>
<li>cookie储存于客户端中，不会给服务器带来压力。</li>
<li>相对于Token，实施成本更低，可以在前后端统一拦截校验，而不需要一个个接口和页面添加。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>Cookie中增加了额外的字段。</li>
<li>如果有其他漏洞（例如XSS），攻击者可以注入Cookie，那么该防御方式失效。</li>
<li>难以做到子域名的隔离。</li>
<li>为了确保Cookie传输安全，采用这种防御方式的最好确保用整站HTTPS的方式，如果还没切HTTPS的使用这种方式也会有风险。</li>
</ul>
<p><strong>Samesite Cookie属性</strong></p>
<p>防止CSRF攻击的办法已经有上面的预防措施。为了从源头上解决这个问题，Google起草了一份草案来改进HTTP协议，那就是为Set-Cookie响应头新增Samesite属性，它用来标明这个 Cookie是个“同站 Cookie”，同站Cookie只能作为第一方Cookie，不能作为第三方Cookie，Samesite 有两个属性值，分别是 Strict 和 Lax。</p>
<p>如果SamesiteCookie被设置为Strict，浏览器在任何跨域请求中都不会携带Cookie，新标签重新打开也不携带，所以说CSRF攻击基本没有机会。假如淘宝网站用来识别用户登录与否的 Cookie 被设置成了 Samesite&#x3D;Strict，那么用户从百度搜索页面甚至天猫页面的链接点击进入淘宝后，淘宝都不会是登录状态，因为淘宝的服务器不会接受到那个 Cookie，其它网站发起的对淘宝的任意请求都不会带上那个 Cookie。</p>
<p>这样跳转子域名或者是新标签重新打开刚登陆的网站，之前的Cookie都不会存在。尤其是有登录的网站，那么我们新打开一个标签进入，或者跳转到子域名的网站，都需要重新登录。对于用户来讲，可能体验不会很好。</p>
<p>如果SamesiteCookie被设置为Lax，假如这个请求是这种请求（改变了当前页面或者打开了新页面）且同时是个GET请求，则这个Cookie可以作为第三方Cookie。那么其他网站通过页面跳转过来的时候可以使用Cookie，可以保障外域连接打开页面时用户的登录状态。但相应的，其安全性也比较低。</p>
<p>另外一个问题是Samesite的兼容性不是很好，现阶段除了从新版Chrome和Firefox支持以外，Safari以及iOS Safari都还不支持，现阶段看来暂时还不能普及。</p>
<p>而且，SamesiteCookie目前有一个致命的缺陷：不支持子域。例如，种在topic.a.com下的Cookie，并不能使用a.com下种植的SamesiteCookie。这就导致了当我们网站有多个子域名时，不能使用SamesiteCookie在主域名存储用户登录信息。每个子域名都需要用户重新登录一次。</p>
<p>总之，SamesiteCookie是一个可能替代同源验证的方案，但目前还并不成熟，其应用场景有待观望。</p>
<h3 id="DDoS攻击"><a href="#DDoS攻击" class="headerlink" title="DDoS攻击"></a>DDoS攻击</h3><p>DDoS 攻击，全称是 Distributed Denial of Service，中文是分布式拒绝服务。一般来说是指攻击者利用“肉鸡”对目标网站在较短的时间内发起大量请求，大规模消耗目标网站的主机资源，让它无法正常服务。在线游戏、互联网金融等领域是 DDoS 攻击的高发行业。</p>
<h4 id="常见的DDoS攻击类型"><a href="#常见的DDoS攻击类型" class="headerlink" title="常见的DDoS攻击类型"></a>常见的DDoS攻击类型</h4><table>
<thead>
<tr>
<th align="left">DDoS攻击分类</th>
<th align="left">攻击子类</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">畸形报文</td>
<td align="left">畸形报文主要包括Frag Flood、Smurf、Stream Flood、Land Flood、IP畸形报文、TCP畸形报文、UDP畸形报文等。</td>
<td align="left">畸形报文攻击指通过向目标系统发送有缺陷的IP报文，使得目标系统在处理这样的报文时出现崩溃，从而达到拒绝服务的攻击目的。</td>
</tr>
<tr>
<td align="left">传输层DDoS攻击</td>
<td align="left">传输层DDoS攻击主要包括Syn Flood、Ack Flood、UDP Flood、ICMP Flood、RstFlood等。</td>
<td align="left">以Syn Flood攻击为例，它利用了TCP协议的三次握手机制，当服务端接收到一个Syn请求时，服务端必须使用一个监听队列将该连接保存一定时间。因此，通过向服务端不停发送Syn请求，但不响应Syn+Ack报文，从而消耗服务端的资源。当监听队列被占满时，服务端将无法响应正常用户的请求，达到拒绝服务攻击的目的。</td>
</tr>
<tr>
<td align="left">DNS DDoS攻击</td>
<td align="left">DNS DDoS攻击主要包括DNS Request Flood、DNS Response Flood、虚假源+真实源DNS Query Flood、权威服务器攻击和Local服务器攻击等。</td>
<td align="left">以DNS Query Flood攻击为例，其本质上执行的是真实的Query请求，属于正常业务行为。但如果多台傀儡机同时发起海量的域名查询请求，服务端无法响应正常的Query请求，从而导致拒绝服务。</td>
</tr>
<tr>
<td align="left">连接型DDoS攻击</td>
<td align="left">连接型DDoS攻击主要是指TCP慢速连接攻击、连接耗尽攻击、Loic、Hoic、Slowloris、 Pyloris、Xoic等慢速攻击。</td>
<td align="left">以Slowloris攻击为例，其攻击目标是Web服务器的并发上限。当Web服务器的连接并发数达到上限后，Web服务即无法接受新的请求。Web服务接收到新的HTTP请求时，建立新的连接来处理请求，并在处理完成后关闭这个连接。如果该连接一直处于连接状态，收到新的HTTP请求时则需要建立新的连接进行处理。而当所有连接都处于连接状态时，Web将无法处理任何新的请求。Slowloris攻击利用HTTP协议的特性来达到攻击目的。HTTP请求以<code>\r\n\r\n</code>标识Headers的结束，如果Web服务端只收到<code>\r\n</code>，则认为HTTP Headers部分没有结束，将保留该连接并等待后续的请求内容。</td>
</tr>
<tr>
<td align="left">Web应用层DDoS攻击</td>
<td align="left">Web应用层攻击主要是指HTTP Get Flood、HTTP Post Flood、CC等攻击。</td>
<td align="left">通常应用层攻击完全模拟用户请求，类似于各种搜索引擎和爬虫一样，这些攻击行为和正常的业务并没有严格的边界，难以辨别。Web服务中一些资源消耗较大的事务和页面。例如，Web应用中的分页和分表，如果控制页面的参数过大，频繁的翻页将会占用较多的Web服务资源。尤其在高并发频繁调用的情况下，类似这样的事务就成了早期CC攻击的目标。由于现在的攻击大都是混合型的，因此模拟用户行为的频繁操作都可以被认为是CC攻击。例如，各种刷票软件对网站的访问，从某种程度上来说就是CC攻击。CC攻击瞄准的是Web应用的后端业务，除了导致拒绝服务外，还会直接影响Web应用的功能和性能，包括Web响应时间、数据库服务、磁盘读写等。</td>
</tr>
</tbody></table>
<h3 id="sql注入"><a href="#sql注入" class="headerlink" title="sql注入"></a>sql注入</h3><p>SQL注入是属于注入式攻击，这种攻击是因为在项目中没有将代码与数据（比如用户敏感数据）隔离，在读取数据的时候，错误的将数据作为代码的一部分执行而导致的。</p>
<p>典型的例子就是当对SQL语句进行字符串拼接的时候，直接使用未转义的用户输入内容作为变量。这时，只要在sql语句的中间做修改，比如加上drop、delete等关键字，执行之后后果不堪设想。</p>
<p>说到这里，那么该怎么处理这种情况呢？三个方面：</p>
<p>1、过滤用户输入参数中的特殊字符，降低风险。</p>
<p>2、禁止通过字符串拼接sql语句，要严格使用参数绑定来传入参数。</p>
<p>3、合理使用数据库框架提供的机制。就比如Mybatis提供的传入参数的方式 #{}，禁止使用${}，后者相当于是字符串拼接sql，要使用参数化的语句。</p>
<p>总结下，就是要正确使用参数化绑定sql变量。</p>
<h2 id="CDN"><a href="#CDN" class="headerlink" title="CDN"></a>CDN</h2><p>CDN主要功能是在不同的地点缓存内容，通过负载均衡技术，将用户的请求定向到最合适的缓存服务器上去获取内容，比如说，是北京的用户，我们让他访问北京的节点，深圳的用户，我们让他访问深圳的节点。通过就近访问，加速用户对网站的访问。解决Internet网络拥堵状况，提高用户访问网络的响应速度。</p>
<p>最简单的CDN网络由一个DNS服务器和几台缓存服务器组成：</p>
<ol>
<li>当用户点击网站页面上的内容URL，经过本地DNS系统解析，DNS系统会最终将域名的解析权交给CNAME指向的CDN专用DNS服务器。</li>
<li>CDN的DNS服务器将CDN的全局负载均衡设备IP地址返回用户。</li>
<li>用户向CDN的全局负载均衡设备发起内容URL访问请求。</li>
<li>CDN全局负载均衡设备根据用户IP地址，以及用户请求的内容URL，选择一台用户所属区域的区域负载均衡设备，告诉用户向这台设备发起请求。</li>
<li>区域负载均衡设备会为用户选择一台合适的缓存服务器提供服务，选择的依据包括：根据用户IP地址，判断哪一台服务器距用户最近；根据用户所请求的URL中携带的内容名称，判断哪一台服务器上有用户所需内容；查询各个服务器当前的负载情况，判断哪一台服务器尚有服务能力。基于以上这些条件的综合分析之后，区域负载均衡设备会向全局负载均衡设备返回一台缓存服务器的IP地址。</li>
<li>全局负载均衡设备把服务器的IP地址返回给用户。</li>
<li>用户向缓存服务器发起请求，缓存服务器响应用户请求，将用户所需内容传送到用户终端。如果这台缓存服务器上并没有用户想要的内容，而区域均衡设备依然将它分配给了用户，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本地。</li>
</ol>
<h2 id="VPN和加速器"><a href="#VPN和加速器" class="headerlink" title="VPN和加速器"></a>VPN和加速器</h2><p>VPN 全称为虚拟私人网络(Virtual Private Network)，常用于连接中、大型企业或团体间私人网络的通讯方法，利用隧道协议（Tunneling Protocol）来达到发送端认证、消息保密与准确性等功能。</p>
<p>比如多地办公的公司，可以使用 VPN 将不同地区连接在同一内网下；或者在家办公的时候也可以通过 VPN 接入公司内网中。</p>
<p>VPN 以 CS 架构运行，工作流程如下：</p>
<p><img src="https://raw.githubusercontent.com/hufei96/Image/main/vpn.png" alt="vpn"></p>
<p>1.VPN工作流程</p>
<p>在外网的用户可以使用 <code>vpn client</code> 连接组织搭建的 <code>vpn server</code> 以建立通信隧道，随后便建立了虚拟的私人网络，处于外网的 <code>worker</code> 和内网中的 <code>server</code> 可以相互通信。</p>
<p>那么我们可以简单理解 VPN，由 <code>VPN client</code> 捕获用户发出的报文，封装报文后通过物理网络通信链路将报文发给 <code>VPN server</code>，<code>VPN server</code> 接收到报文后进行解包，再将其转发给实际的目标，反之同理； VPN 在逻辑层面构建了虚拟网络。</p>
<h2 id="I-x2F-O多路复用"><a href="#I-x2F-O多路复用" class="headerlink" title="I&#x2F;O多路复用"></a>I&#x2F;O多路复用</h2><p>服务器基于多进程或者多线程模型的，新到来一个 TCP 连接，就需要分配一个进程或者线程，那么如果要达到 C10K，意味着要一台机器维护 1 万个连接，相当于要维护 1 万个进程&#x2F;线程，操作系统就算死扛也是扛不住的。</p>
<p>既然为每个请求分配一个进程&#x2F;线程的方式不合适，那有没有可能只使用一个进程来维护多个 Socket 呢？答案是有的，那就是 <strong>I&#x2F;O 多路复用</strong>技术。</p>
<p>一个进程虽然任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，把时间拉长来看，多个请求复用了一个进程，这就是多路复用，这种思想很类似一个 CPU 并发多个进程，所以也叫做时分多路复用。</p>
<p>我们熟悉的 select&#x2F;poll&#x2F;epoll 内核提供给用户态的多路复用系统调用，<strong>进程可以通过一个系统调用函数从内核中获取多个事件</strong>。</p>
<p>select&#x2F;poll&#x2F;epoll 是如何获取网络事件的呢？在获取事件时，先把所有连接（文件描述符）传给内核，再由内核返回产生了事件的连接，然后在用户态中再处理这些连接对应的请求即可。</p>
<p>select&#x2F;poll&#x2F;epoll 这是三个多路复用接口，都能实现 C10K 吗？接下来，我们分别说说它们。</p>
<h3 id="select-x2F-poll"><a href="#select-x2F-poll" class="headerlink" title="select&#x2F;poll"></a>select&#x2F;poll</h3><p>select 实现多路复用的方式是，将已连接的 Socket 都放到一个<strong>文件描述符集合</strong>，然后调用 select 函数将文件描述符集合<strong>拷贝</strong>到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过<strong>遍历</strong>文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合<strong>拷贝</strong>回用户态里，然后用户态还需要再通过<strong>遍历</strong>的方法找到可读或可写的 Socket，然后再对其处理。</p>
<p>所以，对于 select 这种方式，需要进行 <strong>2 次「遍历」文件描述符集合</strong>，一次是在内核态里，一个次是在用户态里 ，而且还会发生 <strong>2 次「拷贝」文件描述符集合</strong>，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。</p>
<p>select 使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最大值为 <code>1024</code>，只能监听 0~1023 的文件描述符。</p>
<p>poll 不再用 BitsMap 来存储所关注的文件描述符，取而代之用动态数组，以链表形式来组织，突破了 select 的文件描述符个数限制，当然还会受到系统文件描述符限制。</p>
<p>但是 poll 和 select 并没有太大的本质区别，<strong>都是使用「线性结构」存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，而且也需要在用户态与内核态之间拷贝文件描述符集合</strong>，这种方式随着并发数上来，性能的损耗会呈指数级增长。</p>
<h3 id="epoll"><a href="#epoll" class="headerlink" title="epoll"></a>epoll</h3><p>先复习下 epoll 的用法。如下的代码中，先用epoll_create 创建一个 epoll对象 epfd，再通过 epoll_ctl 将需要监视的 socket 添加到epfd中，最后调用 epoll_wait 等待数据。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> s = socket(AF_INET, SOCK_STREAM, <span class="number">0</span>);</span><br><span class="line">bind(s, ...);</span><br><span class="line">listen(s, ...)</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> epfd = epoll_create(...);</span><br><span class="line">epoll_ctl(epfd, ...); <span class="comment">//将所有需要监听的socket添加到epfd中</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>) &#123;</span><br><span class="line">    <span class="type">int</span> n = epoll_wait(...);</span><br><span class="line">    <span class="keyword">for</span>(接收到数据的socket)&#123;</span><br><span class="line">        <span class="comment">//处理</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>epoll 通过两个方面，很好解决了 select&#x2F;poll 的问题。</p>
<p><em>第一点</em>，epoll 在内核里使用<strong>红黑树来跟踪进程所有待检测的文件描述字</strong>，把需要监控的 socket 通过 <code>epoll_ctl()</code> 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删改一般时间复杂度是 <code>O(logn)</code>。而 select&#x2F;poll 内核里没有类似 epoll 红黑树这种保存所有待检测的 socket 的数据结构，所以 select&#x2F;poll 每次操作时都传入整个 socket 集合给内核，而 epoll 因为在内核维护了红黑树，可以保存所有待检测的 socket ，所以只需要传入一个待检测的 socket，减少了内核和用户空间大量的数据拷贝和内存分配。</p>
<p><em>第二点</em>， epoll 使用<strong>事件驱动</strong>的机制，内核里<strong>维护了一个链表来记录就绪事件</strong>，当某个 socket 有事件发生时，通过<strong>回调函数</strong>内核会将其加入到这个就绪事件列表中，当用户调用 <code>epoll_wait()</code> 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select&#x2F;poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。(<strong>文件需要支持poll接口，也就是支持回调才可以使用epoll进行监听，常用的文件系统如ext3无法使用epoll</strong>)</p>
<p>从下图你可以看到 epoll 相关的接口作用：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/epoll.png" alt="img"></p>
<p>epoll 的方式即使监听的 Socket 数量越多的时候，效率不会大幅度降低，能够同时监听的 Socket 的数目也非常的多了，上限就为系统定义的进程打开的最大文件描述符个数。因而，<strong>epoll 被称为解决 C10K 问题的利器</strong>。</p>
<p>插个题外话，网上文章不少说，<code>epoll_wait</code> 返回时，对于就绪的事件，epoll 使用的是共享内存的方式，即用户态和内核态都指向了就绪链表，所以就避免了内存拷贝消耗。</p>
<p>这是错的！看过 epoll 内核源码的都知道，<strong>压根就没有使用共享内存这个玩意</strong>。</p>
<p>epoll惊群问题：</p>
<p>现象：当多个进程&#x2F;线程调用<code>epoll_wait</code>时会阻塞等待，当内核触发可读写事件，<strong>所有进程&#x2F;线程都会进行相应</strong>，但<strong>实际</strong>只有一个进程&#x2F;线程真实处理这些事件。</p>
<p>解决方案。</p>
<ol>
<li>配置<code>SO_REUSEPORT</code>实现内核级的负载均衡</li>
<li>增加<code>EPOLLEXCLUSIVE</code>标识，保证一个事件发生时只有一个线程被唤醒【linux 4.5 内核】</li>
</ol>
<h3 id="select和epoll对比"><a href="#select和epoll对比" class="headerlink" title="select和epoll对比"></a>select和epoll对比</h3><p>1.用户态怎么将文件句柄传递到内核态?</p>
<ul>
<li><strong>Selector</strong>。select 创建 3 个文件描述符集，并将这些文件描述符拷贝到内核中，这里限制了文件句柄的最大的数量为 1024（注意是全部传入—第一次拷贝）</li>
<li><strong>Epoll</strong>。首先执行 epoll_create 在内核专属于 epoll 的高速 cache 区，并在该缓冲区建立红黑树和就绪链表，用户态传入的文件句柄将被放到红黑树中（第一次拷贝）</li>
</ul>
<p>2.内核态怎么判断 I&#x2F;O 流可读可写？</p>
<ul>
<li><strong>Selector</strong>。内核针对读缓冲区和写缓冲区来判断是否可读可写,这个动作和 select 无关</li>
<li><strong>Epoll</strong>。内核针对读缓冲区和写缓冲区来判断是否可读可写，这个动作与 epoll 无关</li>
</ul>
<p>3.内核怎么通知监控者有 I&#x2F;O 流可读可写？</p>
<ul>
<li><p><strong>Selector</strong>。内核在检测到文件句柄可读&#x2F;可写时就产生中断通知监控者 select，select 被内核触发之后，就返回可读可写的文件句柄的总数</p>
</li>
<li><p><strong>Epoll</strong>。epoll_ctl 执行 add 动作时除了将文件句柄放到红黑树上之外，还向内核注册了该文件句柄的回调函数，内核在检测到某句柄可读可写时则调用该回调函数，回调函数将文件句柄放到就绪链表</p>
</li>
</ul>
<p>4.监控者如何找到可读可写的 I&#x2F;O 流并传递给用户态应用程序？</p>
<ul>
<li><strong>Selector</strong>。select 会将之前传递给内核的文件句柄再次从内核传到用户态（第 2 次拷贝），select 返回给用户态的只是可读可写的文件句柄总数，再使用 FD_ISSET 宏函数来检测哪些文件 I&#x2F;O 可读可写（遍历）</li>
<li><strong>Epoll</strong>。epoll_wait 只监控就绪链表就可以，如果就绪链表有文件句柄，则表示该文件句柄可读可写，并返回到用户态（少量的拷贝）</li>
</ul>
<p>5.继续循环时监控者怎样重复上述步骤？</p>
<ul>
<li><strong>Selector</strong>。select 对于事件的监控是建立在内核的修改之上的，也就是说经过一次监控之后，内核会修改位，因此再次监控时需要再次从用户态向内核态进行拷贝（第 N 次拷贝）</li>
<li><strong>Epoll</strong>。由于内核不修改文件句柄的位，因此只需要在第一次传入就可以重复监控，直到使用 epoll_ctl 删除，否则不需要重新传入，因此无多次拷贝</li>
</ul>
<h3 id="边缘触发和水平触发"><a href="#边缘触发和水平触发" class="headerlink" title="边缘触发和水平触发"></a>边缘触发和水平触发</h3><p>epoll 支持两种事件触发模式，分别是<strong>边缘触发（edge-triggered，ET和水平触发（level-triggered，LT）</strong>。</p>
<p>这两个术语还挺抽象的，其实它们的区别还是很好理解的。</p>
<ul>
<li>使用边缘触发模式时，当被监控的 Socket 描述符上有可读事件发生时，<strong>服务器端只会从 epoll_wait 中苏醒一次</strong>，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，因此我们程序要保证一次性将内核缓冲区的数据读取完；</li>
<li>使用水平触发模式时，当被监控的 Socket 上有可读事件发生时，<strong>服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束</strong>，目的是告诉我们有数据需要读取；</li>
</ul>
<p>举个例子，你的快递被放到了一个快递箱里，如果快递箱只会通过短信通知你一次，即使你一直没有去取，它也不会再发送第二条短信提醒你，这个方式就是边缘触发；如果快递箱发现你的快递没有被取出，它就会不停地发短信通知你，直到你取出了快递，它才消停，这个就是水平触发的方式。</p>
<p>这就是两者的区别，水平触发的意思是只要满足事件的条件，比如内核中有数据需要读，就一直不断地把这个事件传递给用户；而边缘触发的意思是只有第一次满足条件的时候才触发，之后就不会再传递同样的事件了。</p>
<p>如果使用水平触发模式，当内核通知文件描述符可读写时，接下来还可以继续去检测它的状态，看它是否依然可读或可写。所以在收到通知后，没必要一次执行尽可能多的读写操作。</p>
<p>如果使用边缘触发模式，I&#x2F;O 事件发生时只会通知一次，而且我们不知道到底能读写多少数据，所以在收到通知后应尽可能地读写数据，以免错失读写的机会。因此，我们会<strong>循环</strong>从文件描述符读写数据，那么如果文件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那里，程序就没办法继续往下执行。所以，<strong>边缘触发模式一般和非阻塞 I&#x2F;O 搭配使用</strong>，程序会一直执行 I&#x2F;O 操作，直到系统调用（如 <code>read</code> 和 <code>write</code>）返回错误，错误类型为 <code>EAGAIN</code> 或 <code>EWOULDBLOCK</code>。</p>
<p>一般来说，边缘触发的效率比水平触发的效率要高，因为边缘触发可以减少 epoll_wait 的系统调用次数，系统调用也是有一定的开销的的，毕竟也存在上下文的切换。</p>
<p>select&#x2F;poll 只有水平触发模式，epoll 默认的触发模式是水平触发，但是可以根据应用场景设置为边缘触发模式。</p>
<p>另外，使用 I&#x2F;O 多路复用时，最好搭配非阻塞 I&#x2F;O 一起使用，Linux 手册关于 select 的内容中有如下说明：</p>
<blockquote>
<p>Under Linux, select() may report a socket file descriptor as “ready for reading”, while nevertheless a subsequent read blocks. This could for example happen when data has arrived but upon examination has wrong checksum and is discarded. There may be other circumstances in which a file descriptor is spuriously reported as ready. Thus it may be safer to use O_NONBLOCK on sockets that should not block.</p>
</blockquote>
<p>我谷歌翻译的结果：</p>
<blockquote>
<p>在Linux下，select() 可能会将一个 socket 文件描述符报告为 “准备读取”，而后续的读取块却没有。例如，当数据已经到达，但经检查后发现有错误的校验和而被丢弃时，就会发生这种情况。也有可能在其他情况下，文件描述符被错误地报告为就绪。因此，在不应该阻塞的 socket 上使用 O_NONBLOCK 可能更安全。</p>
</blockquote>
<p>简单点理解，就是<strong>多路复用 API 返回的事件并不一定可读写的</strong>，如果使用阻塞 I&#x2F;O， 那么在调用 read&#x2F;write 时则会发生程序阻塞，因此最好搭配非阻塞 I&#x2F;O，以便应对极少数的特殊情况。</p>
<p>总结：</p>
<p>epoll 是解决 C10K 问题的利器，通过两个方面解决了 select&#x2F;poll 的问题。</p>
<ul>
<li>epoll 在内核里使用「红黑树」来关注进程所有待检测的 Socket，红黑树是个高效的数据结构，增删改一般时间复杂度是 O(logn)，通过对这棵黑红树的管理，不需要像 select&#x2F;poll 在每次操作时都传入整个 Socket 集合，减少了内核和用户空间大量的数据拷贝和内存分配。</li>
<li>epoll 使用事件驱动的机制，内核里维护了一个「链表」来记录就绪事件，只将有事件发生的 Socket 集合传递给应用程序，不需要像 select&#x2F;poll 那样轮询扫描整个集合（包含有和无事件的 Socket ），大大提高了检测的效率。</li>
</ul>
<p>而且，epoll 支持边缘触发和水平触发的方式，而 select&#x2F;poll 只支持水平触发，一般而言，边缘触发的方式会比水平触发的效率高。</p>
<h2 id="Reactor和Proactor"><a href="#Reactor和Proactor" class="headerlink" title="Reactor和Proactor"></a>Reactor和Proactor</h2><h3 id="演进"><a href="#演进" class="headerlink" title="演进"></a>演进</h3><p>如果要让服务器服务多个客户端，那么最直接的方式就是为每一条连接创建线程。</p>
<p>其实创建进程也是可以的，原理是一样的，进程和线程的区别在于线程比较轻量级些，线程的创建和线程间切换的成本要小些，为了描述简述，后面都以线程为例。</p>
<p>处理完业务逻辑后，随着连接关闭后线程也同样要销毁了，但是这样不停地创建和销毁线程，不仅会带来性能开销，也会造成浪费资源，而且如果要连接几万条连接，创建几万个线程去应对也是不现实的。</p>
<p>要这么解决这个问题呢？我们可以使用「资源复用」的方式。</p>
<p>也就是不用再为每个连接创建线程，而是创建一个「线程池」，将连接分配给线程，然后一个线程可以处理多个连接的业务。</p>
<p>不过，这样又引来一个新的问题，线程怎样才能高效地处理多个连接的业务？</p>
<p>当一个连接对应一个线程时，线程一般采用「read -&gt; 业务处理 -&gt; send」的处理流程，如果当前连接没有数据可读，那么线程会阻塞在 <code>read</code> 操作上（ socket 默认情况是阻塞 I&#x2F;O），不过这种阻塞方式并不影响其他线程。</p>
<p>但是引入了线程池，那么一个线程要处理多个连接的业务，线程在处理某个连接的 <code>read</code> 操作时，如果遇到没有数据可读，就会发生阻塞，那么线程就没办法继续处理其他连接的业务。</p>
<p>要解决这一个问题，最简单的方式就是将 socket 改成非阻塞，然后线程不断地轮询调用 <code>read</code> 操作来判断是否有数据，这种方式虽然该能够解决阻塞的问题，但是解决的方式比较粗暴，因为轮询是要消耗 CPU 的，而且随着一个线程处理的连接越多，轮询的效率就会越低。</p>
<p>上面的问题在于，线程并不知道当前连接是否有数据可读，从而需要每次通过 <code>read</code> 去试探。</p>
<p>那有没有办法在只有当连接上有数据的时候，线程才去发起读请求呢？答案是有的，实现这一技术的就是 I&#x2F;O 多路复用。</p>
<p>I&#x2F;O 多路复用技术会用一个系统调用函数来监听我们所有关心的连接，也就说可以在一个监控线程里面监控很多的连接。</p>
<p>我们熟悉的 select&#x2F;poll&#x2F;epoll 就是内核提供给用户态的多路复用系统调用，线程可以通过一个系统调用函数从内核中获取多个事件。select&#x2F;poll&#x2F;epoll 是如何获取网络事件的呢？</p>
<p>在获取事件时，先把我们要关心的连接传给内核，再由内核检测：</p>
<ul>
<li>如果没有事件发生，线程只需阻塞在这个系统调用，而无需像前面的线程池方案那样轮训调用 read 操作来判断是否有数据。</li>
<li>如果有事件发生，内核会返回产生了事件的连接，线程就会从阻塞状态返回，然后在用户态中再处理这些连接对应的业务即可。</li>
</ul>
<p>大佬们基于面向对象的思想，对 I&#x2F;O 多路复用作了一层封装，让使用者不用考虑底层网络 API 的细节，只需要关注应用代码的编写。</p>
<p>大佬们还为这种模式取了个让人第一时间难以理解的名字：<strong>Reactor 模式</strong>。</p>
<p>Reactor 翻译过来的意思是「反应堆」，可能大家会联想到物理学里的核反应堆，实际上并不是的这个意思。</p>
<p>这里的反应指的是「<strong>对事件反应</strong>」，也就是<strong>来了一个事件，Reactor 就有相对应的反应&#x2F;响应</strong>。</p>
<p>事实上，Reactor 模式也叫 <code>Dispatcher</code> 模式，我觉得这个名字更贴合该模式的含义，即 <strong>I&#x2F;O 多路复用监听事件，收到事件后，根据事件类型分配（Dispatch）给某个进程 &#x2F; 线程</strong>。</p>
<p>Reactor 模式主要由 Reactor 和处理资源池这两个核心部分组成，它俩负责的事情如下：</p>
<ul>
<li>Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件；</li>
<li>处理资源池负责处理事件，如 read -&gt; 业务逻辑 -&gt; send；</li>
</ul>
<p>Reactor 模式是灵活多变的，可以应对不同的业务场景，灵活在于：</p>
<ul>
<li>Reactor 的数量可以只有一个，也可以有多个；</li>
<li>处理资源池可以是单个进程 &#x2F; 线程，也可以是多个进程 &#x2F;线程；</li>
</ul>
<p>将上面的两个因素排列组设一下，理论上就可以有 4 种方案选择：</p>
<ul>
<li>单 Reactor 单进程 &#x2F; 线程；</li>
<li>单 Reactor 多进程 &#x2F; 线程；</li>
<li>多 Reactor 单进程 &#x2F; 线程；</li>
<li>多 Reactor 多进程 &#x2F; 线程；</li>
</ul>
<p>其中，「多 Reactor 单进程 &#x2F; 线程」实现方案相比「单 Reactor 单进程 &#x2F; 线程」方案，不仅复杂而且也没有性能优势，因此实际中并没有应用。</p>
<p>剩下的 3 个方案都是比较经典的，且都有应用在实际的项目中：</p>
<ul>
<li>单 Reactor 单进程 &#x2F; 线程；</li>
<li>单 Reactor 多线程 &#x2F; 进程；</li>
<li>多 Reactor 多进程 &#x2F; 线程；</li>
</ul>
<p>方案具体使用进程还是线程，要看使用的编程语言以及平台有关：</p>
<ul>
<li>Java 语言一般使用线程，比如 Netty;</li>
<li>C 语言使用进程和线程都可以，例如 Nginx 使用的是进程，Memcache 使用的是线程。</li>
</ul>
<p>接下来，分别介绍这三个经典的 Reactor 方案。</p>
<h3 id="Reactor"><a href="#Reactor" class="headerlink" title="Reactor"></a>Reactor</h3><h4 id="单Reactor-单进程-x2F-线程"><a href="#单Reactor-单进程-x2F-线程" class="headerlink" title="单Reactor 单进程 &#x2F; 线程"></a>单Reactor 单进程 &#x2F; 线程</h4><p>一般来说，C 语言实现的是「<strong>单 Reactor 单进程</strong>」的方案，因为 C 语言编写完的程序，运行后就是一个独立的进程，不需要在进程中再创建线程。</p>
<p>而 Java 语言实现的是「<strong>单 Reactor 单线程</strong>」的方案，因为 Java 程序是跑在 Java 虚拟机这个进程上面的，虚拟机中有很多线程，我们写的 Java 程序只是其中的一个线程而已。</p>
<p>我们来看看「<strong>单 Reactor 单进程</strong>」的方案示意图：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%8D%95%E8%BF%9B%E7%A8%8B.png" alt="img"></p>
<p>可以看到进程里有 <strong>Reactor、Acceptor、Handler</strong> 这三个对象：</p>
<ul>
<li>Reactor 对象的作用是监听和分发事件；</li>
<li>Acceptor 对象的作用是获取连接；</li>
<li>Handler 对象的作用是处理业务；</li>
</ul>
<p>对象里的 select、accept、read、send 是系统调用函数，dispatch 和 「业务处理」是需要完成的操作，其中 dispatch 是分发事件操作。</p>
<p>接下来，介绍下「单 Reactor 单进程」这个方案：</p>
<ul>
<li>Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；</li>
<li>如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；</li>
<li>如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；</li>
<li>Handler 对象通过 read -&gt; 业务处理 -&gt; send 的流程来完成完整的业务流程。</li>
</ul>
<p>单 Reactor 单进程的方案因为全部工作都在同一个进程内完成，所以实现起来比较简单，不需要考虑进程间通信，也不用担心多进程竞争。</p>
<p>但是，这种方案存在 2 个缺点：</p>
<ul>
<li>第一个缺点，因为只有一个进程，<strong>无法充分利用多核 CPU 的性能</strong>；</li>
<li>第二个缺点，Handler 对象在业务处理时，整个进程是无法处理其他连接的事件的，<strong>如果业务处理耗时比较长，那么就造成响应的延迟</strong>；</li>
</ul>
<p>所以，单 Reactor 单进程的方案<strong>不适用计算密集型的场景，只适用于业务处理非常快速的场景</strong>。</p>
<p>Redis 是由 C 语言实现的，在 Redis 6.0 版本之前采用的正是「单 Reactor 单进程」的方案，因为 Redis 业务处理主要是在内存中完成，操作的速度是很快的，性能瓶颈不在 CPU 上，所以 Redis 对于命令的处理是单进程的方案。</p>
<h4 id="单-Reactor-多线程-x2F-多进程"><a href="#单-Reactor-多线程-x2F-多进程" class="headerlink" title="单 Reactor 多线程 &#x2F; 多进程"></a>单 Reactor 多线程 &#x2F; 多进程</h4><p>如果要克服「单 Reactor 单线程 &#x2F; 进程」方案的缺点，那么就需要引入多线程 &#x2F; 多进程，这样就产生了<strong>单 Reactor 多线程 &#x2F; 多进程</strong>的方案。</p>
<p>闻其名不如看其图，先来看看「单 Reactor 多线程」方案的示意图如下：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%A4%9A%E7%BA%BF%E7%A8%8B.png" alt="img"></p>
<p>详细说一下这个方案：</p>
<ul>
<li>Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；</li>
<li>如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；</li>
<li>如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；</li>
</ul>
<p>上面的三个步骤和单 Reactor 单线程方案是一样的，接下来的步骤就开始不一样了：</p>
<ul>
<li>Handler 对象不再负责业务处理，只负责数据的接收和发送，Handler 对象通过 read 读取到数据后，会将数据发给子线程里的 Processor 对象进行业务处理；</li>
<li>子线程里的 Processor 对象就进行业务处理，处理完后，将结果发给主线程中的 Handler 对象，接着由 Handler 通过 send 方法将响应结果发送给 client；</li>
</ul>
<p>单 Reator 多线程的方案优势在于<strong>能够充分利用多核 CPU 的能</strong>，那既然引入多线程，那么自然就带来了多线程竞争资源的问题。</p>
<p>例如，子线程完成业务处理后，要把结果传递给主线程的 Handler 进行发送，这里涉及共享数据的竞争。</p>
<p>要避免多线程由于竞争共享资源而导致数据错乱的问题，就需要在操作共享资源前加上互斥锁，以保证任意时间里只有一个线程在操作共享资源，待该线程操作完释放互斥锁后，其他线程才有机会操作共享数据。</p>
<p>聊完单 Reactor 多线程的方案，接着来看看单 Reactor 多进程的方案。</p>
<p>事实上，单 Reactor 多进程相比单 Reactor 多线程实现起来很麻烦，主要因为要考虑子进程 &lt;-&gt; 父进程的双向通信，并且父进程还得知道子进程要将数据发送给哪个客户端。</p>
<p>而多线程间可以共享数据，虽然要额外考虑并发问题，但是这远比进程间通信的复杂度低得多，<strong>因此实际应用中也看不到单 Reactor 多进程的模式</strong>。</p>
<p>另外，「单 Reactor」的模式还有个问题，<strong>因为一个 Reactor 对象承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方</strong>。</p>
<h4 id="多-Reactor-多进程-x2F-线程"><a href="#多-Reactor-多进程-x2F-线程" class="headerlink" title="多 Reactor 多进程 &#x2F; 线程"></a>多 Reactor 多进程 &#x2F; 线程</h4><p>要解决「单 Reactor」的问题，就是将「单 Reactor」实现成「多 Reactor」，这样就产生了第 <strong>多 Reactor 多进程 &#x2F; 线程</strong>的方案。</p>
<p>老规矩，闻其名不如看其图。多 Reactor 多进程 &#x2F; 线程方案的示意图如下（以线程为例）：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E4%B8%BB%E4%BB%8EReactor%E5%A4%9A%E7%BA%BF%E7%A8%8B.png" alt="img"></p>
<p>方案详细说明如下：</p>
<ul>
<li>主线程中的 MainReactor 对象通过 select 监控连接建立事件，收到事件后通过 Acceptor 对象中的 accept 获取连接，将新的连接分配给某个子线程；</li>
<li>子线程中的 SubReactor 对象将 MainReactor 对象分配的连接加入 select 继续进行监听，并创建一个 Handler 用于处理连接的响应事件。</li>
<li>如果有新的事件发生时，SubReactor 对象会调用当前连接对应的 Handler 对象来进行响应。</li>
<li>Handler 对象通过 read -&gt; 业务处理 -&gt; send 的流程来完成完整的业务流程。</li>
</ul>
<p>多 Reactor 多线程的方案虽然看起来复杂的，但是实际实现时比单 Reactor 多线程的方案要简单的多，原因如下：</p>
<ul>
<li>主线程和子线程分工明确，<strong>主线程只负责接收新连接，子线程负责完成后续的业务处理。</strong></li>
<li>主线程和子线程的交互很简单，主线程只需要把新连接传给子线程，子线程无须返回数据，直接就可以在子线程将处理结果发送给客户端。</li>
</ul>
<p>大名鼎鼎的两个开源软件 Netty 和 Memcache 都采用了「多 Reactor 多线程」的方案。</p>
<p>采用了「多 Reactor 多进程」方案的开源软件是 Nginx，不过方案与标准的多 Reactor 多进程有些差异。</p>
<p>具体差异表现在主进程中仅仅用来初始化 socket，并没有创建 mainReactor 来 accept 连接，而是由子进程的 Reactor 来 accept 连接，通过锁来控制一次只有一个子进程进行 accept（防止出现惊群现象），子进程 accept 新连接后就放到自己的 Reactor 进行处理，不会再分配给其他子进程。</p>
<h3 id="Proactor"><a href="#Proactor" class="headerlink" title="Proactor"></a>Proactor</h3><p>前面提到的 Reactor 是非阻塞同步网络模式，而 <strong>Proactor 是异步网络模式</strong>。</p>
<ul>
<li><strong>Reactor 是非阻塞同步网络模式，感知的是就绪可读写事件</strong>。在每次感知到有事件发生（比如可读就绪事件）后，就需要应用进程主动调用 read 方法来完成数据的读取，也就是要应用进程主动将 socket 接收缓存中的数据读到应用进程内存中，这个过程是同步的，读取完数据后应用进程才能处理数据。</li>
<li><strong>Proactor 是异步网络模式， 感知的是已完成的读写事件</strong>。在发起异步读写请求时，需要传入数据缓冲区的地址（用来存放结果数据）等信息，这样系统内核才可以自动帮我们把数据的读写工作完成，这里的读写工作全程由操作系统来做，并不需要像 Reactor 那样还需要应用进程主动发起 read&#x2F;write 来读写数据，操作系统完成读写工作后，就会通知应用进程直接处理数据。</li>
</ul>
<p>因此，<strong>Reactor 可以理解为「来了事件操作系统通知应用进程，让应用进程来处理」</strong>，而 <strong>Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应用进程」</strong>。这里的「事件」就是有新连接、有数据可读、有数据可写的这些 I&#x2F;O 事件这里的「处理」包含从驱动读取到内核以及从内核读取到用户空间。</p>
<p>无论是 Reactor，还是 Proactor，都是一种基于「事件分发」的网络编程模式，区别在于 <strong>Reactor 模式是基于「待完成」的 I&#x2F;O 事件，而 Proactor 模式则是基于「已完成」的 I&#x2F;O 事件</strong>。</p>
<p>可惜的是，在 Linux 下的异步 I&#x2F;O 是不完善的， <code>aio</code> 系列函数是由 POSIX 定义的异步操作接口，不是真正的操作系统级别支持的，而是在用户空间模拟出来的异步，并且仅仅支持基于本地文件的 aio 异步操作，网络编程中的 socket 是不支持的，这也使得<strong>基于 Linux 的高性能网络程序都是使用 Reactor 方案</strong>。</p>
<p>而 Windows 里实现了一套完整的支持 socket 的异步编程接口，这套接口就是 <code>IOCP</code>，是由操作系统级别实现的异步 I&#x2F;O，真正意义上异步 I&#x2F;O，因此在 Windows 里实现高性能网络程序可以使用效率更高的 Proactor 方案。</p>
<h2 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h2><p>大多数网站背后肯定不是只有一台服务器提供服务，因为单机的并发量和数据量都是有限的，所以都会用多台服务器构成集群来对外提供服务。</p>
<p>但是问题来了，现在有那么多个节点（后面统称服务器为节点，因为少一个字），要如何分配客户端的请求呢？</p>
<p>其实这个问题就是「负载均衡问题」。解决负载均衡问题的算法很多，不同的负载均衡算法，对应的就是不同的分配策略，适应的业务场景也不同。</p>
<p>最简单的方式，引入一个中间的负载均衡层，让它将外界的请求「轮流」的转发给内部的集群。比如集群有 3 个节点，外界请求有 3 个，那么每个节点都会处理 1 个请求，达到了分配请求的目的。</p>
<p>考虑到每个节点的硬件配置有所区别，我们可以引入权重值，将硬件配置更好的节点的权重值设高，然后根据各个节点的权重值，按照一定比重分配在不同的节点上，让硬件配置更好的节点承担更多的请求，<strong>这种算法叫做加权轮询</strong>。</p>
<p><strong>加权轮询算法使用场景是建立在每个节点存储的数据都是相同的前提</strong>。所以，每次读数据的请求，访问任意一个节点都能得到结果。</p>
<p>但是，加权轮询算法是无法应对「分布式系统（数据分片的系统）」的，因为分布式系统中，每个节点存储的数据是不同的。</p>
<p>当我们想提高系统的容量，就会将数据水平切分到不同的节点来存储，也就是将数据分布到了不同的节点。比如<strong>一个分布式 KV（key-valu） 缓存系统，某个 key 应该到哪个或者哪些节点上获得，应该是确定的</strong>，不是说任意访问一个节点都可以得到缓存结果的。</p>
<p>因此，我们要想一个能应对分布式系统的负载均衡算法。</p>
<h3 id="使用哈希算法有什么问题？"><a href="#使用哈希算法有什么问题？" class="headerlink" title="使用哈希算法有什么问题？"></a>使用哈希算法有什么问题？</h3><p>有的同学可能很快就想到了：<strong>哈希算法</strong>。因为对同一个关键字进行哈希计算，每次计算都是相同的值，这样就可以将某个 key 确定到一个节点了，可以满足分布式系统的负载均衡需求。</p>
<p>哈希算法最简单的做法就是进行取模运算，比如分布式系统中有 3 个节点，基于 <code>hash(key) % 3</code> 公式对数据进行了映射。</p>
<p>如果客户端要获取指定 key 的数据，通过下面的公式可以定位节点：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hash(key) % 3</span><br></pre></td></tr></table></figure>

<p>如果经过上面这个公式计算后得到的值是 0，就说明该 key 需要去第一个节点获取。</p>
<p>但是有一个很致命的问题，<strong>如果节点数量发生了变化，也就是在对系统做扩容或者缩容时，必须迁移改变了映射关系的数据</strong>，否则会出现查询不到数据的问题。</p>
<p>举个例子，假设我们有一个由 A、B、C 三个节点组成分布式 KV 缓存系统，基于计算公式 <code>hash(key) % 3</code> 将数据进行了映射，每个节点存储了不同的数据。</p>
<p>现在有 3 个查询 key 的请求，分别查询 key-01，key-02，key-03 的数据，这三个 key 分别经过 hash() 函数计算后的值为 hash( key-01) &#x3D; 6、hash( key-02) &#x3D; 7、hash(key-03) &#x3D; 8，然后再对这些值进行取模运算。通过这样的哈希算法，每个 key 都可以定位到对应的节点。</p>
<p>当 3 个节点不能满足业务需求了，这时我们增加了一个节点，节点的数量从 3 变化为 4，意味取模哈希函数中基数的变化，这样会导致<strong>大部分映射关系改变</strong></p>
<p>比如，之前的 hash(key-01) % <code>3</code> &#x3D; 0，就变成了 hash(key-01) % <code>4</code> &#x3D; 2，查询 key-01 数据时，寻址到了节点 C，而 key-01 的数据是存储在节点 A 上的，不是在节点 C，所以会查询不到数据。</p>
<p>同样的道理，如果我们对分布式系统进行缩容，比如移除一个节点，也会因为取模哈希函数中基数的变化，可能出现查询不到数据的问题。</p>
<p>要解决这个问题的办法，就需要我们进行<strong>迁移数据</strong>，比如节点的数量从 3 变化为 4 时，要基于新的计算公式 hash(key) % 4 ，重新对数据和节点做映射。</p>
<p>假设总数据条数为 M，哈希算法在面对节点数量变化时，**最坏情况下所有数据都需要迁移，所以它的数据迁移规模是 O(M)**，这样数据的迁移成本太高了。</p>
<p>所以，我们应该要重新想一个新的算法，来避免分布式系统在扩容或者缩容时，发生过多的数据迁移。</p>
<h3 id="使用一致性哈希算法有什么问题？"><a href="#使用一致性哈希算法有什么问题？" class="headerlink" title="使用一致性哈希算法有什么问题？"></a>使用一致性哈希算法有什么问题？</h3><p>一致性哈希算法就很好地解决了分布式系统在扩容或者缩容时，发生过多的数据迁移的问题。</p>
<p>一致哈希算法也用了取模运算，但与哈希算法不同的是，哈希算法是对节点的数量进行取模运算，而<strong>一致哈希算法是对 2^32 进行取模运算，是一个固定的值</strong>。</p>
<p>我们可以把一致哈希算法是对 2^32 进行取模运算的结果值组织成一个圆环，就像钟表一样，钟表的圆可以理解成由 60 个点组成的圆，而此处我们把这个圆想象成由 2^32 个点组成的圆，这个圆环被称为<strong>哈希环</strong>，如下图：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/0ea3960fef48d4cbaeb4bec4345301e7.png" alt="img"></p>
<p>一致性哈希要进行两步哈希：</p>
<ul>
<li>第一步：对存储节点进行哈希计算，也就是对存储节点做哈希映射，比如根据节点的 IP 地址进行哈希；</li>
<li>第二步：当对数据进行存储或访问时，对数据进行哈希映射；</li>
</ul>
<p>所以，<strong>一致性哈希是指将「存储节点」和「数据」都映射到一个首尾相连的哈希环上</strong>。</p>
<p>问题来了，对「数据」进行哈希映射得到一个结果要怎么找到存储该数据的节点呢？</p>
<p>答案是，映射的结果值往<strong>顺时针的方向的找到第一个节点</strong>，就是存储该数据的节点。</p>
<p>举个例子，有 3 个节点经过哈希计算，映射到了如下图的位置：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/83d7f363643353c92d252e34f1d4f687.png" alt="img"></p>
<p>接着，对要查询的 key-01 进行哈希计算，确定此 key-01 映射在哈希环的位置，然后从这个位置往顺时针的方向找到第一个节点，就是存储该 key-01 数据的节点。</p>
<p>比如，下图中的 key-01 映射的位置，往顺时针的方向找到第一个节点就是节点 A。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/30c2c70721c12f9c140358fbdc5f2282.png" alt="img"></p>
<p>所以，当需要对指定 key 的值进行读写的时候，要通过下面 2 步进行寻址：</p>
<ul>
<li>首先，对 key 进行哈希计算，确定此 key 在环上的位置；</li>
<li>然后，从这个位置沿着顺时针方向走，遇到的第一节点就是存储 key 的节点。</li>
</ul>
<p>知道了一致哈希寻址的方式，我们来看看，如果增加一个节点或者减少一个节点会发生大量的数据迁移吗？</p>
<p>假设节点数量从 3 增加到了 4，新的节点 D 经过哈希计算后映射到了下图中的位置：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/f8909edef2f3949f8945bb99380baab3.png" alt="img"></p>
<p>你可以看到，key-01、key-03 都不受影响，只有 key-02 需要被迁移节点 D。</p>
<p>假设节点数量从 3 减少到了 2，比如将节点 A 移除：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/31485046f1303b57d8aaeaab103ea7ab.png" alt="img"></p>
<p>你可以看到，key-02 和 key-03 不会受到影响，只有 key-01 需要被迁移节点 B。</p>
<p>因此，<strong>在一致哈希算法中，如果增加或者移除一个节点，仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响</strong>。</p>
<p>上面这些图中 3 个节点映射在哈希环还是比较分散的，所以看起来请求都会「均衡」到每个节点。</p>
<p>但是<strong>一致性哈希算法并不保证节点能够在哈希环上分布均匀</strong>，这样就会带来一个问题，会有大量的请求集中在一个节点上。</p>
<p>比如，下图中 3 个节点的映射位置都在哈希环的右半边：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/d528bae6fcec2357ba2eb8f324ad9fd5.png" alt="img"></p>
<p>这时候有一半以上的数据的寻址都会找节点 A，也就是访问请求主要集中的节点 A 上，这肯定不行的呀，说好的负载均衡呢，这种情况一点都不均衡。</p>
<p>另外，在这种节点分布不均匀的情况下，进行容灾与扩容时，哈希环上的相邻节点容易受到过大影响，容易发生雪崩式的连锁反应。</p>
<p>比如，上图中如果节点 A 被移除了，当节点 A 宕机后，根据一致性哈希算法的规则，其上数据应该全部迁移到相邻的节点 B 上，这样，节点 B 的数据量、访问量都会迅速增加很多倍，一旦新增的压力超过了节点 B 的处理能力上限，就会导致节点 B 崩溃，进而形成雪崩式的连锁反应。</p>
<p>所以，<strong>一致性哈希算法虽然减少了数据迁移量，但是存在节点分布不均匀的问题</strong>。</p>
<h3 id="如何通过虚拟节点提高均衡度？"><a href="#如何通过虚拟节点提高均衡度？" class="headerlink" title="如何通过虚拟节点提高均衡度？"></a>如何通过虚拟节点提高均衡度？</h3><p>要想解决节点能在哈希环上分配不均匀的问题，就是要有大量的节点，节点数越多，哈希环上的节点分布的就越均匀。</p>
<p>但问题是，实际中我们没有那么多节点。所以这个时候我们就加入<strong>虚拟节点</strong>，也就是对一个真实节点做多个副本。</p>
<p>具体做法是，<strong>不再将真实节点映射到哈希环上，而是将虚拟节点映射到哈希环上，并将虚拟节点映射到实际节点，所以这里有「两层」映射关系。</strong></p>
<p>比如对每个节点分别设置 3 个虚拟节点：</p>
<ul>
<li>对节点 A 加上编号来作为虚拟节点：A-01、A-02、A-03</li>
<li>对节点 B 加上编号来作为虚拟节点：B-01、B-02、B-03</li>
<li>对节点 C 加上编号来作为虚拟节点：C-01、C-02、C-03</li>
</ul>
<p>引入虚拟节点后，原本哈希环上只有 3 个节点的情况，就会变成有 9 个虚拟节点映射到哈希环上，哈希环上的节点数量多了 3 倍。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/dbb57b8d6071d011d05eeadd93269e13.png" alt="img"></p>
<p>你可以看到，<strong>节点数量多了后，节点在哈希环上的分布就相对均匀了</strong>。这时候，如果有访问请求寻址到「A-01」这个虚拟节点，接着再通过「A-01」虚拟节点找到真实节点 A，这样请求就能访问到真实节点 A 了。</p>
<p>上面为了方便你理解，每个真实节点仅包含 3 个虚拟节点，这样能起到的均衡效果其实很有限。而在实际的工程中，虚拟节点的数量会大很多，比如 Nginx 的一致性哈希算法，每个权重为 1 的真实节点就含有160 个虚拟节点。</p>
<p>另外，虚拟节点除了会提高节点的均衡度，还会提高系统的稳定性。<strong>当节点变化时，会有不同的节点共同分担系统的变化，因此稳定性更高</strong>。</p>
<p>比如，当某个节点被移除时，对应该节点的多个虚拟节点均会移除，而这些虚拟节点按顺时针方向的下一个虚拟节点，可能会对应不同的真实节点，即这些不同的真实节点共同分担了节点变化导致的压力。</p>
<p>而且，有了虚拟节点后，还可以为硬件配置更好的节点增加权重，比如对权重更高的节点增加更多的虚拟机节点即可。</p>
<p>因此，<strong>带虚拟节点的一致性哈希方法不仅适合硬件配置不同的节点的场景，而且适合节点规模会发生变化的场景</strong>。</p>
<h2 id="网络性能查看"><a href="#网络性能查看" class="headerlink" title="网络性能查看"></a>网络性能查看</h2><h3 id="性能指标有哪些？"><a href="#性能指标有哪些？" class="headerlink" title="性能指标有哪些？"></a>性能指标有哪些？</h3><p>通常是以 4 个指标来衡量网络的性能，分别是带宽、延时、吞吐率、PPS（Packet Per Second），它们表示的意义如下：</p>
<ul>
<li><em>带宽</em>，表示链路的最大传输速率，单位是 b&#x2F;s （比特 &#x2F; 秒），带宽越大，其传输能力就越强。</li>
<li><em>延时</em>，表示请求数据包发送后，收到对端响应，所需要的时间延迟。不同的场景有着不同的含义，比如可以表示建立 TCP 连接所需的时间延迟，或一个数据包往返所需的时间延迟。</li>
<li><em>吞吐率</em>，表示单位时间内成功传输的数据量，单位是 b&#x2F;s（比特 &#x2F; 秒）或者 B&#x2F;s（字节 &#x2F; 秒），吞吐受带宽限制，带宽越大，吞吐率的上限才可能越高。</li>
<li><em>PPS</em>，全称是 Packet Per Second（包 &#x2F; 秒），表示以网络包为单位的传输速率，一般用来评估系统对于网络的转发能力。</li>
</ul>
<p>当然，除了以上这四种基本的指标，还有一些其他常用的性能指标，比如：</p>
<ul>
<li><em>网络的可用性</em>，表示网络能否正常通信；</li>
<li><em>并发连接数</em>，表示 TCP 连接数量；</li>
<li><em>丢包率</em>，表示所丢失数据包数量占所发送数据组的比率；</li>
<li><em>重传率</em>，表示重传网络包的比例；</li>
</ul>
<p>你可能会问了，如何观测这些性能指标呢？不急，继续往下看。</p>
<h3 id="网络配置查看"><a href="#网络配置查看" class="headerlink" title="网络配置查看"></a>网络配置查看</h3><p>要想知道网络的配置和状态，我们可以使用 <code>ifconfig</code> 或者 <code>ip</code> 命令来查看。</p>
<p>这两个命令功能都差不多，不过它们属于不同的软件包，<code>ifconfig</code> 属于 <code>net-tools</code> 软件包，<code>ip</code> 属于 <code>iproute2</code> 软件包，我的印象中 <code>net-tools</code> 软件包没有人继续维护了，而 <code>iproute2</code> 软件包是有开发者依然在维护，所以更推荐你使用 <code>ip</code> 工具。</p>
<p>虽然这两个命令输出的格式不尽相同，但是输出的内容基本相同，比如都包含了 IP 地址、子网掩码、MAC 地址、网关地址、MTU 大小、网口的状态以及网络包收发的统计信息，下面就来说说这些信息，它们都与网络性能有一定的关系。</p>
<p>第一，网口的连接状态标志。其实也就是表示对应的网口是否连接到交换机或路由器等设备，如果 <code>ifconfig</code> 输出中看到有 <code>RUNNING</code>，或者 <code>ip</code> 输出中有 <code>LOWER_UP</code>，则说明物理网络是连通的，如果看不到，则表示网口没有接网线。</p>
<p>第二，MTU 大小。默认值是 <code>1500</code> 字节，其作用主要是限制网络包的大小，如果 IP 层有一个数据报要传，而且网络包的长度比链路层的 MTU 还大，那么 IP 层就需要进行分片，即把数据报分成若干片，这样每一片就都小于 MTU。事实上，每个网络的链路层 MTU 可能会不一样，所以你可能需要调大或者调小 MTU 的数值。</p>
<p>第三，网口的 IP 地址、子网掩码、MAC 地址、网关地址。这些信息必须要配置正确，网络功能才能正常工作。</p>
<p>第四，网络包收发的统计信息。通常有网络收发的字节数、包数、错误数以及丢包情况的信息，如果 <code>TX</code>（发送） 和 <code>RX</code>（接收） 部分中 errors、dropped、overruns、carrier 以及 collisions 等指标不为 0 时，则说明网络发送或者接收出问题了，这些出错统计信息的指标意义如下：</p>
<ul>
<li><em>errors</em> 表示发生错误的数据包数，比如校验错误、帧同步错误等；</li>
<li><em>dropped</em> 表示丢弃的数据包数，即数据包已经收到了 Ring Buffer（这个缓冲区是在内核内存中，更具体一点是在网卡驱动程序里），但因为系统内存不足等原因而发生的丢包；</li>
<li><em>overruns</em> 表示超限数据包数，即网络接收&#x2F;发送速度过快，导致 Ring Buffer 中的数据包来不及处理，而导致的丢包，因为过多的数据包挤压在 Ring Buffer，这样 Ring Buffer 很容易就溢出了；</li>
<li><em>carrier</em> 表示发生 carrirer 错误的数据包数，比如双工模式不匹配、物理电缆出现问题等；</li>
<li><em>collisions</em> 表示冲突、碰撞数据包数；</li>
</ul>
<p><code>ifconfig</code> 和 <code>ip</code> 命令只显示的是网口的配置以及收发数据包的统计信息，而看不到协议栈里的信息，那接下来就来看看如何查看协议栈里的信息。</p>
<h3 id="socket信息查看"><a href="#socket信息查看" class="headerlink" title="socket信息查看"></a>socket信息查看</h3><p>我们可以使用 <code>netstat</code> 或者 <code>ss</code>，这两个命令查看 socket、网络协议栈、网口以及路由表的信息。</p>
<p>虽然 <code>netstat</code> 与 <code>ss</code> 命令查看的信息都差不多，但是如果在生产环境中要查看这类信息的时候，尽量不要使用 <code>netstat</code> 命令，因为它的性能不好，在系统比较繁忙的情况下，如果频繁使用 <code>netstat</code> 命令则会对性能的开销雪上加霜，所以更推荐你使用性能更好的 <code>ss</code> 命令。</p>
<p>从下面这张图，你可以看到这两个命令的输出内容：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%BD%91%E7%BB%9C/showsocket.png" alt="img"></p>
<p>可以发现，输出的内容都差不多， 比如都包含了 socket 的状态（<em>State</em>）、接收队列（<em>Recv-Q</em>）、发送队列（<em>Send-Q</em>）、本地地址（<em>Local Address</em>）、远端地址（<em>Foreign Address</em>）、进程 PID 和进程名称（<em>PID&#x2F;Program name</em>）等。</p>
<p>使用netstat -s或ss -s可以查看统计信息。</p>
<p><code>ss</code> 命令输出的统计信息相比 <code>netsat</code> 比较少，<code>ss</code> 只显示已经连接（<em>estab</em>）、关闭（<em>closed</em>）、孤儿（<em>orphaned</em>） socket 等简要统计。</p>
<p>而 <code>netstat</code> 则有更详细的网络协议栈信息，比如上面显示了 TCP 协议的主动连接（<em>active connections openings</em>）、被动连接（<em>passive connection openings</em>）、失败重试（<em>failed connection attempts</em>）、发送（<em>segments send out</em>）和接收（<em>segments received</em>）的分段数量等各种信息。</p>
<h3 id="网络吞吐率和-PPS-如何查看？"><a href="#网络吞吐率和-PPS-如何查看？" class="headerlink" title="网络吞吐率和 PPS 如何查看？"></a>网络吞吐率和 PPS 如何查看？</h3><p>可以使用 <code>sar</code> 命令当前网络的吞吐率和 PPS，用法是给 <code>sar</code> 增加 <code>-n</code> 参数就可以查看网络的统计信息，比如</p>
<ul>
<li>sar -n DEV，显示网口的统计数据；</li>
<li>sar -n EDEV，显示关于网络错误的统计数据；</li>
<li>sar -n TCP，显示 TCP 的统计数据</li>
</ul>
<p>比如，我通过 <code>sar</code> 命令获取了网口的统计信息：</p>
<p><img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%BD%91%E7%BB%9C/sar.png" alt="img"></p>
<p>它们的含义：</p>
<ul>
<li><code>rxpck/s</code> 和 <code>txpck/s</code> 分别是接收和发送的 PPS，单位为包 &#x2F; 秒。</li>
<li><code>rxkB/s</code> 和 <code>txkB/s</code> 分别是接收和发送的吞吐率，单位是 KB&#x2F; 秒。</li>
<li><code>rxcmp/s</code> 和 <code>txcmp/s</code> 分别是接收和发送的压缩数据包数，单位是包 &#x2F; 秒。</li>
</ul>
<p>对于带宽，我们可以使用 <code>ethtool</code> 命令来查询，它的单位通常是 <code>Gb/s</code> 或者 <code>Mb/s</code>，不过注意这里小写字母 <code>b</code> ，表示比特而不是字节。我们通常提到的千兆网卡、万兆网卡等，单位也都是比特（<em>bit</em>）。如下你可以看到， eth0 网卡就是一个千兆网卡：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool eth0 | grep Speed</span><br><span class="line">  Speed: 1000Mb/s</span><br></pre></td></tr></table></figure>

<h3 id="连通性和延时如何查看？"><a href="#连通性和延时如何查看？" class="headerlink" title="连通性和延时如何查看？"></a>连通性和延时如何查看？</h3><p>要测试本机与远程主机的连通性和延时，通常是使用 <code>ping</code> 命令，它是基于 ICMP 协议的，工作在网络层。</p>
<p>不过，需要注意的是，<code>ping</code> 不通服务器并不代表 HTTP 请求也不通，因为有的服务器的防火墙是会禁用 ICMP 协议的。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/01/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" data-id="cld6uiojy0011acsf9uj8htdn" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/01/22/%E9%9D%99%E6%80%81%E9%93%BE%E6%8E%A5%E5%92%8C%E5%8A%A8%E6%80%81%E9%93%BE%E6%8E%A5/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2023/01/22/%E8%B0%B7%E6%AD%8Cc++%E9%A3%8E%E6%A0%BC/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">January 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/01/22/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/01/22/%E7%BD%91%E7%BB%9Cio%E6%A8%A1%E5%9E%8B/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/01/22/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/01/22/%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%AE%9A%E6%97%B6%E5%99%A8/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/01/22/%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E9%9D%9E%E6%B3%95%E8%BE%93%E5%85%A5%E4%BB%A5%E5%8F%8A%E6%9E%84%E9%80%A0%E5%BC%82%E5%B8%B8%E6%83%85%E5%86%B5/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>